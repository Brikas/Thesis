{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import brikasutils as bu\n",
    "importlib.reload(bu)\n",
    "import shared_utils as utils\n",
    "from shared_utils import systemMsg, userMsg, assistantMsg\n",
    "importlib.reload(utils)\n",
    "import survey\n",
    "importlib.reload(survey)\n",
    "import persona\n",
    "importlib.reload(persona)\n",
    "\n",
    "import ollama\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "from typing import List\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THE REAL DEAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################### Retrieval below ##################\n",
    "surv = {}\n",
    "surv['POSSIBLE_ANSWERS'] = [\"I LIKE IT\", \"I EXPECT IT\", \"I AM NEUTRAL\", \"I CAN TOLERATE IT\", \"I DISLIKE IT\", \"SOMEWHAT DISAGREE\", \"DISAGREE\", \"NEUTRAL\", \"SOMEWHAT AGREE\", \"AGREE\"]\n",
    "\n",
    "def extract_possible_answer(value):\n",
    "    for phrase in surv['POSSIBLE_ANSWERS']:\n",
    "        pattern = r'(?i)' + re.escape(phrase)\n",
    "        match = re.search(pattern, value)\n",
    "        if match:\n",
    "            return match.group()\n",
    "    return value  # Return the original value if no possible answer is found\n",
    "\n",
    "############ Invalid Answers ##################\n",
    "def get_invalid_answers(value):\n",
    "    if pd.isna(value):\n",
    "        return \"\"\n",
    "    elif value == \"NaN\":\n",
    "        return \"\"\n",
    "    elif value in surv['POSSIBLE_ANSWERS']:\n",
    "        return \"\"\n",
    "    else:\n",
    "        return value\n",
    "    \n",
    "# surv = survey.PersonalitySurvey()\n",
    "# surv = survey.KanoSurvey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the simulation results\n",
    "# both personality and game WIP\n",
    "\n",
    "dfs = []\n",
    "filenames = []\n",
    "# changed_values_per_column = {}\n",
    "\n",
    "# List of folder paths\n",
    "# folder_paths = ['simulations/local/video-game/', 'simulations/local/personality/']\n",
    "# folder_paths = ['batch/output/_pers']\n",
    "folder_paths = ['batch/output/llama3_7b/pers']#, 'batch/output/_pers']\n",
    "# pers_folder_path = ['batch/output/_pers']\n",
    "# kano_folder_path = ['batch/output/_kano']\n",
    "\n",
    "# Read the first CSV file from the first folder to get the 'question' column\n",
    "first_folder_path = folder_paths[0]\n",
    "first_file_path = os.path.join(first_folder_path, os.listdir(first_folder_path)[0])\n",
    "first_df = pd.read_csv(first_file_path)\n",
    "questions = first_df['question']\n",
    "# csv_loop_counter = 0\n",
    "\n",
    "for folder_path in folder_paths:\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Store the original 'answer' column\n",
    "            # original_answers = df['answer'].copy()\n",
    "            \n",
    "            # # remove all characters from a black list from the column answer\n",
    "            df['answer'] = df['answer'].apply(lambda x: x.strip())\n",
    "            for substr in utils.BLACKLIST_ANSWER_SUBSTRINGS:\n",
    "                df['answer'] = df['answer'].apply(lambda x: re.sub(substr, \"\", x))\n",
    "            df['answer'] = df['answer'].str.upper()\n",
    "            df['answer'] = df['answer'].apply(extract_possible_answer)\n",
    "            # df['answer'] = df['answer'].apply(get_invalid_answers)\n",
    "            \n",
    "            # changed_format = (df['answer'] != original_answers).sum()\n",
    "            # same_format = (df['answer'] == original_answers).sum()\n",
    "            \n",
    "            # Calculate the number of changed values for each column\n",
    "            # for col in df.columns:\n",
    "            #     if col not in changed_values_per_column:\n",
    "            #         changed_values_per_column[col] = 0\n",
    "                # changed_values_per_column[col] += (df[col] != original_answers[col]).sum()\n",
    "\n",
    "            filename_without_ext = filename[:-4]  # Remove the '.csv' extension\n",
    "            filename_without_simulation = (\n",
    "                filename_without_ext.replace('_simulation', '')\n",
    "                                    # .replace('video-', '')\n",
    "            )  # Remove '_simulation' from the filename\n",
    "            filenames.append(filename_without_simulation)\n",
    "            dfs.append(df['answer'])\n",
    "            # csv_loop_counter += 1\n",
    "\n",
    "answers_df = pd.concat(dfs, axis=1, keys=filenames)\n",
    "# answers_df.insert(0, 'question', questions)  # Insert the 'question' column at the beginning\n",
    "#print(f\"Values changed: {changed_format}\")\n",
    "df = answers_df\n",
    "df[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SURVEY (both PERSONALITY and KANO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surv = survey.PersonalitySurvey()\n",
    "# df['isValid'] = df['answer'].apply(lambda x: x in surv.POSSIBLE_ANSWERS)\n",
    "# df.iloc[:, 1:] = df.iloc[:, 1:].applymap(lambda x: remap_dict.get(x, x))\n",
    "df['isValid'] = df.iloc[:, 1:].map(lambda x: x in surv.POSSIBLE_ANSWERS).all(axis=1)\n",
    "if df['isValid'].all():\n",
    "    df = df.drop('isValid', axis=1)\n",
    "    print(\"All answers were valid\")\n",
    "else:\n",
    "    print(\"Some answers were not valid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surv = survey.PersonalitySurvey()\n",
    "# surv = survey.KanoSurvey()\n",
    "\n",
    "# # Add airidas and elias answers\n",
    "# air = surv.test_answers[\"airidas\"]\n",
    "# answers_df.insert(1, \"airidas\", air[:len(df)])\n",
    "eli = surv.test_answers[\"elias\"]\n",
    "df.insert(0, \"elias\", eli[:len(df)])\n",
    "\n",
    "# df['answer'] = df['answer'].str.upper()\n",
    "# df['elias'] = df['elias'].str.upper()\n",
    "# df = df.dropna()\n",
    "# df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Str2Int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remaps - UNIVERSAL\n",
    "if isinstance(surv, survey.KanoSurvey):\n",
    "    remap_dict = {\"I EXPECT IT\": 5, \"I LIKE IT\": 4, \"I AM NEUTRAL\": 3, \"I CAN TOLERATE IT\": 2, \"I DISLIKE IT\": 1}\n",
    "    df.iloc[:, 1:] = df.iloc[:, 1:].applymap(lambda x: remap_dict.get(x, x))\n",
    "    # df['answer'] = df['answer'].map(remap_dict)\n",
    "    # df['airidas'] = df['airidas'].map(remap_dict)\n",
    "    # df['elias'] = df['elias'].map(remap_dict)\n",
    "elif isinstance(surv, survey.PersonalitySurvey):\n",
    "    remap_dict = {\"AGREE\": 5, \"SOMEWHAT AGREE\": 4, \"NEUTRAL\": 3, \"SOMEWHAT DISAGREE\": 2, \"DISAGREE\": 1}\n",
    "    df.iloc[:, 1:] = df.iloc[:, 1:].applymap(lambda x: remap_dict.get(x, x))\n",
    "    # df.iloc[:, 2:] = df.iloc[:, 2:].map(lambda x: remap_dict[x])\n",
    "    # remap_cols = df.columns[df.columns.str.contains('|'.join(remap_dict.keys()))]    \n",
    "    # df['answer'] = df['answer'].map(remap_dict)\n",
    "\n",
    "df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.iloc[:, 1:] = df.iloc[:, 1:].applymap(lambda x: int(x) if not pd.isna(x) else x)\n",
    "# df.iloc[:, 1:] = df.iloc[:, 1:].astype(pd.Int64Dtype())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# df.count_non_matching_fields(df, _PersonalitySurvey_)\n",
    "\n",
    "def count_non_matching_fields(df, surv):\n",
    "    if isinstance(surv, survey._KanoSurvey_):\n",
    "        remap_dict = {\"I EXPECT IT\": 5, \"I LIKE IT\": 4, \"I AM NEUTRAL\": 3, \"I CAN TOLERATE IT\": 2, \"I DISLIKE IT\": 1}\n",
    "        possible_answers = list(remap_dict.keys())\n",
    "\n",
    "        non_matching_counts = (\n",
    "            df.iloc[:, 1:]\n",
    "            .applymap(lambda x: x not in possible_answers)\n",
    "            .sum()\n",
    "            .rename(\"non_matching_count\")\n",
    "        )\n",
    "\n",
    "        df.iloc[:, 1:] = df.iloc[:, 1:].applymap(lambda x: remap_dict.get(x, x))\n",
    "\n",
    "    elif isinstance(surv, survey._PersonalitySurvey_):\n",
    "        remap_dict = {\"AGREE\": 5, \"SOMEWHAT AGREE\": 4, \"NEUTRAL\": 3, \"SOMEWHAT DISAGREE\": 2, \"DISAGREE\": 1}\n",
    "        possible_answers = list(remap_dict.keys())\n",
    "\n",
    "        non_matching_counts = (\n",
    "            df.iloc[:, 2:]\n",
    "            .applymap(lambda x: x not in possible_answers)\n",
    "            .sum()\n",
    "            .rename(\"non_matching_count\")\n",
    "        )\n",
    "\n",
    "        df.iloc[:, 2:] = df.iloc[:, 2:].applymap(lambda x: remap_dict.get(x, x))\n",
    "\n",
    "    return non_matching_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your DataFrame is named 'df'\n",
    "for col in df.columns[1:]:  # Iterate over all columns except the target column\n",
    "    matches = df.loc[df[col] == df['elias'], col]  # Find rows where values match the target\n",
    "    print(f\"Exact matches for column '{col}': {len(matches)}\")\n",
    "\n",
    "# Compute correlation between each column and the target column\n",
    "correlations = df.corr()['elias'].drop('elias')\n",
    "print(correlations)\n",
    "\n",
    "for col in df.columns[1:]:\n",
    "    matches = df.loc[df[col] == df['elias'], col]\n",
    "    print(f\"Exact matches for column '{col}': {len(matches)}\")\n",
    "\n",
    "correlations = df.corr()['elias'].drop('elias')\n",
    "print(\"\\nCorrelations:\")\n",
    "print(correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = df.corr()\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"YlGnBu\")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Assuming 'col1' and 'col2' have the highest positive and negative correlations, respectively\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(df['eli_pers_static-IMPERSONATE_5'], df['elias'])\n",
    "plt.xlabel('eli_pers_static-IMPERSONATE_5')\n",
    "plt.ylabel('elias')\n",
    "plt.title('Scatter Plot: eli_pers_static-IMPERSONATE_5 vs. elias')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(df['eli_pers_static-ARE_1'], df['elias'])\n",
    "plt.xlabel('eli_pers_static-ARE_1')\n",
    "plt.ylabel('elias')\n",
    "plt.title('Scatter Plot: eli_pers_static-ARE_1 vs. elias')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:, 1:] = df.iloc[:, 1:].applymap(lambda x: remap_dict.get(x, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_answers = [\"I LIKE IT\", \"I EXPECT IT\", \"I AM NEUTRAL\", \"I CAN TOLERATE IT\", \"I DISLIKE IT\", \"SOMEWHAT DISAGREE\", \"DISAGREE\", \"NEUTRAL\", \"SOMEWHAT AGREE\", \"AGREE\"]#surv['POSSIBLE_ANSWERS']\n",
    "# Assuming you have a DataFrame 'df' with columns ['col1', 'col2', 'elias', 'col3', 'col4', ...]\n",
    "# and a set 'valid_answers' containing the valid answer values\n",
    "# Get the column names except the first two  and 'elias'\n",
    "cols = [col for col in df.columns[0:] if col != 'elias']\n",
    "# Create a dictionary to store the computed values\n",
    "results = {}\n",
    "# Function to compute \"Exact Matches\"\n",
    "def exact_matches(col):\n",
    "    # Filter the rows where both columns have valid answers\n",
    "    valid_rows = (df[col].isin(valid_answers)) & (df['elias'].isin(valid_answers))\n",
    "    # Compute exact matches only for valid rows\n",
    "    return (df.loc[valid_rows, col] == df.loc[valid_rows, 'elias']).mean()\n",
    "# Function to compute \"Correlation\"\n",
    "def correlation(col):\n",
    "    # Filter the rows where both columns have valid answers\n",
    "    valid_rows = (df[col].isin(valid_answers)) & (df['elias'].isin(valid_answers))\n",
    "    # Compute correlation only for valid rows\n",
    "    return df.loc[valid_rows, col].corr(df.loc[valid_rows, 'elias'])\n",
    "# Compute the \"Exact Matches\" and \"Correlation\" for each column against 'elias'\n",
    "for col in cols:\n",
    "    key1 = f\"Exact Matches - {col} vs elias\"\n",
    "    key2 = f\"Correlation - {col} vs elias\"\n",
    "    results[key1] = exact_matches(col)\n",
    "    results[key2] = correlation(col)\n",
    "# Sort the columns by their correlation with 'elias' in descending order\n",
    "sorted_cols = sorted(cols, key=lambda col: results[f\"Correlation - {col} vs elias\"], reverse=True)\n",
    "# Print the results sorted by correlation, from highest to lowest\n",
    "for col in sorted_cols:\n",
    "    em_key = f\"Exact Matches - {col} vs elias\"\n",
    "    corr_key = f\"Correlation - {col} vs elias\"\n",
    "    print(f\"{col}: Exact Matches = {results[em_key]}, Correlation = {results[corr_key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Embed technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Embed technique ################\n",
    "# RETRIEVAL_PROMPT = surv['POSSIBLE_ANSWERS']\n",
    "PRE_DEF_ANSWERS = [\"I LIKE IT\", \"I EXPECT IT\", \"I AM NEUTRAL\", \"I CAN TOLERATE IT\", \"I DISLIKE IT\", \"SOMEWHAT DISAGREE\", \"DISAGREE\", \"NEUTRAL\", \"SOMEWHAT AGREE\", \"AGREE\"]\n",
    "\n",
    "embeddings = [ollama.embeddings(model=EMBED_MODEL, prompt=answer)[\"embedding\"] for answer in PRE_DEF_ANSWERS]\n",
    "\n",
    "FINAL_STRINGS_2_CLEAN = df['answer'] #change to answers_df['answer'] for all simulations\n",
    "# utils.find_most_similar(ollama.embeddings(model=EMBED_MODEL, prompt=FINAL_STRINGS_2_CLEAN[0])[\"embedding\"], embeddings)\n",
    "mapped_results = [utils.find_most_similar(string) for string in FINAL_STRINGS_2_CLEAN]\n",
    "print(mapped_results)\n",
    "# Generate embeddings for each chunk\n",
    "embeddings = []\n",
    "\n",
    "progress, chunks_len = 0, len(chunks) # for progress bar\n",
    "for chunk_text in chunks:\n",
    "    progress += 1\n",
    "    print(f\"\\rChunk {progress}/{chunks_len}\", end=\"\")\n",
    "\n",
    "    embedding = ollama.embeddings(model=EMBED_MODEL, prompt=chunk_text)[\"embedding\"]\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "bu.if_dir_not_exist_make(\"data/3_embeddings\")\n",
    "bu.quickJSON(AUTO_INFO, f\"data/3_embeddings/POSSIBLE_ANSWERS_info.json\")\n",
    "bu.quickJSON({\"chunks\": chunks, \"embeddings\": embeddings}, f\"data/3_embeddings/POSSIBLE_ANSWERS_embeddings.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trash?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import simulation and info sheet\n",
    "SIMULATION_NAMEID = \"airidas-personality_mixtral_cv1\" #f\"{SIM_ID}\"\n",
    "df = pd.read_csv(f'simulations/local/personality/{SIMULATION_NAMEID}_simulation.csv')\n",
    "with open(f'simulations/local/personality/{SIMULATION_NAMEID}_info.json', 'r') as f:\n",
    "    loaded = json.load(f)\n",
    "try:\n",
    "    AUTO_INFO = loaded[\"info\"]\n",
    "    SETTINGS = loaded[\"settings\"]\n",
    "    print(\"Settings and info loaded:\")\n",
    "    for k, v in AUTO_INFO.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "    for k, v in SETTINGS.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "except:\n",
    "    print(\"No settings and/or info found\")\n",
    "try:\n",
    "    if str(type(surv)) != AUTO_INFO[\"survey_type\"]:\n",
    "        print(f\"WARNING: surv variable is not of the same type. {str(type(surv))} != {AUTO_INFO['survey_type']}\")\n",
    "except:\n",
    "    pass\n",
    "df.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LiveCSV\n",
    "bu.if_dir_not_exist_make(\"data/6_sim-clean/results\")\n",
    "res = bu.LiveCSV(f\"data/6_sim-clean/elias_runs.csv\")\n",
    "\n",
    "# survey = survey.PersonalitySurvey()\n",
    "non_matching_counts = count_non_matching_fields(df, surv)\n",
    "print(non_matching_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute one number of how the percentage of correct answers\n",
    "result_data = {\n",
    "    # \"Exact Matches - airi\": (df['answer'] == df['airidas']).sum() / len(df),\n",
    "    # \"Correlation - airi\": df['answer'].corr(df['airidas']),\n",
    "    \"Exact Matches - eli\": (df['answer'] == df['elias']).sum() / len(df),\n",
    "    \"Correlation - eli\": df['answer'].corr(df['elias']),\n",
    "}\n",
    "# compute one number of how the percentage of correct answers\n",
    "# print(f\"Exact Matches: {(df['CLONE_eli'] == df['IRL_eli']).sum() / len(df)}\")\n",
    "# print(f\"Correlation: {df['CLONE_eli'].corr(df['IRL_eli'])}\")\n",
    "# df['elias_correct'] = df['CLONE_eli'] == df['IRL_eli']\n",
    "df.iloc[:, 2:] = df.iloc[:, 2:].applymap(lambda x: remap_dict.get(x, x))\n",
    "for k, v in result_data.items():\n",
    "    print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_res = {\n",
    "    # \"label\": None,\n",
    "    \"SIMULATION_NAMEID\": SIMULATION_NAMEID,#SIM_ID,\n",
    "    \"timestamp\": bu.get_timestamp(),\n",
    "    \"survey_type\": str(type(surv)),\n",
    "    # \"temperature\": SETTINGS[\"temperature\"],\n",
    "    # \"note\": \"\",\n",
    "    \"exact_matches\": result_data[\"Exact Matches\"],\n",
    "    \"corr\": result_data[\"Correlation\"],\n",
    "    \"exact_matches_elias\": result_data[\"Exact Matches - elias\"],\n",
    "    \"corr_elias\": result_data[\"Correlation - elias\"],\n",
    "}\n",
    "\n",
    "tmp = bu.convert_dicts_to_table([new_res])\n",
    "res.append_data(tmp[1], tmp[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _get_invalid_answers_(value, surv, invalid_answers_df=None):\n",
    "#   --||---\n",
    "#     else:\n",
    "#         if invalid_answers_df is None:\n",
    "#             invalid_answers_df = pd.DataFrame({'invalid_answer': [value]})\n",
    "#         elif invalid_answers_df is not None:\n",
    "#             invalid_answers_df = invalid_answers_df.append({'invalid_answer': value}, ignore_index=True)\n",
    "#         else:\n",
    "#             new_row = pd.DataFrame({'invalid_answer': [value]})\n",
    "#             invalid_answers_df = pd.concat([invalid_answers_df, new_row], ignore_index=True)\n",
    "#         return value\n",
    "\n",
    "#################### ^Retrieval ^ ####################\n",
    "################ VALIDATION #############\n",
    "for value in values:\n",
    "    result = _get_invalid_answers_(value, surv, invalid_answers_df)\n",
    "    if result == \"\":\n",
    "        continue\n",
    "    elif isinstance(result, pd.DataFrame):\n",
    "        invalid_answers_df = result\n",
    "    else:\n",
    "        print(result)\n",
    "\n",
    "invalid_answers_df = pd.DataFrame.append(invalid_answers_df, pd.DataFrame({'invalid_answer': [value]}), ignore_index=True)\n",
    "invalid_answers_df = pd.DataFrame(columns=['invalid_answer'])\n",
    "# Update isValid\n",
    "df['isValid'] = df['answer'].apply(lambda x: x in surv.POSSIBLE_ANSWERS)\n",
    "\n",
    "# ################ VALIDATION #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['airidas'] = df['airidas'].str.upper()\n",
    "# df['elias'] = df['elias'].str.upper()\n",
    "# df['answer'] = df['answer'].map(remap_dict)\n",
    "# df['airidas'] = df['airidas'].map(remap_dict)\n",
    "# df['elias'] = df['elias'].map(remap_dict)\n",
    "\n",
    "########################\n",
    "# df = df.drop(columns=['uppercase_text'])\n",
    "# df['CLONE_eli'] = df['answer'].apply(extract_uppercase_text)\n",
    "# df['CLONE_eli'] = df['CLONE_eli'].str.upper()\n",
    "# .str.upper() or .lower()\n",
    "# df['answer'] = df['answer'].map(remap_dict, na_action='ignore')\n",
    "#df['CLONE_eli'] = df['CLONE_eli'].fillna(0).astype(int)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
