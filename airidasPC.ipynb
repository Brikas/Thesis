{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib # Used to add reload functionality, for when we update our own custom libraries\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "\n",
    "# Custom libraries\n",
    "import brikasutils as bu\n",
    "importlib.reload(bu)\n",
    "import shared_utils as utils\n",
    "from shared_utils import systemMsg, userMsg, assistantMsg, find_most_similar\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Custom libraries\n",
    "import standard_msg_reader as fb\n",
    "importlib.reload(fb)\n",
    "import persona\n",
    "importlib.reload(persona)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab.count_chat_tokens(\"petyo\")\n",
    "ab.parse_fb_messages(texts_with_elias, \"elias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import survey\n",
    "importlib.reload(survey)\n",
    "surv = survey.KanoSurvey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "EMBED_MODEL = \"nomic-embed-text\"        # nomic-embed-text = long ctx / mxbai-embed-large = big\n",
    "CHUNK_SIZE = 30                         # Number of messages per chunk\n",
    "OVERLAP_SIZE = 10                       # Number of overlapping messages between consecutive chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Serialization ########\n",
    "EMBEDDING_NAMEID = \"test1\"\n",
    "AUTO_INFO = {\n",
    "    \"model\": EMBED_MODEL,\n",
    "    \"CHUNK_SIZE\": CHUNK_SIZE,\n",
    "    \"OVERLAP_SIZE\": OVERLAP_SIZE,\n",
    "    \"chunks_count\": chunks_count,\n",
    "    \"total_messages\": total_messages,\n",
    "    \"stat_total_msgs_in_chunks\": stat_total_msgs_in_chunks,\n",
    "    \"modules_chat\": token_counts,\n",
    "}\n",
    "##################################\n",
    "\n",
    "# Generate embeddings for each chunk\n",
    "embeddings = []\n",
    "\n",
    "progress, chunks_len = 0, len(chunks) # for progress bar\n",
    "for chunk_text in chunks:\n",
    "    progress += 1\n",
    "    print(f\"Chunk {progress}/{chunks_len}\")\n",
    "\n",
    "    embedding = ollama.embeddings(model=EMBED_MODEL, prompt=chunk_text)[\"embedding\"]\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "\n",
    "# Display and save results (if needed later)\n",
    "print(f\"Chunks:{len(chunks)}, embeds:{len(embeddings)}\")\n",
    "\n",
    "bu.if_dir_not_exist_make(\"embeddings\")\n",
    "bu.quickJSON(AUTO_INFO, f\"embeddings/{EMBEDDING_NAMEID}_info.json\")\n",
    "bu.quickJSON({\"chunks\": chunks, \"embeddings\": embeddings}, f\"embeddings/{EMBEDDING_NAMEID}_embeddings.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Embeddings From File (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_NAMEID = \"test1\"\n",
    "\n",
    "import json\n",
    "with open(f\"embeddings/{EMBEDDING_NAMEID}_embeddings.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    chunks = data[\"chunks\"]\n",
    "    embeddings = data[\"embeddings\"]\n",
    "\n",
    "print(f\"Chunks:{len(chunks)}, embeds:{len(embeddings)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RETRIEVAL_PROMPT = \"video game features\"\n",
    "# RETRIEVAL_PROMPT = \"personality\"\n",
    "CHUNKS_COUNT_IN_CTX = 35 # Number of nearby chunks to put in context window\n",
    "# COMMENT 04-16, perhaps we could try 5x retrievals with isolated semantics\n",
    "\n",
    "# Perform similarity search and print simulations\n",
    "prompt_embedding = ollama.embeddings(model=EMBED_MODEL, prompt=RETRIEVAL_PROMPT)[\"embedding\"]\n",
    "chunks_most_similar_embeddings = find_most_similar(prompt_embedding, embeddings)[:CHUNKS_COUNT_IN_CTX]\n",
    "chunks_most_similar = []\n",
    "for embedding in chunks_most_similar_embeddings:\n",
    "    chunks_most_similar.append(chunks[embedding[1]])\n",
    "\n",
    "# token counts in all similar chunks\n",
    "tokens_in_chunks = 0\n",
    "for chunk in chunks_most_similar:\n",
    "    tokens_in_chunks += utils.count_tokens(chunk)\n",
    "print(f\"Tokens in chunks: {tokens_in_chunks}\")\n",
    "\n",
    "bu.quickTXT(\"\\n\\n\".join(chunks_most_similar), filename=\"ignorefolder/chunks.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (dynamic, same as question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_retrieval_prompts = list(surv.questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (dynamic, custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run dynamic retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNKS_COUNT_IN_CTX = 10 # Number of nearby chunks to put in context window\n",
    "dynamic_chunks_most_similar: List[List[str]] = [] \n",
    "\n",
    "progress = 0\n",
    "lenn = len(dynamic_retrieval_prompts)\n",
    "for prompt in dynamic_retrieval_prompts:\n",
    "    progress += 1\n",
    "    print(f\"\\rPrompt {progress}/{lenn}\", end=\"\")\n",
    "\n",
    "    prompt_embedding = ollama.embeddings(model=EMBED_MODEL, prompt=prompt)[\"embedding\"]\n",
    "    chunks_most_similar_embeddings = find_most_similar(prompt_embedding, embeddings)[:CHUNKS_COUNT_IN_CTX]\n",
    "    chunks_most_similar = []\n",
    "    for embedding in chunks_most_similar_embeddings:\n",
    "        chunks_most_similar.append(chunks[embedding[1]])\n",
    "\n",
    "    dynamic_chunks_most_similar.append(chunks_most_similar)\n",
    "print(end=\"\\n\")\n",
    "    \n",
    "# VANITY PRINT\n",
    "tokens_in_chunks = 0\n",
    "for chunks_most_similar in dynamic_chunks_most_similar:\n",
    "    for chunk in chunks_most_similar:\n",
    "        tokens_in_chunks += utils.count_tokens(chunk)\n",
    "    \n",
    "print(f\"Tokens in average chunk group: {tokens_in_chunks/len(dynamic_chunks_most_similar)}\")\n",
    "bu.quickJSON(dynamic_chunks_most_similar, filename=f\"ignorefolder/dynamic-chunks.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preview generated dynamic chunks (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_text = \"\"\n",
    "PREVIEW_LIMIT = 10\n",
    "\n",
    "for i, chunks_most_similar in enumerate(dynamic_chunks_most_similar):\n",
    "    preview_text += f\"==============Prompt: {dynamic_retrieval_prompts[i]}==============\\n\"\n",
    "    for j, chunk in enumerate(chunks_most_similar):\n",
    "        if j >= PREVIEW_LIMIT:\n",
    "            break\n",
    "        preview_text += f\"=======CHUNK {j}=======\\n{chunk}\\n\\n\"\n",
    "    preview_text += \"\\n\\n\"\n",
    "bu.quickTXT(preview_text, filename=f\"ignorefolder/dynamic-chunks_preview.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survey Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Builder - Persona - Dynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Builder - Persona"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Builder - Base case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Unused / Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persona_text = \"Favorite video games are Rimworld, Minecraft, Age of Empires, 7 Days to Die\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Survey Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis - General"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Simulation File (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For GPT3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Local Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey.validateMatchingSurvey(surv, df[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns\n",
    "df.columns = ['real', 'answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proces simulation output - KANO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proces simulation output - PERSONALITY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remaps - UNIVERSAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Results CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_res = {\n",
    "    # \"label\": None,\n",
    "    \"SIMULATION_NAMEID\": SIMULATION_NAMEID,\n",
    "    \"timestamp\": bu.get_timestamp(),\n",
    "    \"survey_type\": str(type(surv)),\n",
    "    \"temperature\": SETTINGS[\"temperature\"],\n",
    "    # \"note\": \"\",\n",
    "    \"exact_matches\": result_data[\"Exact Matches\"],\n",
    "    \"corr\": result_data[\"Correlation\"],\n",
    "    \"exact_matches_elias\": result_data[\"Exact Matches - elias\"],\n",
    "    \"corr_elias\": result_data[\"Correlation - elias\"],\n",
    "}\n",
    "\n",
    "tmp = bu.convert_dicts_to_table([new_res])\n",
    "res.append_data(tmp[1], tmp[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch processing (can be run without ones above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "import traceback\n",
    "import sys\n",
    "\n",
    "import brikasutils as bu\n",
    "import shared_utils as utils\n",
    "import survey\n",
    "importlib.reload(bu)\n",
    "importlib.reload(utils)\n",
    "importlib.reload(survey)\n",
    "\n",
    "queue = bu.FileRunQueue(queue_folder_path=\"batch/queue\", completed_folder_path=\"batch/done\")\n",
    "report_live_csv = bu.LiveCSV(\"batch/run_reports.csv\")\n",
    "timer = bu.Benchmarker()\n",
    "\n",
    "\n",
    "for filepath in queue:\n",
    "    timer.mark_start(filepath)\n",
    "\n",
    "    try: \n",
    "        ########## Handle batch stuff ########\n",
    "        filename = os.path.splitext(os.path.basename(filepath))[0]\n",
    "        with open(filepath, 'r') as f:\n",
    "            rundata = json.load(f)\n",
    "\n",
    "        # Load prompt file\n",
    "        with open(rundata[\"instructions\"][\"prompt_file\"], 'r') as f:\n",
    "            final_prompts = json.load(f)\n",
    "\n",
    "        # Make the surv\n",
    "        if rundata[\"instructions\"][\"survey_type\"] == \"KanoSurvey\":\n",
    "            surv = survey.KanoSurvey()\n",
    "        elif rundata[\"instructions\"][\"survey_type\"] == \"PersonalitySurvey\":\n",
    "            surv = survey.PersonalitySurvey()\n",
    "        else:\n",
    "            raise Exception(\"Invalid survey type\")\n",
    "\n",
    "        timestamp = bu.get_timestamp()\n",
    "        ######### Run Simulation ########\n",
    "        SIMULATION_NAMEID = filename\n",
    "        LIMIT = rundata[\"instructions\"][\"LIMIT\"] if \"LIMIT\" in rundata[\"instructions\"] else None\n",
    "        AUTO_INFO = {\n",
    "            \"date\": timestamp,\n",
    "            **rundata[\"info\"], # unpacked from rundata\n",
    "            \"limit\": LIMIT,\n",
    "            \"prompt_count\": min(len(final_prompts), LIMIT) if LIMIT != None else len(final_prompts),\n",
    "            \"avg_tokens_in_prompt\": round(utils.describe_prompts(final_prompts)[\"total_all_prompt_tokens\"]/len(final_prompts)),\n",
    "        }\n",
    "        SETTINGS = {\n",
    "            **rundata[\"settings\"], # unpacked from rundata\n",
    "        }\n",
    "\n",
    "        # client depends on if it's local or not\n",
    "        if rundata[\"instructions\"][\"isLocal\"]:\n",
    "            client = OpenAI(\n",
    "                base_url = 'http://localhost:11434/v1',\n",
    "                api_key='ollama', # required, but unused\n",
    "            )\n",
    "        else:\n",
    "            client = OpenAI(\n",
    "                api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "            )\n",
    "\n",
    "        completions = []\n",
    "        l = len(final_prompts)\n",
    "\n",
    "        for i, (prompt, question) in enumerate(list(zip(final_prompts, surv.questions))):\n",
    "            if LIMIT != None and i > LIMIT:\n",
    "                break\n",
    "\n",
    "            print(f\"{i}/{l}...\", end=\"\\t\") # Print progress\n",
    "            # Send the Request\n",
    "            full_response = client.chat.completions.create(\n",
    "                messages=prompt,\n",
    "                **SETTINGS,\n",
    "            )\n",
    "            r = full_response.choices[0].message.content\n",
    "\n",
    "            completions.append({'question': question, 'answer': r})\n",
    "\n",
    "            print(f\"{question}: {r}\")\n",
    "            \n",
    "        ############ Save Important results\n",
    "        df = pd.DataFrame(completions)\n",
    "        df.to_csv(f\"batch/output/{SIMULATION_NAMEID}_simulation.csv\", index=False)\n",
    "        bu.if_dir_not_exist_make(\"batch/output/info\")\n",
    "        bu.quickJSON({\"settings\": SETTINGS, \"info\": AUTO_INFO}, f\"batch/output/info/{SIMULATION_NAMEID}_info.json\")\n",
    "\n",
    "        status = \"OK\"\n",
    "    \n",
    "    except Exception:\n",
    "        print(f\"##### Error while running {filename}.\")\n",
    "        error_string = traceback.format_exc()\n",
    "        print(error_string)\n",
    "        status = \"Failed\"\n",
    "\n",
    "    ########### Time the run\n",
    "    try:\n",
    "        time_taken = timer.mark_end(filepath)\n",
    "    except:\n",
    "        print(\"Error while timing run: \")\n",
    "        print(traceback.format_exc())\n",
    "        time_taken = None\n",
    "\n",
    "    ########### Report the run\n",
    "    try:\n",
    "        new_report = {\n",
    "            \"filename\": filename,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"time_taken\": time_taken,\n",
    "            \"status\": status,\n",
    "            **rundata[\"instructions\"],\n",
    "            \"error\": error_string if status == \"Failed\" else \"\",\n",
    "        }\n",
    "\n",
    "        tmp = bu.convert_dicts_to_table([new_report])\n",
    "        report_live_csv.append_data(tmp[1], tmp[0])\n",
    "    except Exception as e:\n",
    "        print(f\"Error while reporting: \")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    print(f\"Processed {filename}. Stauts: {status}\")\n",
    "\n",
    "timer.print_total_execution_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import persona\n",
    "importlib.reload(persona)\n",
    "import Thesis.standard_msg_reader as standard_msg_reader\n",
    "importlib.reload(standard_msg_reader)\n",
    "import whatsapp_msg_reader\n",
    "importlib.reload(whatsapp_msg_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs = whatsapp_msg_reader.get_messages_from_JSONs([\"selected-data/elias-wa/messages_1000.json\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for msg in msgs[:10]:\n",
    "    print(msg.timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
