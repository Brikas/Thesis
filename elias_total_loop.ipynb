{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import brikasutils as bu\n",
    "importlib.reload(bu)\n",
    "import shared_utils as utils\n",
    "from shared_utils import systemMsg, userMsg, assistantMsg\n",
    "importlib.reload(utils)\n",
    "import survey\n",
    "importlib.reload(survey)\n",
    "import persona\n",
    "importlib.reload(persona)\n",
    "import ollama\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "from typing import List\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 1916 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-09-13 to 2024-03-06\n",
      "Messages saved to self.chats['airidas']\n",
      "Read 618 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-09-10 to 2024-03-03\n",
      "Messages saved to self.chats['christian']\n",
      "Read 297 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2018-07-25 to 2024-01-01\n",
      "Messages saved to self.chats['nikolay']\n",
      "Read 144 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-03-28 to 2021-12-30\n",
      "Messages saved to self.chats['mathis']\n",
      "Read 104 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-08-25 to 2024-03-05\n",
      "Messages saved to self.chats['jacob']\n",
      "Read 159 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-10-12 to 2023-04-30\n",
      "Messages saved to self.chats['chris']\n",
      "Read 161 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-03-28 to 2021-06-06\n",
      "Messages saved to self.chats['aziz']\n",
      "Read 350 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-03-26 to 2022-04-11\n",
      "Messages saved to self.chats['daniela']\n",
      "Read 105 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-03-28 to 2022-07-10\n",
      "Messages saved to self.chats['mihi']\n",
      "Read 117 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-05-17 to 2022-04-11\n",
      "Messages saved to self.chats['viktoria']\n",
      "Read 172 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2019-08-11 to 2023-12-21\n",
      "Messages saved to self.chats['diba']\n",
      "Read 154 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-12-02 to 2023-04-29\n",
      "Messages saved to self.chats['filip']\n",
      "Read 659 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2023-03-12 to 2024-02-16\n",
      "Messages saved to self.chats['rebecca']\n",
      "Filtering\n",
      "link-filter: 133\n",
      "react-filter: 0\n",
      "cookie-data-filter: 12\n",
      "Selected chat rebecca for 16706 (634 messages)\n",
      "Selected chat airidas for 41064 (1671 messages)\n",
      "Selected chat christian for 10435 (576 messages)\n",
      "Selected chat nikolay for 5522 (266 messages)\n",
      "Selected chat mathis for 2531 (134 messages)\n",
      "Selected chat daniela for 5518 (293 messages)\n",
      "Selected chat diba for 3203 (156 messages)\n",
      "Selected chat aziz for 2444 (136 messages)\n",
      "Selected chat jacob for 1961 (93 messages)\n",
      "Selected chat chris for 2893 (151 messages)\n",
      "Selected chat filip for 2619 (146 messages)\n",
      "Selected chat mihi for 2016 (102 messages)\n",
      "Selected chat viktoria for 2131 (107 messages)\n"
     ]
    }
   ],
   "source": [
    "et = persona.PersonaEncoder()\n",
    "et.parse_fb_messages([\"data/1_raw/1_airidas.json\"], \"airidas\")\n",
    "et.parse_fb_messages([\"data/1_raw/2_christian.json\"], \"christian\")\n",
    "et.parse_fb_messages([\"data/1_raw/1_nikolay.json\"], \"nikolay\")\n",
    "et.parse_fb_messages([\"data/1_raw/2_mathis.json\"], \"mathis\")\n",
    "et.parse_fb_messages([\"data/1_raw/2_jacob.json\"], \"jacob\")\n",
    "et.parse_fb_messages([\"data/1_raw/2_chris.json\"], \"chris\")\n",
    "et.parse_fb_messages([\"data/1_raw/3_aziz.json\"], \"aziz\")\n",
    "et.parse_fb_messages([\"data/1_raw/3_daniela.json\"], \"daniela\")\n",
    "et.parse_fb_messages([\"data/1_raw/3_mihi.json\"], \"mihi\")\n",
    "et.parse_fb_messages([\"data/1_raw/3_viktoria.json\"], \"viktoria\")\n",
    "et.parse_fb_messages([\"data/1_raw/4_diba.json\"], \"diba\")\n",
    "et.parse_fb_messages([\"data/1_raw/6_filip.json\"], \"filip\")\n",
    "et.parse_wa_messages([\"data/1_raw/messages_1000.json\"], \"rebecca\")\n",
    "et.filter_chats_empty()\n",
    "et.filter_chats_regex(utils.BLACKLIST_CHAT_REGEX_FILTERS)\n",
    "for nameid, chat in et.chats.items():\n",
    "    for msg in chat:  \n",
    "        msg.sender = \"Persona\" if msg.sender == \"Elias Salvador Smidt Torjani\"  else \"Friend\"\n",
    "et.select_chat_full(\"rebecca\")\n",
    "et.select_chat_full(\"airidas\")\n",
    "et.select_chat_full(\"christian\")\n",
    "et.select_chat_full(\"nikolay\")\n",
    "et.select_chat_full(\"mathis\") \n",
    "et.select_chat_full(\"daniela\")\n",
    "et.select_chat_full(\"diba\")\n",
    "et.select_chat_full(\"aziz\")\n",
    "et.select_chat_full(\"jacob\")  \n",
    "et.select_chat_full(\"chris\")\n",
    "et.select_chat_full(\"filip\")\n",
    "et.select_chat_full(\"mihi\")\n",
    "et.select_chat_full(\"viktoria\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default Personality Survey CSV file: surveys/survey_personality-test.csv\n"
     ]
    }
   ],
   "source": [
    "# fixed variables (7b model, static, impersonate, 3 runs each)\n",
    "EMBED_MODEL = \"nomic-embed-text\"\n",
    "PROMPT_METHOD =\"IMPERSONATE\"\n",
    "# MODEL = \"llama3:70b\"\n",
    "SUBJECT = \"eli\"\n",
    "surv = survey.PersonalitySurvey()\n",
    "# SURVEY_PROMPT = \"Determine how much {subject} aggree with the statement. Guestimate how {subject} would answer to the question\"\n",
    "# TINY_MODULE = \"You are Elias, a 24 year old business and IT student from Copenhagen, where you now live in a dormatory.\"\n",
    "# \"MED_MODULE\": \" \"\n",
    "# \"PERSONA_TEXT\": \"Favorite video games are Minecraft, Fortnite, and Call of Duty.\",\n",
    "\n",
    "if isinstance(surv, survey.KanoSurvey):\n",
    "    DYNAMIC_RETRIEVAL_PROMPTS = list(surv.questions)\n",
    "    PROMPT_COUNT = 40\n",
    "    SURVEY_TYPE = \"KanoSurvey\",\n",
    "    WHICH_SURVEY = \"kano\"\n",
    "    RETRIEVAL_PROMPT = \"video game features\"\n",
    "    SURVEY = \"video game preferences\"\n",
    "    METHOD = \"Kano survey\"\n",
    "elif isinstance(surv, survey.PersonalitySurvey):\n",
    "    DYNAMIC_RETRIEVAL_PROMPTS = list(surv.questions)\n",
    "    PROMPT_COUNT = 50\n",
    "    SURVEY_TYPE = \"PersonalitySurvey\",\n",
    "    WHICH_SURVEY = \"pers\"\n",
    "    RETRIEVAL_PROMPT = \"openess conciousness extrovert aggreableness neuroticism\"\n",
    "    SURVEY = \"personality traits\"\n",
    "    METHOD = \"OCEAN test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof of concept run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZES = [10, 20, 30]\n",
    "OVERLAP_SIZES = [1, 3, 5]\n",
    "CHUNKS_COUNT_IN_CTX = [1, 2, 3]\n",
    "# RETRIAVAL_METHODS = [\"static\", \"dynamic\"]\n",
    "# SURVEYS = [survey.KanoSurvey(), survey.PersonalitySurvey()]\n",
    "# CHUNK_ORDER_IN_CTX = [\"CHRONOLOGICAL\", \"REVERSE\"]\n",
    "\n",
    "# 27 iterations below\n",
    "for chunk_size in CHUNK_SIZES:\n",
    "    chunks = []\n",
    "    for chat in et.selectedChats.values():\n",
    "        messages = list(chat)  # Convert chat iterator to list for easier slicing\n",
    "        num_messages = len(messages)\n",
    "        for i in range(0, num_messages - chunk_size + 1, chunk_size - overlap_size):\n",
    "            chunk = messages[i:i + chunk_size]  # Extract chunk of messages\n",
    "            chunk_text = \"\\n\".join(str(msg) for msg in chunk)  # Concatenate msgs into a single string\n",
    "            chunks.append(chunk_text)  # Append chunk to list of chunks\n",
    "            \n",
    "    for overlap_size in OVERLAP_SIZES:\n",
    "        embeddings = []\n",
    "        progress, chunks_len = 0, len(chunks) # for progress bar\n",
    "        for chunk_text in chunks:\n",
    "            progress += 1\n",
    "            print(f\"\\rChunk {progress}/{chunks_len}\", end=\"\")\n",
    "            embedding = ollama.embeddings(model=EMBED_MODEL, prompt=chunk_text)[\"embedding\"]\n",
    "            embeddings.append(embedding)\n",
    "            \n",
    "        for chunks_count_in_ctx in CHUNKS_COUNT_IN_CTX:\n",
    "            SIM_ID = f\"{chunk_size}-{overlap_size}-{chunks_count_in_ctx}-{RETRIAVAL_METHOD}\"\n",
    "            \n",
    "            final_prompts = []\n",
    "            if RETRIAVAL_METHOD == \"static\":\n",
    "                prompt_embedding = ollama.embeddings(model=EMBED_MODEL, prompt=RETRIEVAL_PROMPT)[\"embedding\"]\n",
    "                chunks_most_similar_embeddings  = utils.find_most_similar(prompt_embedding, embeddings)[:chunks_count_in_ctx]\n",
    "                chunks_most_similar = []\n",
    "                for embedding in chunks_most_similar_embeddings:\n",
    "                    chunks_most_similar.append(chunks[embedding[1]])\n",
    "                tokens_in_chunks = 0\n",
    "                for chunk in chunks_most_similar:\n",
    "                    tokens_in_chunks += utils.count_tokens(chunk)\n",
    "                print(f\"Tokens in chunks: {tokens_in_chunks}\")\n",
    "                #Retrieve^\n",
    "                prompt_template = \"\"\"\n",
    "for question in surv.questions:\n",
    "    p = [\n",
    "        systemMsg(\"\\\\n\".join([\n",
    "            f\"You are an expert actor, specializing in impersonation of non-famouns people. You will be presented to the subject through explicit datapoints of their digital footprint. In addition, you will deduct their implicit {SURVEY} by shadowing chats between the subject and friends. You will be asked to fully immerse yourself in the role, and answer questions from the point of view of the persona. \\\\n\\\\n**The persona, which you will be tasked to mimick is named '{SUBJECT}'.** \\\\n#Context \\\\n##Chat conversations between the subject and their friends:\\\\n**From most to least related**\\\\n\",\n",
    "            \"\\\\nNEW CONVERSATION:\\\\n\".join(chunks_most_similar)\n",
    "        ])),  \n",
    "        assistantMsg(\"Understood. I will answer from the point of view of the persona, {subject}, based on what I could the deduct from the text provided.\"),\n",
    "        userMsg(\"\\\\n\".join([\n",
    "            f\"Persona is questioned about their {SURVEY} in an {METHOD}. The persona must choose an appropriate answer to the question below with one of these five given options: {', '.join(surv.POSSIBLE_ANSWERS)}. Persona's answer must only contain the chosen option, without any elaboration, nor introduction.\\\\n\\\\n**Your question is:**\\\\n\\\\n\",\n",
    "            question,\n",
    "            \"\\\\nThe persona chooses:\"\n",
    "        ]))]\n",
    "    final_prompts.append(p)\n",
    "                \"\"\"\n",
    "\n",
    "#             elif RETRIAVAL_METHOD == \"dynamic\":\n",
    "#                 dynamic_retrieval_prompts = list(surv.questions)\n",
    "#                 dynamic_chunks_most_similar: List[List[str]] = [] \n",
    "\n",
    "#                 progress = 0\n",
    "#                 lenn = len(dynamic_retrieval_prompts)\n",
    "#                 for prompt in dynamic_retrieval_prompts:\n",
    "#                     progress += 1\n",
    "#                     print(f\"\\rPrompt {progress}/{lenn}\", end=\"\")\n",
    "\n",
    "#                     prompt_embedding = ollama.embeddings(model=EMBED_MODEL, prompt=prompt)[\"embedding\"]\n",
    "#                     chunks_most_similar_embeddings = utils.find_most_similar(prompt_embedding, embeddings)[:CHUNKS_COUNT_IN_CTX]\n",
    "#                     chunks_most_similar = []\n",
    "#                     for embedding in chunks_most_similar_embeddings:\n",
    "#                         chunks_most_similar.append(chunks[embedding[1]])\n",
    "\n",
    "#                     dynamic_chunks_most_similar.append(chunks_most_similar)\n",
    "#                 print(end=\"\\n\")\n",
    "#                 tokens_in_chunks = 0\n",
    "#                 for chunks_most_similar in dynamic_chunks_most_similar:\n",
    "#                     for chunk in chunks_most_similar:\n",
    "#                         tokens_in_chunks += utils.count_tokens(chunk)\n",
    "\n",
    "#                 del chunks_most_similar_embeddings # free memory\n",
    "#                 print(f\"Tokens in average chunk group: {tokens_in_chunks/len(dynamic_chunks_most_similar)}\")\n",
    "#                 #Retrieve^\n",
    "#                 prompt_template = \"\"\"\n",
    "# for question, chunks_most_similar in zip(surv.questions, dynamic_chunks_most_similar):\n",
    "# p = [\n",
    "#     systemMsg(\"\\\\n\".join([\n",
    "#         f\"You are an expert actor, specializing in impersonation of non-famouns people. You will be presented to the subject through explicit datapoints of their digital footprint. In addition, you will deduct their implicit {SURVEY} by shadowing chats between the subject and friends. You will be asked to fully immerse yourself in the role, and answer questions from the point of view of the persona. \\\\n\\\\n**The persona, which you will be tasked to mimick is named '{SUBJECT}'.** \\\\n#Context \\\\n##Chat conversations between the subject and their friends:\\\\n**From most to least related**\\\\n\",\n",
    "#         \"\\\\nNEW CONVERSATION:\\\\n\".join(chunks_most_similar)\n",
    "#     ])),      \n",
    "#     assistantMsg(\"Understood. I will answer from the point of view of the persona, {subject}, based on what I could the deduct from the text provided.\"),\n",
    "#     userMsg(\"\\\\n\".join([\n",
    "#         f\"Persona is questioned about their {SURVEY} in an {METHOD}. The persona must choose an appropriate answer to the question below with one of these five given options: {', '.join(surv.POSSIBLE_ANSWERS)}. Persona's answer must only contain the chosen option, without any elaboration, nor introduction.\\\\n\\\\n**Your question is:**\\\\n\\\\n\",\n",
    "#         question,\n",
    "#         \"\\\\nThe persona chooses:\"\n",
    "#     ]))]\n",
    "# final_prompts.append(p)\n",
    "#                 \"\"\"\n",
    "\n",
    "            else: print(\"couldn't ID retrieval method\")\n",
    "            \n",
    "            exec(prompt_template)\n",
    "            prompt_info = utils.describe_prompts_and_print(final_prompts) # Vanity print\n",
    "            bu.quickJSON(final_prompts, f\"data/5_monster_prep/{SIM_ID}_prompts.json\")\n",
    "\n",
    "            instructions = {\n",
    "                \"prompt_file\": f\"batch/prompts/{SIM_ID}_prompt.json\",\n",
    "                \"survey_type\": f\"{SURVEY_TYPE}\",\n",
    "                \"isLocal\": True,\n",
    "                \"LIMIT\": None\n",
    "            }\n",
    "            settings = {\n",
    "                \"model\": MODEL,\n",
    "                \"timeout\": 300\n",
    "            }\n",
    "            AUTO_INFO = {\n",
    "                \"CHUNK_SIZE\": chunk_size,\n",
    "                \"OVERLAP_SIZE\": overlap_size,\n",
    "                \"tokens_in_chunks\": tokens_in_chunks,\n",
    "                \"CHUNKS_COUNT_IN_CTX\": chunks_count_in_ctx,\n",
    "                \"model\": EMBED_MODEL,\n",
    "                \"prompt method\": PROMPT_METHOD,\n",
    "                \"retrieval method\": RETRIAVAL_METHOD,\n",
    "                \"retrieval prompt\": RETRIEVAL_PROMPT,\n",
    "                \"prompt_count\": PROMPT_COUNT,\n",
    "                \"survey\": WHICH_SURVEY,\n",
    "                \"subject\": SUBJECT,\n",
    "                \"prompt_template\": prompt_template,\n",
    "                **utils.describe_prompts([])\n",
    "            }\n",
    "            bu.quickJSON({\"instructions\": instructions, \"settings\": settings, \"info\": AUTO_INFO}, f\"data/5_monster_prep/batch-schema/{SIM_ID}_prompts.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V2 WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZES = [10, 20, 30]\n",
    "OVERLAP_SIZES = [1, 3, 5]\n",
    "max_tokens = 7000\n",
    "\n",
    "for chunk_size in CHUNK_SIZES:\n",
    "    chunks = []\n",
    "    chunk_token_counts = []  # Initialize a list to store token counts of each chunk\n",
    "\n",
    "    for chat in et.selectedChats.values():\n",
    "        messages = list(chat)  # Convert chat iterator to list for easier slicing\n",
    "        num_messages = len(messages)\n",
    "        for i in range(0, num_messages - chunk_size + 1, chunk_size - overlap_size):\n",
    "            chunk = messages[i:i + chunk_size]  # Extract chunk of messages\n",
    "            chunk_text = \"\\\\n\".join(str(msg) for msg in chunk)  # Concatenate msgs into a single string\n",
    "            chunks.append(chunk_text)  # Append chunk to list of chunks\n",
    "            chunk_token_counts.append(utils.count_tokens(chunk_text))  # Append token count of the chunk\n",
    "\n",
    "    avg_chunk_token_count = sum(chunk_token_counts) / len(chunk_token_counts)  # Calculate average token count per chunk\n",
    "\n",
    "    for overlap_size in OVERLAP_SIZES:\n",
    "        embeddings = []\n",
    "        progress, chunks_len = 0, len(chunks)\n",
    "        for chunk_text in chunks:\n",
    "            progress += 1\n",
    "            print(f\"\\rChunk {progress}/{chunks_len}\", end=\"\")\n",
    "            embedding = ollama.embeddings(model=EMBED_MODEL, prompt=chunk_text)[\"embedding\"]\n",
    "            embeddings.append(embedding)\n",
    "\n",
    "        if RETRIAVAL_METHOD == \"static\":\n",
    "            prompt_embedding = ollama.embeddings(model=EMBED_MODEL, prompt=RETRIEVAL_PROMPT)[\"embedding\"]\n",
    "            tokens_in_prompt = utils.count_tokens(RETRIEVAL_PROMPT)\n",
    "            tokens_remaining = max_tokens - tokens_in_prompt\n",
    "            # Calculate the maximum number of chunks that can fit within the context window\n",
    "            max_chunks_count = int((tokens_remaining / avg_chunk_token_count)-1)\n",
    "            # Retrieve the most similar chunks based on embeddings\n",
    "            chunks_most_similar_embeddings = utils.find_most_similar(prompt_embedding, embeddings)[:max_chunks_count]\n",
    "            chunks_most_similar = [chunks[embedding[1]] for embedding in chunks_most_similar_embeddings]\n",
    "            # Calculate the actual token count of the selected chunks\n",
    "            tokens_in_chunks = sum(utils.count_tokens(chunk) for chunk in chunks_most_similar)\n",
    "            print(f\"\\nAvg tokens in chunks: {avg_chunk_token_count:.1f} – Chunks in CTX: {max_chunks_count}\")\n",
    "            #Retrieve^\n",
    "            prompt_template = \"\"\"\n",
    "for question in surv.questions:\n",
    "    p = [\n",
    "        systemMsg(\"\\\\n\".join([\n",
    "            f\"You are an expert actor, specializing in impersonation of non-famouns people. You will be presented to the subject through explicit datapoints of their digital footprint. In addition, you will deduct their implicit {SURVEY} by shadowing chats between the subject and friends. You will be asked to fully immerse yourself in the role, and answer questions from the point of view of the persona. \\\\n\\\\n**The persona, which you will be tasked to mimick is named '{SUBJECT}'.** \\\\n#Context \\\\n##Chat conversations between the subject and their friends:\\\\n**From most to least related**\\\\n\",\n",
    "            \"\\\\nNEW CONVERSATION:\\\\n\".join(chunks_most_similar)\n",
    "        ])),  \n",
    "        assistantMsg(\"Understood. I will answer from the point of view of the persona, {subject}, based on what I could the deduct from the text provided.\"),\n",
    "        userMsg(\"\\\\n\".join([\n",
    "            f\"Persona is questioned about their {SURVEY} in an {METHOD}. The persona must choose an appropriate answer to the question below with one of these five given options: {', '.join(surv.POSSIBLE_ANSWERS)}. Persona's answer must only contain the chosen option, without any elaboration, nor introduction.\\\\n\\\\n**Your question is:**\\\\n\\\\n\",\n",
    "            question,\n",
    "            \"\\\\nThe persona chooses:\"\n",
    "        ]))]\n",
    "    final_prompts.append(p)\n",
    "        \"\"\"\n",
    "        else: print(\"couldn't ID retrieval method\")            \n",
    "        exec(prompt_template)\n",
    "        SIM_ID = f\"{chunk_size}-{overlap_size}-{chunks_count_in_ctx}-{RETRIAVAL_METHOD}\"\n",
    "        prompt_info = utils.describe_prompts_and_print(final_prompts) # Vanity print\n",
    "        bu.quickJSON(final_prompts, f\"data/5_monster_prep/dynamic-ctx_{SIM_ID}_prompts.json\")\n",
    "\n",
    "        instructions = {\n",
    "            \"prompt_file\": f\"batch/prompts/{SIM_ID}_prompt.json\",\n",
    "            \"survey_type\": f\"{SURVEY_TYPE}\",\n",
    "            \"isLocal\": True,\n",
    "            \"LIMIT\": None\n",
    "        }\n",
    "        settings = {\n",
    "            \"model\": MODEL,\n",
    "            \"timeout\": 300}\n",
    "        AUTO_INFO = {\n",
    "            \"CHUNK_SIZE\": chunk_size,\n",
    "            \"OVERLAP_SIZE\": overlap_size,\n",
    "            \"tokens_in_chunks\": tokens_in_chunks,\n",
    "            \"CHUNKS_COUNT_IN_CTX\": max_chunks_count,#chunks_count_in_ctx,\n",
    "            \"CTX_limit\": max_tokens,\n",
    "            \"model\": EMBED_MODEL,\n",
    "            \"prompt method\": PROMPT_METHOD,\n",
    "            \"retrieval method\": RETRIAVAL_METHOD,\n",
    "            \"retrieval prompt\": RETRIEVAL_PROMPT,\n",
    "            \"prompt_count\": PROMPT_COUNT,\n",
    "            \"survey\": WHICH_SURVEY,\n",
    "            \"subject\": SUBJECT,\n",
    "            \"prompt_template\": prompt_template,\n",
    "            \"prompt_info\": prompt_info,\n",
    "            **utils.describe_prompts([])\n",
    "        }\n",
    "        bu.quickJSON({\"instructions\": instructions, \"settings\": settings, \"info\": AUTO_INFO}, f\"data/5_monster_prep/batch-schema/dynamic-ctx_{SIM_ID}_prompts.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V3 w/o dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZES = [10, 20, 30]\n",
    "OVERLAP_SIZES = [1, 3, 5]\n",
    "max_tokens = 7000\n",
    "\n",
    "for chunk_size in CHUNK_SIZES:\n",
    "    for overlap_size in OVERLAP_SIZES:\n",
    "        chunks = []\n",
    "        chunk_token_counts = []\n",
    "        for chat in et.selectedChats.values():\n",
    "            messages = list(chat)  # Convert chat iterator to list for easier slicing\n",
    "            num_messages = len(messages)\n",
    "            for i in range(0, num_messages - chunk_size + 1, chunk_size - overlap_size):\n",
    "                chunk = messages[i:i + chunk_size]  # Extract chunk of messages\n",
    "                chunk_text = \"\\\\n\".join(str(msg) for msg in chunk)  # Concatenate msgs into a single string\n",
    "                chunks.append(chunk_text)  # Append chunk to list of chunks\n",
    "                chunk_token_counts.append(utils.count_tokens(chunk_text))  # Append token count of the chunk\n",
    "        avg_chunk_token_count = sum(chunk_token_counts) / len(chunk_token_counts)\n",
    "\n",
    "        embeddings = []\n",
    "        progress, chunks_len = 0, len(chunks)\n",
    "        for chunk_text in chunks:\n",
    "            progress += 1\n",
    "            print(f\"\\rChunk {progress}/{chunks_len}\", end=\"\")\n",
    "            embedding = ollama.embeddings(model=EMBED_MODEL, prompt=chunk_text)[\"embedding\"]\n",
    "            embeddings.append(embedding)\n",
    "       \n",
    "        if RETRIAVAL_METHOD == \"static\":\n",
    "            prompt_embedding = ollama.embeddings(model=EMBED_MODEL, prompt=RETRIEVAL_PROMPT)[\"embedding\"]\n",
    "            tokens_in_prompt = utils.count_tokens(RETRIEVAL_PROMPT)\n",
    "            tokens_remaining = max_tokens - tokens_in_prompt\n",
    "            max_chunks_count = int((tokens_remaining / avg_chunk_token_count)-1)\n",
    "            chunks_most_similar_embeddings = utils.find_most_similar(prompt_embedding, embeddings)[:max_chunks_count]\n",
    "            chunks_most_similar = [chunks[embedding[1]] for embedding in chunks_most_similar_embeddings]\n",
    "            tokens_in_chunks = sum(utils.count_tokens(chunk) for chunk in chunks_most_similar)\n",
    "            print(f\"\\nAvg tokens in chunks: {avg_chunk_token_count:.1f} – Chunks in CTX: {max_chunks_count}\")\n",
    "            #Retrieve^\n",
    "            final_prompts = []\n",
    "            prompt_template = \"\"\"\n",
    "for question in surv.questions:\n",
    "    p = [\n",
    "        systemMsg(\"\\\\n\".join([\n",
    "            f\"You are an expert actor, specializing in impersonation of non-famouns people. You will be presented to the subject through explicit datapoints of their digital footprint. In addition, you will deduct their implicit {SURVEY} by shadowing chats between the subject and friends. You will be asked to fully immerse yourself in the role, and answer questions from the point of view of the persona. \\\\n\\\\n**The persona, which you will be tasked to mimick is named '{SUBJECT}'.** \\\\n#Context \\\\n##Chat conversations between the subject and their friends:\\\\n**From most to least related**\\\\n\",\n",
    "            \"\\\\nNEW CONVERSATION:\\\\n\".join(chunks_most_similar)\n",
    "        ])),  \n",
    "        assistantMsg(\"Understood. I will answer from the point of view of the persona, {SUBJECT}, based on what I could the deduct from the text provided.\"),\n",
    "        userMsg(\"\\\\n\".join([\n",
    "            f\"Persona is questioned about their {SURVEY} in an {METHOD}. The persona must choose an appropriate answer to the question below with one of these five given options: {', '.join(surv.POSSIBLE_ANSWERS)}. Persona's answer must only contain the chosen option, without any elaboration, nor introduction.\\\\n\\\\n**Your question is:**\\\\n\\\\n\",\n",
    "            question,\n",
    "            \"\\\\nThe persona chooses:\"\n",
    "        ]))]\n",
    "    final_prompts.append(p)\n",
    "            \"\"\"\n",
    "            exec(prompt_template)\n",
    "            prompt_info = utils.describe_prompts_and_print(final_prompts) # Vanity print\n",
    "            SIM_ID = f\"{chunk_size}-0{overlap_size}-{chunks_count_in_ctx}-{RETRIAVAL_METHOD}\"\n",
    "            bu.quickJSON(final_prompts, f\"data/5_monster_prep/2ndgen-dynamic-ctx_{SIM_ID}_prompts.json\")\n",
    "\n",
    "            instructions = {\n",
    "                \"prompt_file\": f\"batch/prompts/{SIM_ID}_prompt.json\",\n",
    "                \"survey_type\": f\"{SURVEY_TYPE}\",\n",
    "                \"isLocal\": True,\n",
    "                \"LIMIT\": None\n",
    "            }\n",
    "            settings = {\n",
    "                \"model\": MODEL,\n",
    "                \"timeout\": 300}\n",
    "            AUTO_INFO = {\n",
    "                \"CHUNK_SIZE\": chunk_size,\n",
    "                \"OVERLAP_SIZE\": overlap_size,\n",
    "                \"tokens_in_chunks\": tokens_in_chunks,\n",
    "                \"CHUNKS_COUNT_IN_CTX\": max_chunks_count,#chunks_count_in_ctx,\n",
    "                \"CTX_limit\": max_tokens,\n",
    "                \"model\": EMBED_MODEL,\n",
    "                \"prompt method\": PROMPT_METHOD,\n",
    "                \"retrieval method\": RETRIAVAL_METHOD,\n",
    "                \"retrieval prompt\": RETRIEVAL_PROMPT,\n",
    "                \"prompt_count\": PROMPT_COUNT,\n",
    "                \"survey\": WHICH_SURVEY,\n",
    "                \"subject\": SUBJECT,\n",
    "                \"prompt_template\": prompt_template,\n",
    "                \"prompt_info\": prompt_info,\n",
    "                **utils.describe_prompts([])\n",
    "            }\n",
    "            bu.quickJSON({\"instructions\": instructions, \"settings\": settings, \"info\": AUTO_INFO}, f\"data/5_monster_prep/batch-schema/2ndgen-dynamic-ctx_{SIM_ID}_prompts.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V4 w/ dynamic WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 50/50\n",
      "Tokens in average chunk group: 5705.08\n",
      "Created 50 prompts.\n",
      "Average prompt size: 5950 tokens.\n",
      "Min prompt size: 4907, Max prompt size: 7709\n",
      "Prompt 50/50\n",
      "Tokens in average chunk group: 5869.26\n",
      "Created 50 prompts.\n",
      "Average prompt size: 6113 tokens.\n",
      "Min prompt size: 5204, Max prompt size: 7466\n",
      "Prompt 50/50\n",
      "Tokens in average chunk group: 5590.34\n",
      "Created 50 prompts.\n",
      "Average prompt size: 5834 tokens.\n",
      "Min prompt size: 4604, Max prompt size: 7878\n",
      "Prompt 50/50\n",
      "Tokens in average chunk group: 5196.0\n",
      "Created 50 prompts.\n",
      "Average prompt size: 5435 tokens.\n",
      "Min prompt size: 4152, Max prompt size: 7068\n",
      "Prompt 50/50\n",
      "Tokens in average chunk group: 4905.16\n",
      "Created 50 prompts.\n",
      "Average prompt size: 5144 tokens.\n",
      "Min prompt size: 4091, Max prompt size: 6549\n",
      "Prompt 50/50\n",
      "Tokens in average chunk group: 4985.74\n",
      "Created 50 prompts.\n",
      "Average prompt size: 5224 tokens.\n",
      "Min prompt size: 3920, Max prompt size: 7062\n",
      "Prompt 50/50\n",
      "Tokens in average chunk group: 4934.98\n",
      "Created 50 prompts.\n",
      "Average prompt size: 5174 tokens.\n",
      "Min prompt size: 4233, Max prompt size: 6357\n",
      "Prompt 50/50\n",
      "Tokens in average chunk group: 5421.62\n",
      "Created 50 prompts.\n",
      "Average prompt size: 5660 tokens.\n",
      "Min prompt size: 4255, Max prompt size: 7682\n",
      "Prompt 50/50\n",
      "Tokens in average chunk group: 5263.64\n",
      "Created 50 prompts.\n",
      "Average prompt size: 5502 tokens.\n",
      "Min prompt size: 4155, Max prompt size: 7071\n"
     ]
    }
   ],
   "source": [
    "CHUNK_SIZES = [75]\n",
    "chunk_size = CHUNK_SIZES[0]\n",
    "OVERLAP_SIZES = [3]\n",
    "overlap_size = OVERLAP_SIZES[0]\n",
    "RETRIAVAL_METHODS = [\"dynamic\"]#, \"hybrid\"]\n",
    "MODELS = [\"llama3:70b\", \"mixtral8x22b\"]\n",
    "max_tokens = [3500, 7000]\n",
    "for max_token in max_tokens:\n",
    "    chunks = []\n",
    "    chunk_token_counts = []\n",
    "    for chat in et.selectedChats.values():\n",
    "        messages = list(chat)  # Convert chat iterator to list for easier slicing\n",
    "        num_messages = len(messages)\n",
    "        for i in range(0, num_messages - chunk_size + 1, chunk_size - overlap_size):\n",
    "            chunk = messages[i:i + chunk_size]  # Extract chunk of messages\n",
    "            chunk_text = \"\\\\n\".join(str(msg) for msg in chunk)  # Concatenate msgs into a single string\n",
    "            chunks.append(chunk_text)  # Append chunk to list of chunks\n",
    "            chunk_token_counts.append(utils.count_tokens(chunk_text))  # Append token count of the chunk\n",
    "    avg_chunk_token_count = sum(chunk_token_counts) / len(chunk_token_counts)\n",
    "    embeddings = []\n",
    "    progress, chunks_len = 0, len(chunks)\n",
    "    for chunk_text in chunks:\n",
    "        progress += 1\n",
    "        print(f\"\\rChunk {progress}/{chunks_len}\", end=\"\")\n",
    "        embedding = ollama.embeddings(model=EMBED_MODEL, prompt=chunk_text)[\"embedding\"]\n",
    "        embeddings.append(embedding)\n",
    "    for MODEL in MODELS:\n",
    "        for RETRIAVAL_METHOD in RETRIAVAL_METHODS:\n",
    "            if RETRIAVAL_METHOD == \"static\":\n",
    "                prompt_embedding = ollama.embeddings(model=EMBED_MODEL, prompt=RETRIEVAL_PROMPT)[\"embedding\"]\n",
    "                max_chunks_count = int((max_tokens / avg_chunk_token_count)-1)\n",
    "                chunks_most_similar_embeddings = utils.find_most_similar(prompt_embedding, embeddings)[:max_chunks_count]\n",
    "                chunks_most_similar = [chunks[embedding[1]] for embedding in chunks_most_similar_embeddings]\n",
    "                tokens_in_chunks = sum(utils.count_tokens(chunk) for chunk in chunks_most_similar)\n",
    "                print(f\"\\nAvg tokens in chunks: {avg_chunk_token_count:.1f} – Chunks in CTX: {max_chunks_count}\")\n",
    "                final_prompts = []\n",
    "                prompt_template = \"\"\"\n",
    "for question in surv.questions:\n",
    "    p = [\n",
    "        systemMsg(\"\\\\n\".join([\n",
    "            f\"You are an expert actor, specializing in impersonation of non-famouns people. You will be presented to the subject through explicit datapoints of their digital footprint. In addition, you will deduct their implicit {SURVEY} by shadowing chats between the subject and friends. You will be asked to fully immerse yourself in the role, and answer questions from the point of view of the persona. \\\\n#Context \\\\n##Chat conversations between the subject and their friends:\\\\n**From most to least related**\\\\n\",\n",
    "            \"\\\\n\\\\nNEW CONVERSATION:\\\\n\".join(chunks_most_similar)\n",
    "        ])),  \n",
    "        assistantMsg(\"Understood. I will answer from the point of view of the persona, based on what I could the deduct from the text provided.\"),\n",
    "        userMsg(\"\\\\n\".join([\n",
    "            f\"Persona is questioned about their {SURVEY} in an {METHOD}. The persona must choose an appropriate answer to the question below with one of these five given options: {', '.join(surv.POSSIBLE_ANSWERS)}. Persona's answer must only contain the chosen option, without any elaboration, nor introduction.\\\\n\\\\n**Your question is:**\\\\n\",\n",
    "            question,\n",
    "            \"\\\\nThe persona chooses:\"\n",
    "        ]))]\n",
    "    final_prompts.append(p)\n",
    "                \"\"\"\n",
    "            elif RETRIAVAL_METHOD == \"dynamic\":\n",
    "                dynamic_retrieval_prompts = list(surv.questions)\n",
    "                dynamic_chunks_most_similar: List[List[str]] = [] \n",
    "                progress = 0\n",
    "                lenn = len(dynamic_retrieval_prompts)\n",
    "                for prompt in dynamic_retrieval_prompts:\n",
    "                    progress += 1\n",
    "                    print(f\"\\rPrompt {progress}/{lenn}\", end=\"\")\n",
    "                    prompt_embedding = ollama.embeddings(model=EMBED_MODEL, prompt=prompt)[\"embedding\"]\n",
    "                    max_chunks_count = int((max_tokens / avg_chunk_token_count)-1)\n",
    "                    chunks_most_similar_embeddings = utils.find_most_similar(prompt_embedding, embeddings)[:max_chunks_count]\n",
    "                    chunks_most_similar = [chunks[embedding[1]] for embedding in chunks_most_similar_embeddings]\n",
    "                    tokens_in_chunks = sum(utils.count_tokens(chunk) for chunk in chunks_most_similar)\n",
    "                    dynamic_chunks_most_similar.append(chunks_most_similar)\n",
    "                print(end=\"\\n\")\n",
    "                tokens_in_chunks = 0\n",
    "                for chunks_most_similar in dynamic_chunks_most_similar:\n",
    "                    for chunk in chunks_most_similar:\n",
    "                        tokens_in_chunks += utils.count_tokens(chunk)\n",
    "                del chunks_most_similar_embeddings  # free memory\n",
    "                print(f\"Tokens in average chunk group: {tokens_in_chunks/len(dynamic_chunks_most_similar)}\")\n",
    "                final_prompts = []\n",
    "                prompt_template = \"\"\"\n",
    "for question, chunks_most_similar in zip(surv.questions, dynamic_chunks_most_similar):\n",
    "    p = [\n",
    "        systemMsg(\"\\\\n\".join([\n",
    "            f\"You are an expert actor, specializing in impersonation of non-famouns people. You will be presented to the subject through explicit datapoints of their digital footprint. In addition, you will deduct their implicit {SURVEY} by shadowing chats between the subject and friends. You will be asked to fully immerse yourself in the role, and answer questions from the point of view of the persona. \\\\n#Context \\\\n##Chat conversations between the subject and their friends:\\\\n**From most to least related**\\\\n\",\n",
    "            \"\\\\n\\\\nNEW CONVERSATION:\\\\n\".join(chunks_most_similar)\n",
    "        ])),      \n",
    "        assistantMsg(\"Understood. I will answer from the point of view of the persona, based on what I could the deduct from the text provided.\"),\n",
    "        userMsg(\"\\\\n\".join([\n",
    "            f\"Persona is questioned about their {SURVEY} in an {METHOD}. The persona must choose an appropriate answer to the question below with one of these five given options: {', '.join(surv.POSSIBLE_ANSWERS)}. Persona's answer must only contain the chosen option, without any elaboration, nor introduction.\\\\n\\\\n**Your question is:**\\\\n\",\n",
    "            question,\n",
    "            \"\\\\nThe persona chooses:\"\n",
    "        ]))]\n",
    "    final_prompts.append(p)\n",
    "                \"\"\"\n",
    "            else: print(\"neither static, nor dynamic\")\n",
    "            exec(prompt_template)\n",
    "            # prompt_info = utils.describe_prompts_and_print(final_prompts) # Vanity print\n",
    "            SIM_ID = f\"{SUBJECT}-{WHICH_SURVEY}-{RETRIAVAL_METHOD}-{chunk_size}-{str(overlap_size).zfill(2)}-{str(max_chunks_count).zfill(2)}_V5_{MODEL}\"\n",
    "            bu.quickJSON(final_prompts, f\"data/5_monster_prep/{SIM_ID}_prompts.json\")\n",
    "            instructions = {\n",
    "                \"prompt_file\": f\"batch/prompts/{SIM_ID}_prompts.json\",\n",
    "                \"survey_type\": f\"{SURVEY_TYPE[0]}\",\n",
    "                \"isLocal\": True,\n",
    "                \"LIMIT\": None\n",
    "            }\n",
    "            settings = {\n",
    "                \"model\": MODEL,\n",
    "                \"timeout\": 300}\n",
    "            AUTO_INFO = {\n",
    "                \"CHUNK_SIZE\": chunk_size,\n",
    "                \"OVERLAP_SIZE\": overlap_size,\n",
    "                \"tokens_in_chunks\": tokens_in_chunks,\n",
    "                \"CTX_limit\": max_token,\n",
    "                \"model\": EMBED_MODEL,\n",
    "                \"prompt method\": PROMPT_METHOD,\n",
    "                \"retrieval method\": RETRIAVAL_METHOD,\n",
    "                \"retrieval prompt\": RETRIEVAL_PROMPT,\n",
    "                \"prompt_count\": PROMPT_COUNT,\n",
    "                \"survey\": WHICH_SURVEY,\n",
    "                \"subject\": SUBJECT,\n",
    "                \"prompt_template\": prompt_template,\n",
    "                \"CHUNKS_COUNT_IN_CTX\": max_chunks_count,#chunks_count_in_ctx,\n",
    "                **utils.describe_prompts(final_prompts)\n",
    "            }\n",
    "            bu.quickJSON({\"instructions\": instructions, \"settings\": settings, \"info\": AUTO_INFO}, f\"data/5_monster_prep/batch-schema/{SIM_ID}_schema.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V5/6 - 32k models w/ timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 50/50\n",
      "Tokens in average chunk group: 2642.48\n",
      "Created 50 prompts.\n",
      "Average prompt size: 2858 tokens.\n",
      "Min prompt size: 2665, Max prompt size: 4963\n",
      "Prompt 50/50\n",
      "Tokens in average chunk group: 2638.7450980392155\n",
      "Created 50 prompts.\n",
      "Average prompt size: 5311 tokens.\n",
      "Min prompt size: 5118, Max prompt size: 7416\n"
     ]
    }
   ],
   "source": [
    "CHUNK_SIZES = [250]\n",
    "MODELS = [\"mixtral:8x22b-instruct-v0.1-q2_K\"]\n",
    "# OVERLAP_SIZES = [5]\n",
    "overlap_size = 5\n",
    "RETRIAVAL_METHODS = [\"dynamic\", \"hybrid\"]\n",
    "max_tokens = 10000\n",
    "for chunk_size in CHUNK_SIZES:\n",
    "    for MODEL in MODELS:\n",
    "        chunks = []\n",
    "        chunk_token_counts = []\n",
    "        for chat in et.selectedChats.values():\n",
    "            messages = list(chat)  # Convert chat iterator to list for easier slicing\n",
    "            num_messages = len(messages)\n",
    "            for i in range(0, num_messages - chunk_size + 1, chunk_size - overlap_size):\n",
    "                chunk = messages[i:i + chunk_size]  # Extract chunk of messages\n",
    "                chunk_text = \"\\\\n\".join(str(msg) for msg in chunk)  # Concatenate msgs into a single string\n",
    "                chunks.append(chunk_text)  # Append chunk to list of chunks\n",
    "                chunk_token_counts.append(utils.count_tokens(chunk_text))  # Append token count of the chunk\n",
    "        avg_chunk_token_count = sum(chunk_token_counts) / len(chunk_token_counts)\n",
    "        embeddings = []\n",
    "        progress, chunks_len = 0, len(chunks)\n",
    "        for chunk_text in chunks:\n",
    "            progress += 1\n",
    "            print(f\"\\rChunk {progress}/{chunks_len}\", end=\"\")\n",
    "            embedding = ollama.embeddings(model=EMBED_MODEL, prompt=chunk_text)[\"embedding\"]\n",
    "            embeddings.append(embedding)\n",
    "        for RETRIAVAL_METHOD in RETRIAVAL_METHODS:\n",
    "            if RETRIAVAL_METHOD == \"dynamic\":\n",
    "                dynamic_retrieval_prompts = list(surv.questions)\n",
    "                dynamic_chunks_most_similar: List[List[str]] = [] \n",
    "                progress = 0\n",
    "                lenn = len(dynamic_retrieval_prompts)\n",
    "                for prompt in dynamic_retrieval_prompts:\n",
    "                    progress += 1\n",
    "                    print(f\"\\rPrompt {progress}/{lenn}\", end=\"\")\n",
    "                    prompt_embedding = ollama.embeddings(model=EMBED_MODEL, prompt=prompt)[\"embedding\"]\n",
    "                    max_chunks_count = int((max_tokens / avg_chunk_token_count)-1)\n",
    "                    chunks_most_similar_embeddings = utils.find_most_similar(prompt_embedding, embeddings)[:max_chunks_count]\n",
    "                    chunks_most_similar = [chunks[embedding[1]] for embedding in chunks_most_similar_embeddings]\n",
    "                    tokens_in_chunks = sum(utils.count_tokens(chunk) for chunk in chunks_most_similar)\n",
    "                    dynamic_chunks_most_similar.append(chunks_most_similar)\n",
    "                print(end=\"\\n\")\n",
    "                tokens_in_chunks = 0\n",
    "                for chunks_most_similar in dynamic_chunks_most_similar:\n",
    "                    for chunk in chunks_most_similar:\n",
    "                        tokens_in_chunks += utils.count_tokens(chunk)\n",
    "                del chunks_most_similar_embeddings  # free memory\n",
    "                print(f\"Tokens in average chunk group: {tokens_in_chunks/len(dynamic_chunks_most_similar)}\")\n",
    "                final_prompts = []\n",
    "                prompt_template = \"\"\"\n",
    "for question, chunks_most_similar in zip(surv.questions, dynamic_chunks_most_similar):\n",
    "    p = [\n",
    "        systemMsg(\"\\\\n\".join([\n",
    "            f\"You are an expert actor, specializing in impersonation of non-famouns people. You will be presented to the subject through explicit datapoints of their digital footprint. In addition, you will deduct their implicit {SURVEY} by shadowing chats between the subject and friends. You will be asked to fully immerse yourself in the role, and answer questions from the point of view of the persona. \\\\n#Context \\\\n##Chat conversations between the subject and their friends:\\\\n**From most to least related**\\\\n\",\n",
    "            \"\\\\n\\\\nNEW CONVERSATION:\\\\n\".join(chunks_most_similar)\n",
    "        ])),      \n",
    "        assistantMsg(\"Understood. I will answer from the point of view of the persona, based on what I could the deduct from the text provided.\"),\n",
    "        userMsg(\"\\\\n\".join([\n",
    "            f\"Persona is questioned about their {SURVEY} in an {METHOD}. The persona must choose an appropriate answer to the question below with one of these five given options: {', '.join(surv.POSSIBLE_ANSWERS)}. Persona's answer must only contain the chosen option, without any elaboration, nor introduction.\\\\n\\\\n**Your question is:**\\\\n\",\n",
    "            question,\n",
    "            \"\\\\nThe persona chooses:\"\n",
    "        ]))]\n",
    "    final_prompts.append(p)\n",
    "                \"\"\"\n",
    "            elif RETRIAVAL_METHOD == \"hybrid\":\n",
    "                prompt_embedding = ollama.embeddings(model=EMBED_MODEL, prompt=RETRIEVAL_PROMPT)[\"embedding\"]\n",
    "                max_chunks_count = int((max_tokens / avg_chunk_token_count))\n",
    "                chunks_most_similar_embeddings_static = utils.find_most_similar(prompt_embedding, embeddings)[:max_chunks_count // 2]\n",
    "                chunks_most_similar_static = [chunks[embedding[1]] for embedding in chunks_most_similar_embeddings_static]\n",
    "\n",
    "                dynamic_retrieval_prompts = list(surv.questions)\n",
    "                dynamic_chunks_most_similar: List[List[str]] = []\n",
    "                progress = 0\n",
    "                lenn = len(dynamic_retrieval_prompts)\n",
    "                for prompt in dynamic_retrieval_prompts:\n",
    "                    progress += 1\n",
    "                    print(f\"\\rPrompt {progress}/{lenn}\", end=\"\")\n",
    "                    prompt_embedding = ollama.embeddings(model=EMBED_MODEL, prompt=prompt)[\"embedding\"]\n",
    "                    chunks_most_similar_embeddings_dynamic = utils.find_most_similar(prompt_embedding, embeddings)[:max_chunks_count // 2]\n",
    "                    chunks_most_similar_dynamic = [chunks[embedding[1]] for embedding in chunks_most_similar_embeddings_dynamic]\n",
    "                    dynamic_chunks_most_similar.append(chunks_most_similar_dynamic)\n",
    "                print(end=\"\\n\")\n",
    "\n",
    "                chunks_most_similar = chunks_most_similar_static + [chunk for sublist in dynamic_chunks_most_similar for chunk in sublist]\n",
    "                tokens_in_chunks = sum(utils.count_tokens(chunk) for chunk in chunks_most_similar)\n",
    "                del chunks_most_similar_embeddings_static, chunks_most_similar_embeddings_dynamic  # free memory\n",
    "\n",
    "                print(f\"Tokens in average chunk group: {tokens_in_chunks / len(chunks_most_similar)}\")\n",
    "                final_prompts = []\n",
    "                prompt_template = \"\"\"\n",
    "for question, chunks_most_similar_dynamic in zip(surv.questions, dynamic_chunks_most_similar):\n",
    "    p = [\n",
    "        systemMsg(\"\\\\n\".join([\n",
    "            f\"You are an expert actor, specializing in impersonation of non-famouns people. You will be presented to the subject through explicit datapoints of their digital footprint. In addition, you will deduct their implicit {SURVEY} by shadowing chats between the subject and friends. You will be asked to fully immerse yourself in the role, and answer questions from the point of view of the persona. \\\\n#Context \\\\n##Chat conversations between the subject and their friends:\\\\n**From most to least related**\\\\n\",\n",
    "            \"\\\\n\\\\nNEW CONVERSATION RELATED TO THE SURVEY OVERALL:\\\\n\".join(chunks_most_similar_static),\n",
    "            \"\\\\n\\\\nNEW CONVERSATION RELATED TO THE PARTICULAR QUESTION:\\\\n\".join(chunks_most_similar_dynamic)\n",
    "        ])),      \n",
    "        assistantMsg(\"Understood. I will answer from the point of view of the persona, based on what I could the deduct from the text provided.\"),\n",
    "        userMsg(\"\\\\n\".join([\n",
    "            f\"Persona is questioned about their {SURVEY} in an {METHOD}. The persona must choose an appropriate answer to the question below with one of these five given options: {', '.join(surv.POSSIBLE_ANSWERS)}. Persona's answer must only contain the chosen option, without any elaboration, nor introduction.\\\\n\\\\n**Your question is:**\\\\n\",\n",
    "            question,\n",
    "            \"\\\\nThe persona chooses:\"\n",
    "        ]))]\n",
    "    final_prompts.append(p)\n",
    "                \"\"\"\n",
    "            else: print(\"neither hybrid, nor dynamic\")\n",
    "            exec(prompt_template)\n",
    "            prompt_info = utils.describe_prompts_and_print(final_prompts) # Vanity print\n",
    "            SIM_ID = f\"{SUBJECT}-{WHICH_SURVEY}-{RETRIAVAL_METHOD}-{chunk_size}-{str(overlap_size).zfill(2)}-{str(max_chunks_count).zfill(2)}_{MODEL}_V6\"\n",
    "            bu.quickJSON(final_prompts, f\"data/5_monster_prep/{SIM_ID}_prompts.json\")\n",
    "            instructions = {\n",
    "                \"prompt_file\": f\"batch/prompts/{SIM_ID}_prompts.json\",\n",
    "                \"survey_type\": f\"{SURVEY_TYPE[0]}\",\n",
    "                \"isLocal\": True,\n",
    "                \"LIMIT\": None\n",
    "            }\n",
    "            settings = {\n",
    "                \"model\": MODEL,\n",
    "                \"timeout\": 300}\n",
    "            AUTO_INFO = {\n",
    "                \"CHUNK_SIZE\": chunk_size,\n",
    "                \"OVERLAP_SIZE\": overlap_size,\n",
    "                \"CHUNKS_COUNT_IN_CTX\": max_chunks_count,#chunks_count_in_ctx,\n",
    "                \"CTX_limit\": max_tokens,\n",
    "                \"tokens_in_chunks\": tokens_in_chunks,\n",
    "                \"model\": EMBED_MODEL,\n",
    "                \"prompt method\": PROMPT_METHOD,\n",
    "                \"retrieval method\": RETRIAVAL_METHOD,\n",
    "                \"retrieval prompt\": RETRIEVAL_PROMPT,\n",
    "                \"prompt_count\": PROMPT_COUNT,\n",
    "                \"survey\": WHICH_SURVEY,\n",
    "                \"subject\": SUBJECT,\n",
    "                \"prompt_template\": prompt_template,\n",
    "                **prompt_info,\n",
    "                **utils.describe_prompts([])\n",
    "            }\n",
    "            bu.quickJSON({\"instructions\": instructions, \"settings\": settings, \"info\": AUTO_INFO}, f\"data/5_monster_prep/batch-schema/{SIM_ID}_schema.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Display Info\n",
    "total_messages = sum(len(chat) for chat in et.selectedChats.values())\n",
    "chunks_count = len(chunks)\n",
    "avg_chunk_char_len = np.mean([len(chunk) for chunk in chunks])\n",
    "print(\n",
    "    f\"Chunk count: {chunks_count}\",\n",
    "    f\"Rough estimate of tokens per chunk: {round(avg_chunk_char_len / 4)} (4 characters per token)\",\n",
    "    f\"Messagees in input count: {total_messages}\",\n",
    "    f\"Messages in chunks count: {stat_total_msgs_in_chunks}\",\n",
    "    f\"Chunk \\ Input ratio: {round(stat_total_msgs_in_chunks / total_messages,2)} (OVERLAP_SIZE={OVERLAP_SIZE})\",\n",
    "    f\"Chunk Python type: {type(chunks[0])}\",\n",
    "    sep=\"\\n\"\n",
    ") \n",
    "bu.if_dir_not_exist_make(\"data/3_embeddings\")\n",
    "bu.quickJSON(AUTO_INFO, f\"data/3_embeddings/{EMBEDDING_ID}_info.json\")\n",
    "bu.quickJSON({\"chunks\": chunks, \"embeddings\": embeddings}, f\"data/3_embeddings/{EMBEDDING_ID}_embeddings.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Tokens in chunks: {tokens_in_chunks}\")\n",
    "print(f\"Chunks:{len(chunks)}, embeds:{len(embeddings)}\")\n",
    "\n",
    "# bu.quickTXT(\"\\n\\n\".join(chunks_most_similar), filename=f\"data/4_chunks/{CHECKPOINT}-static_chunks.txt\")\n",
    "# bu.if_dir_not_exist_make(\"data/4_chunks\")\n",
    "# bu.quickJSON(AUTO_INFO, f\"data/4_chunks/{CHECKPOINT}-static_info.json\")\n",
    "# bu.quickJSON({\"chunks\": chunks, \"embeddings\": embeddings}, f\"data/4_chunks/{CHECKPOINT}-static_embeddings.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "# bu.if_dir_not_exist_make(\"data/4_chunks\")\n",
    "# bu.quickJSON(AUTO_INFO, f\"data/4_chunks/{CHECKPOINT}-dynamic_info.json\")\n",
    "# bu.quickJSON({\"chunks\": chunks, \"embeddings\": embeddings}, f\"data/4_chunks/{CHECKPOINT}-dynamic_embeddings.json\")\n",
    "############################################ VANITY BELOW ########################################\n",
    "tokens_in_chunks = 0\n",
    "for chunks_most_similar in dynamic_chunks_most_similar:\n",
    "    for chunk in chunks_most_similar:\n",
    "        tokens_in_chunks += utils.count_tokens(chunk)\n",
    "\n",
    "del chunks_most_similar_embeddings # free memory\n",
    "print(f\"Tokens in average chunk group: {tokens_in_chunks/len(dynamic_chunks_most_similar)}\")\n",
    "# bu.quickJSON(dynamic_chunks_most_similar, filename=f\"data/4_chunks/{CHECKPOINT}-dynamic_chunks.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################### Method A ############################################\n",
    "pre_final_prompts = []\n",
    "if PROMPT_METHOD == \"IMPERSONATE\":\n",
    "    pre_prompt_template = \"\"\"\n",
    "    SYS_MSG = {\n",
    "        \"role\": \"system\", \n",
    "        \"content\": f\"You are an expert actor, specializing in impersonation of non-famouns people. You will be presented to the subject through explicit datapoints of their digital footprint. In addition, you will deduct their implicit {SURVEY} by shadowing chats between the subject and friends. You will be asked to fully immerse yourself in the role, and answer questions from the point of view of the persona. \\\\n\\\\n**The persona, which you will be tasked to mimick is named '{SUBJECT}'.** \\\\n#Context \\\\n##Chat conversations between the subject and their friends:\\\\n**From most to least related**\\\\n\"\n",
    "    }\n",
    "    ASSIST_MSG = {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Understood. I will answer from the point of view of the persona, {subject}, based on what I could the deduct from the text provided.\"\n",
    "    }\n",
    "    USER_MSG = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Persona is questioned about their {SURVEY} in an {METHOD}. The persona must choose an appropriate answer to the question below with one of these five given options: {', '.join(surv.POSSIBLE_ANSWERS)}. Persona's answer must only contain the chosen option, without any elaboration, nor introduction.\"\n",
    "    }\n",
    "    pre_final_prompts.append(SYS_MSG, ASSIST_MSG, USER_MSG)\n",
    "    \"\"\"\n",
    "    exec(pre_prompt_template)\n",
    "\n",
    "########################################### Method B ###########################################\n",
    "elif PROMPT_METHOD == \"ARE\":\n",
    "    pre_prompt_template = \"\"\"\n",
    "    SYS_MSG = {\n",
    "        \"role\": \"system\", \n",
    "        \"content\": f\"**{TINY_MODULE}**. You have shared your thoughts, feelings, and experiences through text messages with friedns. Answer the following questions honestly and naturally, as you would in everyday conversations. \\\\n\\\\n#Context \\\\n##Conversations between persona and friends:\"\n",
    "    }\n",
    "    ASSIST_MSG = {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": f\"Understood. I am {SUBJECT}, and I will answer the survey to the best of my ability.\"\n",
    "    }\n",
    "    USER_MSG = {   \n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"The survey is about your {SURVEY}. You must choose an appropriate answer to the question below with one of these five given options: {', '.join(surv.POSSIBLE_ANSWERS)}. Your answer must only contain the chosen option, without any elaboration, nor introduction.\\\\n**From most to least related**\\\\n\"\n",
    "    }\n",
    "    \"\"\"\n",
    "exec(pre_prompt_template)\n",
    "\n",
    "print(f\"{SYS_MSG['content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BASE\n",
    "base_final_prompts = []\n",
    "prompt_template = \"\"\"\n",
    "for question in surv.questions:\n",
    "    p = [\n",
    "        systemMsg(\n",
    "            \"You are participating in a survey. You will be presented with a series of questions about your {SURVEY}.\",\n",
    "            f\"You must choose answer to the question below with one of the five options: {', '.join(surv.POSSIBLE_ANSWERS)}. The answer must only contain the chosen option. \"\n",
    "        ),\n",
    "        assistantMsg('Understood. I will answer the question below with one of the given options.'),\n",
    "        userMsg(\n",
    "            question,\n",
    "            \"Your choice: \"\n",
    "        )]\n",
    "    base_final_prompts.append(p)\n",
    "\"\"\"\n",
    "exec(prompt_template)\n",
    "# prompt_info = utils.describe_prompts_and_print(base_final_prompts) # Vanity print\n",
    "# bu.quickJSON(base_final_prompts, f\"data/5_prep/{WHICH_SURVEY}_base_prompts.json\")\n",
    "\n",
    "utils.count_tokens(prompt_template)\n",
    "#Static\n",
    "# static_final_prompts = []\n",
    "# prompt_template = \"\"\"\n",
    "# for question in surv.questions:\n",
    "#     p = [\n",
    "#         systemMsg(\"\\\\n\".join([\n",
    "#             f\"You are an expert actor, specializing in impersonation of non-famouns people. You will be presented to the subject through explicit datapoints of their digital footprint. In addition, you will deduct their implicit {SURVEY} by shadowing chats between the subject and friends. You will be asked to fully immerse yourself in the role, and answer questions from the point of view of the persona. \\\\n\\\\n**The persona, which you will be tasked to mimick is named '{SUBJECT}'.** \\\\n#Context \\\\n##Chat conversations between the subject and their friends:\\\\n**From most to least related**\\\\n\",\n",
    "#             \"\\\\nNEW CONVERSATION:\\\\n\".join(chunks_most_similar)\n",
    "#         ])),  \n",
    "#         assistantMsg(\"Understood. I will answer from the point of view of the persona, {subject}, based on what I could the deduct from the text provided.\"),\n",
    "#         userMsg(\"\\\\n\".join([\n",
    "#             f\"Persona is questioned about their {SURVEY} in an {METHOD}. The persona must choose an appropriate answer to the question below with one of these five given options: {', '.join(surv.POSSIBLE_ANSWERS)}. Persona's answer must only contain the chosen option, without any elaboration, nor introduction.\\\\n\\\\n**Your question is:**\\\\n\\\\n\",\n",
    "#             question,\n",
    "#             \"\\\\nThe persona chooses:\"\n",
    "#         ]))]\n",
    "#     static_final_prompts.append(p)\n",
    "# \"\"\"\n",
    "# exec(prompt_template)\n",
    "# prompt_info = utils.describe_prompts_and_print(static_final_prompts) # Vanity print\n",
    "# bu.quickJSON(final_prompts, f\"data/5_prep/{PREP_CHECKPOINT}_prompts.json\")\n",
    "# # print(f\"{len(static_final_prompts)}\")#,{final_prompts[:1]}\")\n",
    "\n",
    "# #Dynamic\n",
    "# dynamic_final_prompts = []\n",
    "# prompt_template = \"\"\"\n",
    "# for question, chunks_most_similar in zip(surv.questions, dynamic_chunks_most_similar):\n",
    "#     p = [\n",
    "#         systemMsg(\"\\\\n\".join([\n",
    "#             f\"{SYS_MSG['content']}\",\n",
    "#             \"\\\\nNEW CONVERSATION:\\\\n\".join(chunks_most_similar)\n",
    "#         ])),      \n",
    "#         assistantMsg(ASSIST_MSG['content']),\n",
    "#         userMsg(\"\\\\n\".join([\n",
    "#             f\"{USER_MSG['content']}\\\\n\\\\n**Your question is:**\\\\n\\\\n\",\n",
    "#             question,\n",
    "#             \"\\\\nThe persona chooses:\"\n",
    "#         ]))]\n",
    "#     dynamic_final_prompts.append(p)\n",
    "# \"\"\"    \n",
    "# exec(prompt_template)\n",
    "# prompt_info = utils.describe_prompts_and_print(dynamic_final_prompts)\n",
    "# bu.quickJSON(dynamic_final_prompts, f\"data/5_prep/{PREP_CHECKPOINT}_prompts.json\")\n",
    "# # print(f\"{len(dynamic_final_prompts)}\")#,{final_prompts[:1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{SURVEY_TYPE[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Set the directory where the JSON files are located\n",
    "directory = 'batch/done/monster_7b'  # Replace with the actual directory path if needed\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    # Check if the file has a .json extension\n",
    "    if filename.endswith('.json'):\n",
    "        # Open the JSON file\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # Modify the \"model\" value\n",
    "        if \"settings\" in data and \"model\" in data[\"settings\"]:\n",
    "            data[\"settings\"][\"model\"] = \"llama3\"\n",
    "\n",
    "        # Write the modified data back to the file\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default Personality Survey CSV file: surveys/survey_personality-test.csv\n"
     ]
    }
   ],
   "source": [
    "surv = survey.PersonalitySurvey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>airidas</th>\n",
       "      <th>elias</th>\n",
       "      <th>format</th>\n",
       "      <th>category</th>\n",
       "      <th>retrieval_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am the life of the party.</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>EXT1</td>\n",
       "      <td>engaged or not engaged in parties</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I don't talk a lot.</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>EXT2</td>\n",
       "      <td>talking or not talking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I feel comfortable around people.</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>EXT3</td>\n",
       "      <td>comfort or discomfort around people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I keep in the background.</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>EXT4</td>\n",
       "      <td>staying or not staying in the background</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I start conversations.</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>EXT5</td>\n",
       "      <td>starting or not starting conversations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I have little to say.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>EXT6</td>\n",
       "      <td>having little or a lot to say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I talk to a lot of different people at parties.</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>EXT7</td>\n",
       "      <td>talking to a lot or few people at parties</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I don't like to draw attention to myself.</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>EXT8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I don't mind being the center of attention.</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>EXT9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I am quiet around strangers.</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>EXT10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I get stressed out easily.</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>EST1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I am relaxed most of the time.</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>EST2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I worry about things.</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>EST3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>I seldom feel blue.</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>EST4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>I am easily disturbed.</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>EST5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>I get upset easily.</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>EST6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>I change my mood a lot.</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>EST7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>I have frequent mood swings.</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>EST8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>I get irritated easily.</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>EST9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>I often feel blue.</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>EST10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>I feel little concern for others.</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>AGR1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>I am interested in people.</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>AGR2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>I insult people.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>AGR3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>I sympathize with others' feelings.</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>AGR4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>I am not interested in other people's problems.</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>AGR5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>I have a soft heart.</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>AGR6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>I am not really interested in others.</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>AGR7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>I take time out for others.</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>AGR8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>I feel others' emotions.</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>AGR9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>I make people feel at ease.</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>AGR10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>I am always prepared.</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>CSN1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>I leave my belongings around.</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>CSN2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>I pay attention to details.</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>CSN3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>I make a mess of things.</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>CSN4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>I get chores done right away.</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>CSN5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>I often forget to put things back in their pro...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>CSN6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>I like order.</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>CSN7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>I shirk my duties.</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>CSN8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>I follow a schedule.</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>CSN9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>I am exacting in my work.</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>CSN10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>I have a rich vocabulary.</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>OPN1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>I have difficulty understanding abstract ideas.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>OPN2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>I have a vivid imagination.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>OPN3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>I am not interested in abstract ideas.</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>OPN4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>I have excellent ideas.</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>OPN5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>I do not have a good imagination.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>OPN6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>I am quick to understand things.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>OPN7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>I use difficult words.</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>OPN8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>I spend time reflecting on things.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>OPN9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>I am full of ideas.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>A (1-5: 1=Disagree, 3=Neutral, 5=Agree)</td>\n",
       "      <td>OPN10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  airidas  elias  \\\n",
       "0                         I am the life of the party.        3      3   \n",
       "1                                 I don't talk a lot.        2      2   \n",
       "2                   I feel comfortable around people.        4      4   \n",
       "3                           I keep in the background.        2      4   \n",
       "4                              I start conversations.        4      3   \n",
       "5                               I have little to say.        1      1   \n",
       "6     I talk to a lot of different people at parties.        5      4   \n",
       "7           I don't like to draw attention to myself.        2      3   \n",
       "8         I don't mind being the center of attention.        4      1   \n",
       "9                        I am quiet around strangers.        1      4   \n",
       "10                         I get stressed out easily.        4      3   \n",
       "11                     I am relaxed most of the time.        1      4   \n",
       "12                              I worry about things.        3      3   \n",
       "13                                I seldom feel blue.        3      3   \n",
       "14                             I am easily disturbed.        1      5   \n",
       "15                                I get upset easily.        2      2   \n",
       "16                            I change my mood a lot.        4      1   \n",
       "17                       I have frequent mood swings.        4      2   \n",
       "18                            I get irritated easily.        3      3   \n",
       "19                                 I often feel blue.        1      3   \n",
       "20                  I feel little concern for others.        2      2   \n",
       "21                         I am interested in people.        4      5   \n",
       "22                                   I insult people.        1      1   \n",
       "23                I sympathize with others' feelings.        4      4   \n",
       "24    I am not interested in other people's problems.        1      4   \n",
       "25                               I have a soft heart.        4      5   \n",
       "26              I am not really interested in others.        4      2   \n",
       "27                        I take time out for others.        2      5   \n",
       "28                           I feel others' emotions.        2      4   \n",
       "29                        I make people feel at ease.        2      4   \n",
       "30                              I am always prepared.        4      1   \n",
       "31                      I leave my belongings around.        2      5   \n",
       "32                        I pay attention to details.        2      3   \n",
       "33                           I make a mess of things.        2      4   \n",
       "34                      I get chores done right away.        1      2   \n",
       "35  I often forget to put things back in their pro...        2      2   \n",
       "36                                      I like order.        4      4   \n",
       "37                                 I shirk my duties.        2      1   \n",
       "38                               I follow a schedule.        4      2   \n",
       "39                          I am exacting in my work.        3      5   \n",
       "40                          I have a rich vocabulary.        3      4   \n",
       "41    I have difficulty understanding abstract ideas.        1      1   \n",
       "42                        I have a vivid imagination.        5      5   \n",
       "43             I am not interested in abstract ideas.        2      1   \n",
       "44                            I have excellent ideas.        5      4   \n",
       "45                  I do not have a good imagination.        1      1   \n",
       "46                   I am quick to understand things.        5      5   \n",
       "47                             I use difficult words.        1      4   \n",
       "48                 I spend time reflecting on things.        5      5   \n",
       "49                                I am full of ideas.        5      5   \n",
       "\n",
       "                                     format category  \\\n",
       "0   A (1-5: 1=Disagree, 3=Neutral, 5=Agree)     EXT1   \n",
       "1   A (1-5: 1=Disagree, 3=Neutral, 5=Agree)     EXT2   \n",
       "2   A (1-5: 1=Disagree, 3=Neutral, 5=Agree)     EXT3   \n",
       "3   A (1-5: 1=Disagree, 3=Neutral, 5=Agree)     EXT4   \n",
       "4   A (1-5: 1=Disagree, 3=Neutral, 5=Agree)     EXT5   \n",
       "5   A (1-5: 1=Disagree, 3=Neutral, 5=Agree)     EXT6   \n",
       "6   A (1-5: 1=Disagree, 3=Neutral, 5=Agree)     EXT7   \n",
       "7   A (1-5: 1=Disagree, 3=Neutral, 5=Agree)     EXT8   \n",
       "8   A (1-5: 1=Disagree, 3=Neutral, 5=Agree)     EXT9   \n",
       "9   A (1-5: 1=Disagree, 3=Neutral, 5=Agree)    EXT10   \n",
       "10  A (1-5: 1=Disagree, 3=Neutral, 5=Agree)     EST1   \n",
       "11  A (1-5: 1=Disagree, 3=Neutral, 5=Agree)     EST2   \n",
       "12  A (1-5: 1=Disagree, 3=Neutral, 5=Agree)     EST3   \n",
       "13  A (1-5: 1=Disagree, 3=Neutral, 5=Agree)     EST4   \n",
       "14  A (1-5: 1=Disagree, 3=Neutral, 5=Agree)     EST5   \n",
       "15  A (1-5: 1=Disagree, 3=Neutral, 5=Agree)     EST6   \n",
       "16  A (1-5: 1=Disagree, 3=Neutral, 5=Agree)     EST7   \n",
       "17  A (1-5: 1=Disagree, 3=Neutral, 5=Agree)     EST8   \n",
       "18  A (1-5: 1=Disagree, 3=Neutral, 5=Agree)     EST9   \n",
       "19  A (1-5: 1=Disagree, 3=Neutral, 5=Agree)    EST10   \n",
       "20  A (1-5: 1=Disagree, 3=Neutral, 5=Agree)     AGR1   \n",
       "21  A (1-5: 1=Disagree, 3=Neutral, 5=Agree)     AGR2   \n",
       "22  A (1-5: 1=Disagree, 3=Neutral, 5=Agree)     AGR3   \n",
       "23  A (1-5: 1=Disagree, 3=Neutral, 5=Agree)     AGR4   \n",
       "24  A (1-5: 1=Disagree, 3=Neutral, 5=Agree)     AGR5   \n",
       "25  A (1-5: 1=Disagree, 3=Neutral, 5=Agree)     AGR6   \n",
       "26  A (1-5: 1=Disagree, 3=Neutral, 5=Agree)     AGR7   \n",
       "27  A (1-5: 1=Disagree, 3=Neutral, 5=Agree)     AGR8   \n",
       "28  A (1-5: 1=Disagree, 3=Neutral, 5=Agree)     AGR9   \n",
       "29  A (1-5: 1=Disagree, 3=Neutral, 5=Agree)    AGR10   \n",
       "30  A (1-5: 1=Disagree, 3=Neutral, 5=Agree)     CSN1   \n",
       "31  A (1-5: 1=Disagree, 3=Neutral, 5=Agree)     CSN2   \n",
       "32  A (1-5: 1=Disagree, 3=Neutral, 5=Agree)     CSN3   \n",
       "33  A (1-5: 1=Disagree, 3=Neutral, 5=Agree)     CSN4   \n",
       "34  A (1-5: 1=Disagree, 3=Neutral, 5=Agree)     CSN5   \n",
       "35  A (1-5: 1=Disagree, 3=Neutral, 5=Agree)     CSN6   \n",
       "36  A (1-5: 1=Disagree, 3=Neutral, 5=Agree)     CSN7   \n",
       "37  A (1-5: 1=Disagree, 3=Neutral, 5=Agree)     CSN8   \n",
       "38  A (1-5: 1=Disagree, 3=Neutral, 5=Agree)     CSN9   \n",
       "39  A (1-5: 1=Disagree, 3=Neutral, 5=Agree)    CSN10   \n",
       "40  A (1-5: 1=Disagree, 3=Neutral, 5=Agree)     OPN1   \n",
       "41  A (1-5: 1=Disagree, 3=Neutral, 5=Agree)     OPN2   \n",
       "42  A (1-5: 1=Disagree, 3=Neutral, 5=Agree)     OPN3   \n",
       "43  A (1-5: 1=Disagree, 3=Neutral, 5=Agree)     OPN4   \n",
       "44  A (1-5: 1=Disagree, 3=Neutral, 5=Agree)     OPN5   \n",
       "45  A (1-5: 1=Disagree, 3=Neutral, 5=Agree)     OPN6   \n",
       "46  A (1-5: 1=Disagree, 3=Neutral, 5=Agree)     OPN7   \n",
       "47  A (1-5: 1=Disagree, 3=Neutral, 5=Agree)     OPN8   \n",
       "48  A (1-5: 1=Disagree, 3=Neutral, 5=Agree)     OPN9   \n",
       "49  A (1-5: 1=Disagree, 3=Neutral, 5=Agree)    OPN10   \n",
       "\n",
       "                                  retrieval_1  \n",
       "0           engaged or not engaged in parties  \n",
       "1                      talking or not talking  \n",
       "2         comfort or discomfort around people  \n",
       "3    staying or not staying in the background  \n",
       "4      starting or not starting conversations  \n",
       "5               having little or a lot to say  \n",
       "6   talking to a lot or few people at parties  \n",
       "7                                         NaN  \n",
       "8                                         NaN  \n",
       "9                                         NaN  \n",
       "10                                        NaN  \n",
       "11                                        NaN  \n",
       "12                                        NaN  \n",
       "13                                        NaN  \n",
       "14                                        NaN  \n",
       "15                                        NaN  \n",
       "16                                        NaN  \n",
       "17                                        NaN  \n",
       "18                                        NaN  \n",
       "19                                        NaN  \n",
       "20                                        NaN  \n",
       "21                                        NaN  \n",
       "22                                        NaN  \n",
       "23                                        NaN  \n",
       "24                                        NaN  \n",
       "25                                        NaN  \n",
       "26                                        NaN  \n",
       "27                                        NaN  \n",
       "28                                        NaN  \n",
       "29                                        NaN  \n",
       "30                                        NaN  \n",
       "31                                        NaN  \n",
       "32                                        NaN  \n",
       "33                                        NaN  \n",
       "34                                        NaN  \n",
       "35                                        NaN  \n",
       "36                                        NaN  \n",
       "37                                        NaN  \n",
       "38                                        NaN  \n",
       "39                                        NaN  \n",
       "40                                        NaN  \n",
       "41                                        NaN  \n",
       "42                                        NaN  \n",
       "43                                        NaN  \n",
       "44                                        NaN  \n",
       "45                                        NaN  \n",
       "46                                        NaN  \n",
       "47                                        NaN  \n",
       "48                                        NaN  \n",
       "49                                        NaN  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surv.df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
