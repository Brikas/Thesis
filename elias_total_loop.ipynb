{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import brikasutils as bu\n",
    "importlib.reload(bu)\n",
    "import shared_utils as utils\n",
    "from shared_utils import systemMsg, userMsg, assistantMsg\n",
    "importlib.reload(utils)\n",
    "import survey\n",
    "importlib.reload(survey)\n",
    "import persona\n",
    "importlib.reload(persona)\n",
    "import ollama\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "from typing import List\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 1946 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-09-13 to 2024-03-06\n",
      "Messages saved to self.chats['elias']\n",
      "Read 40036 messages from 5 files. Failed to read 0 messages.\n",
      "Messages ranged from 2020-08-17 to 2024-03-04\n",
      "Messages saved to self.chats['petyo']\n",
      "Read 7953 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2023-05-12 to 2024-03-04\n",
      "Messages saved to self.chats['anna']\n",
      "Read 5734 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2020-09-18 to 2024-03-02\n",
      "Messages saved to self.chats['patryk']\n",
      "Read 372 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2022-03-19 to 2024-02-24\n",
      "Messages saved to self.chats['andreas']\n",
      "Read 3399 messages from 2 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-08-23 to 2024-03-02\n",
      "Messages saved to self.chats['victoria']\n",
      "Read 2951 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2022-11-13 to 2024-02-19\n",
      "Messages saved to self.chats['joanna']\n",
      "Read 409 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2020-09-07 to 2024-01-24\n",
      "Messages saved to self.chats['antoni']\n",
      "Read 1661 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2020-09-02 to 2023-09-29\n",
      "Messages saved to self.chats['arijan']\n",
      "Read 1350 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2022-11-03 to 2024-02-24\n",
      "Messages saved to self.chats['denis']\n",
      "Read 553 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-01-01 to 2021-04-01\n",
      "Messages saved to self.chats['alexandra']\n",
      "Read 349 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2023-04-21 to 2023-05-17\n",
      "Messages saved to self.chats['FED']\n",
      "Read 661 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-09-03 to 2023-12-13\n",
      "Messages saved to self.chats['filip']\n",
      "Read 1520 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2020-09-11 to 2023-07-23\n",
      "Messages saved to self.chats['kuba']\n",
      "Read 332 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2020-09-01 to 2023-03-19\n",
      "Messages saved to self.chats['laura']\n",
      "Read 812 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2020-09-01 to 2023-08-10\n",
      "Messages saved to self.chats['liisa']\n",
      "Read 753 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2020-09-30 to 2024-01-15\n",
      "Messages saved to self.chats['luiza']\n",
      "Read 2899 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2022-09-06 to 2024-01-22\n",
      "Messages saved to self.chats['marcus']\n",
      "Filtering\n",
      "link-filter: 1354\n",
      "react-filter: 33\n",
      "cookie-data-filter: 13\n",
      "Selected chat elias for 26008 (1671 messages)\n",
      "Selected chat petyo for 479653 (36995 messages)\n",
      "Selected chat anna for 86964 (7348 messages)\n",
      "Selected chat patryk for 53291 (4824 messages)\n",
      "Selected chat andreas for 4533 (338 messages)\n",
      "Selected chat victoria for 41688 (3152 messages)\n",
      "Selected chat joanna for 60978 (2808 messages)\n",
      "Selected chat antoni for 4692 (386 messages)\n",
      "Selected chat arijan for 20900 (1413 messages)\n",
      "Selected chat denis for 12326 (1287 messages)\n",
      "Selected chat alexandra for 5312 (542 messages)\n",
      "Selected chat FED for 4450 (310 messages)\n",
      "Selected chat filip for 6843 (628 messages)\n",
      "Selected chat kuba for 16892 (1464 messages)\n",
      "Selected chat laura for 3357 (318 messages)\n",
      "Selected chat liisa for 7927 (762 messages)\n",
      "Selected chat luiza for 8399 (690 messages)\n",
      "Selected chat marcus for 36691 (2736 messages)\n",
      "Combined tokens: 880904\n"
     ]
    }
   ],
   "source": [
    "texts_with_elias = [\n",
    "    \"selected-data/elias/message_1.json\",\n",
    "]\n",
    "texts_with_petyo = [\n",
    "    \"selected-data/petyo/message_1.json\",\n",
    "    \"selected-data/petyo/message_2.json\",\n",
    "    \"selected-data/petyo/message_3.json\",\n",
    "    \"selected-data/petyo/message_4.json\",\n",
    "    \"selected-data/petyo/message_5.json\",\n",
    "]\n",
    "texts_with_others_dict = {\n",
    "    \"anna\": [\"selected-data/others/anna.json\"],\n",
    "    \"patryk\": [\"selected-data/others/patryk.json\"],\n",
    "    \"andreas\": [\"selected-data/others/andreas.json\"],\n",
    "    \"victoria\": [\"selected-data/others/victoria.json\", \"selected-data/others/victoria2.json\"],\n",
    "    \"joanna\": [\"selected-data/others/joanna.json\"],\n",
    "    \"antoni\": [\"selected-data/others/antoni.json\"],\n",
    "    \"arijan\": [\"selected-data/others/arijan.json\"],\n",
    "    \"denis\": [\"selected-data/others/denis.json\"],\n",
    "    \"alexandra\": [\"selected-data/others/alexandra.json\"],\n",
    "    \"FED\": [\"selected-data/others/FED.json\"],\n",
    "    \"filip\": [\"selected-data/others/filip.json\"],\n",
    "    \"kuba\": [\"selected-data/others/kuba.json\"],\n",
    "    \"laura\": [\"selected-data/others/laura.json\"],\n",
    "    \"liisa\": [\"selected-data/others/liisa.json\"],\n",
    "    \"luiza\": [\"selected-data/others/luiza.json\"],\n",
    "    \"marcus\": [\"selected-data/others/marcus.json\"],\n",
    "}\n",
    "\n",
    "\n",
    "et = persona.PersonaEncoder()\n",
    "et.parse_fb_messages(texts_with_elias, \"elias\")\n",
    "et.parse_fb_messages(texts_with_petyo, \"petyo\")\n",
    "for name, texts in texts_with_others_dict.items():\n",
    "    et.parse_fb_messages(texts, name)\n",
    "\n",
    "# et.parse_rosebud_entries(\"selected-data/rosebud.md\", \"rosebud\")\n",
    "et.filter_chats_empty()\n",
    "et.filter_chats_regex(utils.BLACKLIST_CHAT_REGEX_FILTERS)\n",
    "\n",
    "\n",
    "for nameid, chat in et.chats.items():\n",
    "    for msg in chat:  \n",
    "        msg.sender = \"Persona\" if msg.sender == \"Airidas Brikas\" else \"Friend\"\n",
    "\n",
    "# Start all chats from 2/3rds\n",
    "# for nameid, chat in et.chats.items():\n",
    "#     et.chats[nameid] = chat[int(len(chat)/3 * 2):]\n",
    "\n",
    "# et.select_chat_limited_by_tokens(\"elias\", 6000)\n",
    "# et.select_chat_limited_by_tokens(\"petyo\", 6000)\n",
    "et.select_chat_full(\"elias\")\n",
    "et.select_chat_full(\"petyo\")\n",
    "\n",
    "for name in texts_with_others_dict.keys():\n",
    "    et.select_chat_full(name)\n",
    "\n",
    "# et.select_nonChat_module_full(\"rosebud\")\n",
    "\n",
    "token_counts = et.count_all_selected_chat_tokens() # token_counts used later for statistics\n",
    "print(f\"Combined tokens: {sum(token_counts.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed variables (7b model, static, impersonate, 3 runs each)\n",
    "EMBED_MODEL = \"nomic-embed-text\"\n",
    "PROMPT_METHOD =\"IMPERSONATE\"\n",
    "# MODEL = \"llama3:70b\"\n",
    "SUBJECT = \"eli\"\n",
    "surv = survey.PersonalitySurvey()\n",
    "# SURVEY_PROMPT = \"Determine how much {subject} aggree with the statement. Guestimate how {subject} would answer to the question\"\n",
    "# TINY_MODULE = \"You are Elias, a 24 year old business and IT student from Copenhagen, where you now live in a dormatory.\"\n",
    "# \"MED_MODULE\": \" \"\n",
    "# \"PERSONA_TEXT\": \"Favorite video games are Minecraft, Fortnite, and Call of Duty.\",\n",
    "\n",
    "if isinstance(surv, survey.KanoSurvey):\n",
    "    DYNAMIC_RETRIEVAL_PROMPTS = list(surv.questions)\n",
    "    PROMPT_COUNT = 40\n",
    "    SURVEY_TYPE = \"KanoSurvey\",\n",
    "    WHICH_SURVEY = \"kano\"\n",
    "    RETRIEVAL_PROMPT = \"video game features\"\n",
    "    SURVEY = \"video game preferences\"\n",
    "    METHOD = \"Kano survey\"\n",
    "elif isinstance(surv, survey.PersonalitySurvey):\n",
    "    DYNAMIC_RETRIEVAL_PROMPTS = list(surv.questions)\n",
    "    PROMPT_COUNT = 50\n",
    "    SURVEY_TYPE = \"PersonalitySurvey\",\n",
    "    WHICH_SURVEY = \"pers\"\n",
    "    RETRIEVAL_PROMPT = \"openess conciousness extrovert aggreableness neuroticism\"\n",
    "    SURVEY = \"personality traits\"\n",
    "    METHOD = \"OCEAN test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Boss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 48/929"
     ]
    }
   ],
   "source": [
    "CHUNK_SIZES = [75]\n",
    "chunk_size = CHUNK_SIZES[0]\n",
    "EMBED_MODEL = \"nomic-embed-text\"\n",
    "OVERLAP_SIZES = [3]\n",
    "overlap_size = OVERLAP_SIZES[0]\n",
    "\n",
    "chunks = []\n",
    "chunk_token_counts = []\n",
    "for chat in et.selectedChats.values():\n",
    "    messages = list(chat)  # Convert chat iterator to list for easier slicing\n",
    "    num_messages = len(messages)\n",
    "    for i in range(0, num_messages - chunk_size + 1, chunk_size - overlap_size):\n",
    "        chunk = messages[i:i + chunk_size]  # Extract chunk of messages\n",
    "        chunk_text = \"\\\\n\".join(str(msg) for msg in chunk)  # Concatenate msgs into a single string\n",
    "        chunks.append(chunk_text)  # Append chunk to list of chunks\n",
    "        chunk_token_counts.append(utils.count_tokens(chunk_text))  # Append token count of the chunk\n",
    "avg_chunk_token_count = sum(chunk_token_counts) / len(chunk_token_counts)\n",
    "embeddings = []\n",
    "progress, chunks_len = 0, len(chunks)\n",
    "for chunk_text in chunks:\n",
    "    progress += 1\n",
    "    print(f\"\\rChunk {progress}/{chunks_len}\", end=\"\")\n",
    "    embedding = ollama.embeddings(model=EMBED_MODEL, prompt=chunk_text)[\"embedding\"]\n",
    "    embeddings.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 385/1858"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m     progress \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124mChunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprogress\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunks_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m \u001b[43mollama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEMBED_MODEL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_text\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     27\u001b[0m     embeddings\u001b[38;5;241m.\u001b[39mappend(embedding)\n\u001b[1;32m     28\u001b[0m MODEL \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama3-70b\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/ollama/_client.py:198\u001b[0m, in \u001b[0;36mClient.embeddings\u001b[0;34m(self, model, prompt, options, keep_alive)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membeddings\u001b[39m(\n\u001b[1;32m    192\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    193\u001b[0m   model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    197\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Sequence[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m--> 198\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/api/embeddings\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkeep_alive\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/ollama/_client.py:68\u001b[0m, in \u001b[0;36mClient._request\u001b[0;34m(self, method, url, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, method: \u001b[38;5;28mstr\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m httpx\u001b[38;5;241m.\u001b[39mResponse:\n\u001b[0;32m---> 68\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/httpx/_client.py:827\u001b[0m, in \u001b[0;36mClient.request\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m    812\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m    814\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[1;32m    815\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    816\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    825\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[1;32m    826\u001b[0m )\n\u001b[0;32m--> 827\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    906\u001b[0m follow_redirects \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_redirects\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m follow_redirects\n\u001b[1;32m    910\u001b[0m )\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/httpx/_client.py:1015\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1011\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1012\u001b[0m     )\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1015\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1019\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/httpx/_transports/default.py:233\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    220\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    221\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    222\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    231\u001b[0m )\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 233\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    238\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    239\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    240\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    241\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    242\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/httpcore/_sync/connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    213\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, Iterable)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/httpcore/_sync/connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    192\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/httpcore/_sync/connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/httpcore/_sync/http11.py:143\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/httpcore/_sync/http11.py:113\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    106\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    107\u001b[0m     (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m         trailing_data,\n\u001b[0;32m--> 113\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    115\u001b[0m         http_version,\n\u001b[1;32m    116\u001b[0m         status,\n\u001b[1;32m    117\u001b[0m         reason_phrase,\n\u001b[1;32m    118\u001b[0m         headers,\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    121\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/httpcore/_sync/http11.py:186\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    183\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/httpcore/_sync/http11.py:224\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    221\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "PROMPT_METHOD =\"IMPERSONATE\"\n",
    "SUBJECT = \"airidas\"\n",
    "RETRIAVAL_METHODS = [\"dynamic\"]#, \"hybrid\"]\n",
    "NUM_RUNS = [3]\n",
    "MODEL = \"llama3-70b\"\n",
    "# MODEL = \"llama3-8b\"\n",
    "# MODEL = \"mixtral-8x22b\"\n",
    "max_tokens = [0, 3500, 7000]\n",
    "#max_tokens = [0, 3500, 31000]\n",
    "\n",
    "survs = [survey.KanoSurvey(), survey.PersonalitySurvey()]\n",
    "for surv in survs:\n",
    "    if isinstance(surv, survey.KanoSurvey):\n",
    "        DYNAMIC_RETRIEVAL_PROMPTS = list(surv.questions)\n",
    "        PROMPT_COUNT = 40\n",
    "        SURVEY_TYPE = \"KanoSurvey\",\n",
    "        WHICH_SURVEY = \"kano\"\n",
    "        RETRIEVAL_PROMPT = \"video game features\"\n",
    "        SURVEY = \"video game preferences\"\n",
    "        METHOD = \"a Kano survey\"\n",
    "    elif isinstance(surv, survey.PersonalitySurvey):\n",
    "        DYNAMIC_RETRIEVAL_PROMPTS = list(surv.questions)\n",
    "        PROMPT_COUNT = 50\n",
    "        SURVEY_TYPE = \"PersonalitySurvey\",\n",
    "        WHICH_SURVEY = \"pers\"\n",
    "        RETRIEVAL_PROMPT = \"openess conciousness extrovert aggreableness neuroticism\"\n",
    "        SURVEY = \"personality traits\"\n",
    "        METHOD = \"an OCEAN test\"\n",
    "\n",
    "    for max_token in max_tokens:\n",
    "\n",
    "        for RETRIAVAL_METHOD in RETRIAVAL_METHODS:\n",
    "            if RETRIAVAL_METHOD == \"dynamic\":\n",
    "                dynamic_retrieval_prompts = list(surv.questions)\n",
    "                dynamic_chunks_most_similar: List[List[str]] = [] \n",
    "                progress = 0\n",
    "                lenn = len(dynamic_retrieval_prompts)\n",
    "                for prompt in dynamic_retrieval_prompts:\n",
    "                    progress += 1\n",
    "                    print(f\"\\rPrompt {progress}/{lenn}\", end=\"\")\n",
    "                    prompt_embedding = ollama.embeddings(model=EMBED_MODEL, prompt=prompt)[\"embedding\"]\n",
    "\n",
    "                    ## Chunking\n",
    "                    max_chunks_count = int((max_token / avg_chunk_token_count))\n",
    "                    chunks_most_similar_embeddings = utils.find_most_similar(prompt_embedding, embeddings)\n",
    "                    chunks_most_similar = [chunks[embedding[1]] for embedding in chunks_most_similar_embeddings]\n",
    "\n",
    "                    if max_tokens == 0:\n",
    "                        chunks_most_similar = chunks_most_similar[:1]\n",
    "                    else:\n",
    "                        # Limit chunks by man token count\n",
    "                        cur_tc = 0 # current token count\n",
    "                        selected_chunks = []\n",
    "                        for chunk in chunks_most_similar:\n",
    "                            tk_in_chunk = utils.count_tokens(chunk)\n",
    "                            if cur_tc + tk_in_chunk >= max_token:\n",
    "                                break\n",
    "                            cur_tc += tk_in_chunk\n",
    "                            selected_chunks.append(chunk)\n",
    "                        chunks_most_similar = selected_chunks\n",
    "                    ##\n",
    "\n",
    "                    tokens_in_chunks = sum(utils.count_tokens(chunk) for chunk in chunks_most_similar)\n",
    "                    dynamic_chunks_most_similar.append(chunks_most_similar)\n",
    "                print(end=\"\\n\")\n",
    "                tokens_in_chunks = 0\n",
    "                for chunks_most_similar in dynamic_chunks_most_similar:\n",
    "                    for chunk in chunks_most_similar:\n",
    "                        tokens_in_chunks += utils.count_tokens(chunk)\n",
    "                del chunks_most_similar_embeddings  # free memory\n",
    "                print(f\"Tokens in average chunk group: {tokens_in_chunks/len(dynamic_chunks_most_similar)}\")\n",
    "                final_prompts = []\n",
    "                prompt_template = \"\"\"\n",
    "for question, chunks_most_similar in zip(surv.questions, dynamic_chunks_most_similar):\n",
    "    p = [\n",
    "        systemMsg(\"\\\\n\".join([\n",
    "            f\"You are an expert actor, specializing in impersonation of non-famous people. You will be presented to the subject through explicit datapoints of their digital footprint. In addition, you will deduct their implicit {SURVEY} by shadowing chats between the subject and friends. You will be asked to fully immerse yourself in the role, and answer questions from the point of view of the persona. \\\\n#Context \\\\n##Chat conversations between the subject and their friends:\\\\n\",\n",
    "            \"\\\\n\\\\nNEW CONVERSATION:\\\\n\".join(chunks_most_similar)\n",
    "        ])),     \n",
    "        assistantMsg(\"Understood. I will answer from the point of view of the persona, based on what I could the deduct from the text provided.\"),\n",
    "        userMsg(\"\\\\n\".join([\n",
    "            f\"Persona is questioned about their {SURVEY} in {METHOD}. The persona must choose an appropriate answer to the question below with one of these five given options: {', '.join(surv.POSSIBLE_ANSWERS)}. Persona's answer must only contain the chosen option, without any elaboration, nor introduction.\\\\n\\\\n**Your question is:**\\\\n\",\n",
    "            question,\n",
    "            \"\\\\nThe persona chooses:\"\n",
    "        ]))]\n",
    "    final_prompts.append(p)\n",
    "                \"\"\"\n",
    "            else: print(\"not dynamic\")\n",
    "            exec(prompt_template)\n",
    "            SIM_ID = f\"{SUBJECT}-{WHICH_SURVEY}-{str(max_token).zfill(2)}_{MODEL}_V7\"\n",
    "            bu.quickJSON(final_prompts, f\"temp/{SIM_ID}_prompts.json\")\n",
    "            for num_run in range(NUM_RUNS):\n",
    "                instructions = {\n",
    "                    \"prompt_file\": f\"batch/prompts/{SIM_ID}_prompts.json\",\n",
    "                    \"survey_type\": f\"{SURVEY_TYPE[0]}\",\n",
    "                    \"isLocal\": True,\n",
    "                    \"LIMIT\": None\n",
    "                }\n",
    "                settings = {\n",
    "                    \"model\": MODEL,\n",
    "                    \"timeout\": 300}\n",
    "                AUTO_INFO = {\n",
    "                    \"CHUNK_SIZE\": chunk_size,\n",
    "                    \"OVERLAP_SIZE\": overlap_size,\n",
    "                    \"CTX_limit\": max_token,\n",
    "                    \"chunk_count\": len(chunks_most_similar),\n",
    "                    \"EMBED_MODEL\": EMBED_MODEL,\n",
    "                    \"prompt method\": PROMPT_METHOD,\n",
    "                    \"retrieval method\": RETRIAVAL_METHOD,\n",
    "                    \"RETRIEVAL_PROMPT\": RETRIEVAL_PROMPT,\n",
    "                    \"prompt_count\": PROMPT_COUNT,\n",
    "                    \"survey\": WHICH_SURVEY,\n",
    "                    \"SUBJECT\": SUBJECT,\n",
    "                    \"prompt_template\": prompt_template,\n",
    "                    \"CHUNKS_COUNT_IN_CTX\": max_chunks_count,\n",
    "                    **utils.describe_prompts(final_prompts)\n",
    "                }\n",
    "                bu.quickJSON({\"instructions\": instructions, \"settings\": settings, \"info\": AUTO_INFO}, f\"temp/runs/{SIM_ID}_{num_run}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_most_similar_embeddings = utils.find_most_similar(prompt_embedding, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens in chunks: 6947\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tokens = 8000\n",
    "cur_tc = 0 # current token count\n",
    "selected_chunks = []\n",
    "for chunk in chunks_most_similar:\n",
    "    tk_in_chunk = utils.count_tokens(chunk)\n",
    "    if cur_tc + tk_in_chunk >= max_tokens:\n",
    "        break\n",
    "    cur_tc += tk_in_chunk\n",
    "    selected_chunks.append(chunk)\n",
    "print(f\"Tokens in chunks: {cur_tc}\")\n",
    "len(selected_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V5/6 - 32k models w/ timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZES = [250]\n",
    "MODELS = [\"mixtral:8x22b-instruct-v0.1-q2_K\"]\n",
    "# OVERLAP_SIZES = [5]\n",
    "overlap_size = 5\n",
    "RETRIAVAL_METHODS = [\"dynamic\", \"hybrid\"]\n",
    "max_tokens = 10000\n",
    "for chunk_size in CHUNK_SIZES:\n",
    "    for MODEL in MODELS:\n",
    "        chunks = []\n",
    "        chunk_token_counts = []\n",
    "        for chat in et.selectedChats.values():\n",
    "            messages = list(chat)  # Convert chat iterator to list for easier slicing\n",
    "            num_messages = len(messages)\n",
    "            for i in range(0, num_messages - chunk_size + 1, chunk_size - overlap_size):\n",
    "                chunk = messages[i:i + chunk_size]  # Extract chunk of messages\n",
    "                chunk_text = \"\\\\n\".join(str(msg) for msg in chunk)  # Concatenate msgs into a single string\n",
    "                chunks.append(chunk_text)  # Append chunk to list of chunks\n",
    "                chunk_token_counts.append(utils.count_tokens(chunk_text))  # Append token count of the chunk\n",
    "        avg_chunk_token_count = sum(chunk_token_counts) / len(chunk_token_counts)\n",
    "        embeddings = []\n",
    "        progress, chunks_len = 0, len(chunks)\n",
    "        for chunk_text in chunks:\n",
    "            progress += 1\n",
    "            print(f\"\\rChunk {progress}/{chunks_len}\", end=\"\")\n",
    "            embedding = ollama.embeddings(model=EMBED_MODEL, prompt=chunk_text)[\"embedding\"]\n",
    "            embeddings.append(embedding)\n",
    "        for RETRIAVAL_METHOD in RETRIAVAL_METHODS:\n",
    "            if RETRIAVAL_METHOD == \"dynamic\":\n",
    "                dynamic_retrieval_prompts = list(surv.questions)\n",
    "                dynamic_chunks_most_similar: List[List[str]] = [] \n",
    "                progress = 0\n",
    "                lenn = len(dynamic_retrieval_prompts)\n",
    "                for prompt in dynamic_retrieval_prompts:\n",
    "                    progress += 1\n",
    "                    print(f\"\\rPrompt {progress}/{lenn}\", end=\"\")\n",
    "                    prompt_embedding = ollama.embeddings(model=EMBED_MODEL, prompt=prompt)[\"embedding\"]\n",
    "                    max_chunks_count = int((max_tokens / avg_chunk_token_count)-1)\n",
    "                    chunks_most_similar_embeddings = utils.find_most_similar(prompt_embedding, embeddings)[:max_chunks_count]\n",
    "                    chunks_most_similar = [chunks[embedding[1]] for embedding in chunks_most_similar_embeddings]\n",
    "                    tokens_in_chunks = sum(utils.count_tokens(chunk) for chunk in chunks_most_similar)\n",
    "                    dynamic_chunks_most_similar.append(chunks_most_similar)\n",
    "                print(end=\"\\n\")\n",
    "                tokens_in_chunks = 0\n",
    "                for chunks_most_similar in dynamic_chunks_most_similar:\n",
    "                    for chunk in chunks_most_similar:\n",
    "                        tokens_in_chunks += utils.count_tokens(chunk)\n",
    "                del chunks_most_similar_embeddings  # free memory\n",
    "                print(f\"Tokens in average chunk group: {tokens_in_chunks/len(dynamic_chunks_most_similar)}\")\n",
    "                final_prompts = []\n",
    "                prompt_template = \"\"\"\n",
    "for question, chunks_most_similar in zip(surv.questions, dynamic_chunks_most_similar):\n",
    "    p = [\n",
    "        systemMsg(\"\\\\n\".join([\n",
    "            f\"You are an expert actor, specializing in impersonation of non-famouns people. You will be presented to the subject through explicit datapoints of their digital footprint. In addition, you will deduct their implicit {SURVEY} by shadowing chats between the subject and friends. You will be asked to fully immerse yourself in the role, and answer questions from the point of view of the persona. \\\\n#Context \\\\n##Chat conversations between the subject and their friends:\\\\n**From most to least related**\\\\n\",\n",
    "            \"\\\\n\\\\nNEW CONVERSATION:\\\\n\".join(chunks_most_similar)\n",
    "        ])),      \n",
    "        assistantMsg(\"Understood. I will answer from the point of view of the persona, based on what I could the deduct from the text provided.\"),\n",
    "        userMsg(\"\\\\n\".join([\n",
    "            f\"Persona is questioned about their {SURVEY} in an {METHOD}. The persona must choose an appropriate answer to the question below with one of these five given options: {', '.join(surv.POSSIBLE_ANSWERS)}. Persona's answer must only contain the chosen option, without any elaboration, nor introduction.\\\\n\\\\n**Your question is:**\\\\n\",\n",
    "            question,\n",
    "            \"\\\\nThe persona chooses:\"\n",
    "        ]))]\n",
    "    final_prompts.append(p)\n",
    "                \"\"\"\n",
    "            elif RETRIAVAL_METHOD == \"hybrid\":\n",
    "                prompt_embedding = ollama.embeddings(model=EMBED_MODEL, prompt=RETRIEVAL_PROMPT)[\"embedding\"]\n",
    "                max_chunks_count = int((max_tokens / avg_chunk_token_count))\n",
    "                chunks_most_similar_embeddings_static = utils.find_most_similar(prompt_embedding, embeddings)[:max_chunks_count // 2]\n",
    "                chunks_most_similar_static = [chunks[embedding[1]] for embedding in chunks_most_similar_embeddings_static]\n",
    "\n",
    "                dynamic_retrieval_prompts = list(surv.questions)\n",
    "                dynamic_chunks_most_similar: List[List[str]] = []\n",
    "                progress = 0\n",
    "                lenn = len(dynamic_retrieval_prompts)\n",
    "                for prompt in dynamic_retrieval_prompts:\n",
    "                    progress += 1\n",
    "                    print(f\"\\rPrompt {progress}/{lenn}\", end=\"\")\n",
    "                    prompt_embedding = ollama.embeddings(model=EMBED_MODEL, prompt=prompt)[\"embedding\"]\n",
    "                    chunks_most_similar_embeddings_dynamic = utils.find_most_similar(prompt_embedding, embeddings)[:max_chunks_count // 2]\n",
    "                    chunks_most_similar_dynamic = [chunks[embedding[1]] for embedding in chunks_most_similar_embeddings_dynamic]\n",
    "                    dynamic_chunks_most_similar.append(chunks_most_similar_dynamic)\n",
    "                print(end=\"\\n\")\n",
    "\n",
    "                chunks_most_similar = chunks_most_similar_static + [chunk for sublist in dynamic_chunks_most_similar for chunk in sublist]\n",
    "                tokens_in_chunks = sum(utils.count_tokens(chunk) for chunk in chunks_most_similar)\n",
    "                del chunks_most_similar_embeddings_static, chunks_most_similar_embeddings_dynamic  # free memory\n",
    "\n",
    "                print(f\"Tokens in average chunk group: {tokens_in_chunks / len(chunks_most_similar)}\")\n",
    "                final_prompts = []\n",
    "                prompt_template = \"\"\"\n",
    "for question, chunks_most_similar_dynamic in zip(surv.questions, dynamic_chunks_most_similar):\n",
    "    p = [\n",
    "        systemMsg(\"\\\\n\".join([\n",
    "            f\"You are an expert actor, specializing in impersonation of non-famouns people. You will be presented to the subject through explicit datapoints of their digital footprint. In addition, you will deduct their implicit {SURVEY} by shadowing chats between the subject and friends. You will be asked to fully immerse yourself in the role, and answer questions from the point of view of the persona. \\\\n#Context \\\\n##Chat conversations between the subject and their friends:\\\\n**From most to least related**\\\\n\",\n",
    "            \"\\\\n\\\\nNEW CONVERSATION RELATED TO THE SURVEY OVERALL:\\\\n\".join(chunks_most_similar_static),\n",
    "            \"\\\\n\\\\nNEW CONVERSATION RELATED TO THE PARTICULAR QUESTION:\\\\n\".join(chunks_most_similar_dynamic)\n",
    "        ])),      \n",
    "        assistantMsg(\"Understood. I will answer from the point of view of the persona, based on what I could the deduct from the text provided.\"),\n",
    "        userMsg(\"\\\\n\".join([\n",
    "            f\"Persona is questioned about their {SURVEY} in an {METHOD}. The persona must choose an appropriate answer to the question below with one of these five given options: {', '.join(surv.POSSIBLE_ANSWERS)}. Persona's answer must only contain the chosen option, without any elaboration, nor introduction.\\\\n\\\\n**Your question is:**\\\\n\",\n",
    "            question,\n",
    "            \"\\\\nThe persona chooses:\"\n",
    "        ]))]\n",
    "    final_prompts.append(p)\n",
    "                \"\"\"\n",
    "            else: print(\"neither hybrid, nor dynamic\")\n",
    "            exec(prompt_template)\n",
    "            prompt_info = utils.describe_prompts_and_print(final_prompts) # Vanity print\n",
    "            SIM_ID = f\"{SUBJECT}-{WHICH_SURVEY}-{RETRIAVAL_METHOD}-{chunk_size}-{str(overlap_size).zfill(2)}-{str(max_chunks_count).zfill(2)}_{MODEL}_V6\"\n",
    "            bu.quickJSON(final_prompts, f\"data/5_monster_prep/{SIM_ID}_prompts.json\")\n",
    "            instructions = {\n",
    "                \"prompt_file\": f\"batch/prompts/{SIM_ID}_prompts.json\",\n",
    "                \"survey_type\": f\"{SURVEY_TYPE[0]}\",\n",
    "                \"isLocal\": True,\n",
    "                \"LIMIT\": None\n",
    "            }\n",
    "            settings = {\n",
    "                \"model\": MODEL,\n",
    "                \"timeout\": 300}\n",
    "            AUTO_INFO = {\n",
    "                \"CHUNK_SIZE\": chunk_size,\n",
    "                \"OVERLAP_SIZE\": overlap_size,\n",
    "                \"CHUNKS_COUNT_IN_CTX\": max_chunks_count,#chunks_count_in_ctx,\n",
    "                \"CTX_limit\": max_tokens,\n",
    "                \"tokens_in_chunks\": tokens_in_chunks,\n",
    "                \"model\": EMBED_MODEL,\n",
    "                \"prompt method\": PROMPT_METHOD,\n",
    "                \"retrieval method\": RETRIAVAL_METHOD,\n",
    "                \"retrieval prompt\": RETRIEVAL_PROMPT,\n",
    "                \"prompt_count\": PROMPT_COUNT,\n",
    "                \"survey\": WHICH_SURVEY,\n",
    "                \"subject\": SUBJECT,\n",
    "                \"prompt_template\": prompt_template,\n",
    "                **prompt_info,\n",
    "                **utils.describe_prompts([])\n",
    "            }\n",
    "            bu.quickJSON({\"instructions\": instructions, \"settings\": settings, \"info\": AUTO_INFO}, f\"data/5_monster_prep/batch-schema/{SIM_ID}_schema.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Display Info\n",
    "total_messages = sum(len(chat) for chat in et.selectedChats.values())\n",
    "chunks_count = len(chunks)\n",
    "avg_chunk_char_len = np.mean([len(chunk) for chunk in chunks])\n",
    "print(\n",
    "    f\"Chunk count: {chunks_count}\",\n",
    "    f\"Rough estimate of tokens per chunk: {round(avg_chunk_char_len / 4)} (4 characters per token)\",\n",
    "    f\"Messagees in input count: {total_messages}\",\n",
    "    f\"Messages in chunks count: {stat_total_msgs_in_chunks}\",\n",
    "    f\"Chunk \\ Input ratio: {round(stat_total_msgs_in_chunks / total_messages,2)} (OVERLAP_SIZE={OVERLAP_SIZE})\",\n",
    "    f\"Chunk Python type: {type(chunks[0])}\",\n",
    "    sep=\"\\n\"\n",
    ") \n",
    "bu.if_dir_not_exist_make(\"data/3_embeddings\")\n",
    "bu.quickJSON(AUTO_INFO, f\"data/3_embeddings/{EMBEDDING_ID}_info.json\")\n",
    "bu.quickJSON({\"chunks\": chunks, \"embeddings\": embeddings}, f\"data/3_embeddings/{EMBEDDING_ID}_embeddings.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Tokens in chunks: {tokens_in_chunks}\")\n",
    "print(f\"Chunks:{len(chunks)}, embeds:{len(embeddings)}\")\n",
    "\n",
    "# bu.quickTXT(\"\\n\\n\".join(chunks_most_similar), filename=f\"data/4_chunks/{CHECKPOINT}-static_chunks.txt\")\n",
    "# bu.if_dir_not_exist_make(\"data/4_chunks\")\n",
    "# bu.quickJSON(AUTO_INFO, f\"data/4_chunks/{CHECKPOINT}-static_info.json\")\n",
    "# bu.quickJSON({\"chunks\": chunks, \"embeddings\": embeddings}, f\"data/4_chunks/{CHECKPOINT}-static_embeddings.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "# bu.if_dir_not_exist_make(\"data/4_chunks\")\n",
    "# bu.quickJSON(AUTO_INFO, f\"data/4_chunks/{CHECKPOINT}-dynamic_info.json\")\n",
    "# bu.quickJSON({\"chunks\": chunks, \"embeddings\": embeddings}, f\"data/4_chunks/{CHECKPOINT}-dynamic_embeddings.json\")\n",
    "############################################ VANITY BELOW ########################################\n",
    "tokens_in_chunks = 0\n",
    "for chunks_most_similar in dynamic_chunks_most_similar:\n",
    "    for chunk in chunks_most_similar:\n",
    "        tokens_in_chunks += utils.count_tokens(chunk)\n",
    "\n",
    "del chunks_most_similar_embeddings # free memory\n",
    "print(f\"Tokens in average chunk group: {tokens_in_chunks/len(dynamic_chunks_most_similar)}\")\n",
    "# bu.quickJSON(dynamic_chunks_most_similar, filename=f\"data/4_chunks/{CHECKPOINT}-dynamic_chunks.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################### Method A ############################################\n",
    "pre_final_prompts = []\n",
    "if PROMPT_METHOD == \"IMPERSONATE\":\n",
    "    pre_prompt_template = \"\"\"\n",
    "    SYS_MSG = {\n",
    "        \"role\": \"system\", \n",
    "        \"content\": f\"You are an expert actor, specializing in impersonation of non-famouns people. You will be presented to the subject through explicit datapoints of their digital footprint. In addition, you will deduct their implicit {SURVEY} by shadowing chats between the subject and friends. You will be asked to fully immerse yourself in the role, and answer questions from the point of view of the persona. \\\\n\\\\n**The persona, which you will be tasked to mimick is named '{SUBJECT}'.** \\\\n#Context \\\\n##Chat conversations between the subject and their friends:\\\\n**From most to least related**\\\\n\"\n",
    "    }\n",
    "    ASSIST_MSG = {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Understood. I will answer from the point of view of the persona, {subject}, based on what I could the deduct from the text provided.\"\n",
    "    }\n",
    "    USER_MSG = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Persona is questioned about their {SURVEY} in an {METHOD}. The persona must choose an appropriate answer to the question below with one of these five given options: {', '.join(surv.POSSIBLE_ANSWERS)}. Persona's answer must only contain the chosen option, without any elaboration, nor introduction.\"\n",
    "    }\n",
    "    pre_final_prompts.append(SYS_MSG, ASSIST_MSG, USER_MSG)\n",
    "    \"\"\"\n",
    "    exec(pre_prompt_template)\n",
    "\n",
    "########################################### Method B ###########################################\n",
    "elif PROMPT_METHOD == \"ARE\":\n",
    "    pre_prompt_template = \"\"\"\n",
    "    SYS_MSG = {\n",
    "        \"role\": \"system\", \n",
    "        \"content\": f\"**{TINY_MODULE}**. You have shared your thoughts, feelings, and experiences through text messages with friedns. Answer the following questions honestly and naturally, as you would in everyday conversations. \\\\n\\\\n#Context \\\\n##Conversations between persona and friends:\"\n",
    "    }\n",
    "    ASSIST_MSG = {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": f\"Understood. I am {SUBJECT}, and I will answer the survey to the best of my ability.\"\n",
    "    }\n",
    "    USER_MSG = {   \n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"The survey is about your {SURVEY}. You must choose an appropriate answer to the question below with one of these five given options: {', '.join(surv.POSSIBLE_ANSWERS)}. Your answer must only contain the chosen option, without any elaboration, nor introduction.\\\\n**From most to least related**\\\\n\"\n",
    "    }\n",
    "    \"\"\"\n",
    "exec(pre_prompt_template)\n",
    "\n",
    "print(f\"{SYS_MSG['content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BASE\n",
    "base_final_prompts = []\n",
    "prompt_template = \"\"\"\n",
    "for question in surv.questions:\n",
    "    p = [\n",
    "        systemMsg(\n",
    "            \"You are participating in a survey. You will be presented with a series of questions about your {SURVEY}.\",\n",
    "            f\"You must choose answer to the question below with one of the five options: {', '.join(surv.POSSIBLE_ANSWERS)}. The answer must only contain the chosen option. \"\n",
    "        ),\n",
    "        assistantMsg('Understood. I will answer the question below with one of the given options.'),\n",
    "        userMsg(\n",
    "            question,\n",
    "            \"Your choice: \"\n",
    "        )]\n",
    "    base_final_prompts.append(p)\n",
    "\"\"\"\n",
    "exec(prompt_template)\n",
    "# prompt_info = utils.describe_prompts_and_print(base_final_prompts) # Vanity print\n",
    "# bu.quickJSON(base_final_prompts, f\"data/5_prep/{WHICH_SURVEY}_base_prompts.json\")\n",
    "\n",
    "utils.count_tokens(prompt_template)\n",
    "#Static\n",
    "# static_final_prompts = []\n",
    "# prompt_template = \"\"\"\n",
    "# for question in surv.questions:\n",
    "#     p = [\n",
    "#         systemMsg(\"\\\\n\".join([\n",
    "#             f\"You are an expert actor, specializing in impersonation of non-famouns people. You will be presented to the subject through explicit datapoints of their digital footprint. In addition, you will deduct their implicit {SURVEY} by shadowing chats between the subject and friends. You will be asked to fully immerse yourself in the role, and answer questions from the point of view of the persona. \\\\n\\\\n**The persona, which you will be tasked to mimick is named '{SUBJECT}'.** \\\\n#Context \\\\n##Chat conversations between the subject and their friends:\\\\n**From most to least related**\\\\n\",\n",
    "#             \"\\\\nNEW CONVERSATION:\\\\n\".join(chunks_most_similar)\n",
    "#         ])),  \n",
    "#         assistantMsg(\"Understood. I will answer from the point of view of the persona, {subject}, based on what I could the deduct from the text provided.\"),\n",
    "#         userMsg(\"\\\\n\".join([\n",
    "#             f\"Persona is questioned about their {SURVEY} in an {METHOD}. The persona must choose an appropriate answer to the question below with one of these five given options: {', '.join(surv.POSSIBLE_ANSWERS)}. Persona's answer must only contain the chosen option, without any elaboration, nor introduction.\\\\n\\\\n**Your question is:**\\\\n\\\\n\",\n",
    "#             question,\n",
    "#             \"\\\\nThe persona chooses:\"\n",
    "#         ]))]\n",
    "#     static_final_prompts.append(p)\n",
    "# \"\"\"\n",
    "# exec(prompt_template)\n",
    "# prompt_info = utils.describe_prompts_and_print(static_final_prompts) # Vanity print\n",
    "# bu.quickJSON(final_prompts, f\"data/5_prep/{PREP_CHECKPOINT}_prompts.json\")\n",
    "# # print(f\"{len(static_final_prompts)}\")#,{final_prompts[:1]}\")\n",
    "\n",
    "# #Dynamic\n",
    "# dynamic_final_prompts = []\n",
    "# prompt_template = \"\"\"\n",
    "# for question, chunks_most_similar in zip(surv.questions, dynamic_chunks_most_similar):\n",
    "#     p = [\n",
    "#         systemMsg(\"\\\\n\".join([\n",
    "#             f\"{SYS_MSG['content']}\",\n",
    "#             \"\\\\nNEW CONVERSATION:\\\\n\".join(chunks_most_similar)\n",
    "#         ])),      \n",
    "#         assistantMsg(ASSIST_MSG['content']),\n",
    "#         userMsg(\"\\\\n\".join([\n",
    "#             f\"{USER_MSG['content']}\\\\n\\\\n**Your question is:**\\\\n\\\\n\",\n",
    "#             question,\n",
    "#             \"\\\\nThe persona chooses:\"\n",
    "#         ]))]\n",
    "#     dynamic_final_prompts.append(p)\n",
    "# \"\"\"    \n",
    "# exec(prompt_template)\n",
    "# prompt_info = utils.describe_prompts_and_print(dynamic_final_prompts)\n",
    "# bu.quickJSON(dynamic_final_prompts, f\"data/5_prep/{PREP_CHECKPOINT}_prompts.json\")\n",
    "# # print(f\"{len(dynamic_final_prompts)}\")#,{final_prompts[:1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{SURVEY_TYPE[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Set the directory where the JSON files are located\n",
    "directory = 'batch/done/monster_7b'  # Replace with the actual directory path if needed\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    # Check if the file has a .json extension\n",
    "    if filename.endswith('.json'):\n",
    "        # Open the JSON file\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # Modify the \"model\" value\n",
    "        if \"settings\" in data and \"model\" in data[\"settings\"]:\n",
    "            data[\"settings\"][\"model\"] = \"llama3\"\n",
    "\n",
    "        # Write the modified data back to the file\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surv = survey.PersonalitySurvey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surv.df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
