{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import brikasutils as bu\n",
    "importlib.reload(bu)\n",
    "import shared_utils as utils\n",
    "from shared_utils import systemMsg, userMsg, assistantMsg\n",
    "importlib.reload(utils)\n",
    "import survey\n",
    "importlib.reload(survey)\n",
    "import persona\n",
    "importlib.reload(persona)\n",
    "import ollama\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "from typing import List\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 1946 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-09-13 to 2024-03-06\n",
      "Messages saved to self.chats['elias']\n",
      "Read 40036 messages from 5 files. Failed to read 0 messages.\n",
      "Messages ranged from 2020-08-17 to 2024-03-04\n",
      "Messages saved to self.chats['petyo']\n",
      "Read 7953 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2023-05-12 to 2024-03-04\n",
      "Messages saved to self.chats['anna']\n",
      "Read 5734 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2020-09-18 to 2024-03-02\n",
      "Messages saved to self.chats['patryk']\n",
      "Read 372 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2022-03-19 to 2024-02-24\n",
      "Messages saved to self.chats['andreas']\n",
      "Read 3399 messages from 2 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-08-23 to 2024-03-02\n",
      "Messages saved to self.chats['victoria']\n",
      "Read 2951 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2022-11-13 to 2024-02-19\n",
      "Messages saved to self.chats['joanna']\n",
      "Read 409 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2020-09-07 to 2024-01-24\n",
      "Messages saved to self.chats['antoni']\n",
      "Read 1661 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2020-09-02 to 2023-09-29\n",
      "Messages saved to self.chats['arijan']\n",
      "Read 1350 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2022-11-03 to 2024-02-24\n",
      "Messages saved to self.chats['denis']\n",
      "Read 553 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-01-01 to 2021-04-01\n",
      "Messages saved to self.chats['alexandra']\n",
      "Read 349 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2023-04-21 to 2023-05-17\n",
      "Messages saved to self.chats['FED']\n",
      "Read 661 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-09-03 to 2023-12-13\n",
      "Messages saved to self.chats['filip']\n",
      "Read 1520 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2020-09-11 to 2023-07-23\n",
      "Messages saved to self.chats['kuba']\n",
      "Read 332 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2020-09-01 to 2023-03-19\n",
      "Messages saved to self.chats['laura']\n",
      "Read 812 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2020-09-01 to 2023-08-10\n",
      "Messages saved to self.chats['liisa']\n",
      "Read 753 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2020-09-30 to 2024-01-15\n",
      "Messages saved to self.chats['luiza']\n",
      "Read 2899 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2022-09-06 to 2024-01-22\n",
      "Messages saved to self.chats['marcus']\n",
      "Filtering\n",
      "link-filter: 1354\n",
      "react-filter: 33\n",
      "cookie-data-filter: 13\n",
      "Selected chat elias for 26008 (1671 messages)\n",
      "Selected chat petyo for 479653 (36995 messages)\n",
      "Selected chat anna for 86964 (7348 messages)\n",
      "Selected chat patryk for 53291 (4824 messages)\n",
      "Selected chat andreas for 4533 (338 messages)\n",
      "Selected chat victoria for 41688 (3152 messages)\n",
      "Selected chat joanna for 60978 (2808 messages)\n",
      "Selected chat antoni for 4692 (386 messages)\n",
      "Selected chat arijan for 20900 (1413 messages)\n",
      "Selected chat denis for 12326 (1287 messages)\n",
      "Selected chat alexandra for 5312 (542 messages)\n",
      "Selected chat FED for 4450 (310 messages)\n",
      "Selected chat filip for 6843 (628 messages)\n",
      "Selected chat kuba for 16892 (1464 messages)\n",
      "Selected chat laura for 3357 (318 messages)\n",
      "Selected chat liisa for 7927 (762 messages)\n",
      "Selected chat luiza for 8399 (690 messages)\n",
      "Selected chat marcus for 36691 (2736 messages)\n",
      "Combined tokens: 880904\n"
     ]
    }
   ],
   "source": [
    "texts_with_elias = [\n",
    "    \"selected-data/elias/message_1.json\",\n",
    "]\n",
    "texts_with_petyo = [\n",
    "    \"selected-data/petyo/message_1.json\",\n",
    "    \"selected-data/petyo/message_2.json\",\n",
    "    \"selected-data/petyo/message_3.json\",\n",
    "    \"selected-data/petyo/message_4.json\",\n",
    "    \"selected-data/petyo/message_5.json\",\n",
    "]\n",
    "texts_with_others_dict = {\n",
    "    \"anna\": [\"selected-data/others/anna.json\"],\n",
    "    \"patryk\": [\"selected-data/others/patryk.json\"],\n",
    "    \"andreas\": [\"selected-data/others/andreas.json\"],\n",
    "    \"victoria\": [\"selected-data/others/victoria.json\", \"selected-data/others/victoria2.json\"],\n",
    "    \"joanna\": [\"selected-data/others/joanna.json\"],\n",
    "    \"antoni\": [\"selected-data/others/antoni.json\"],\n",
    "    \"arijan\": [\"selected-data/others/arijan.json\"],\n",
    "    \"denis\": [\"selected-data/others/denis.json\"],\n",
    "    \"alexandra\": [\"selected-data/others/alexandra.json\"],\n",
    "    \"FED\": [\"selected-data/others/FED.json\"],\n",
    "    \"filip\": [\"selected-data/others/filip.json\"],\n",
    "    \"kuba\": [\"selected-data/others/kuba.json\"],\n",
    "    \"laura\": [\"selected-data/others/laura.json\"],\n",
    "    \"liisa\": [\"selected-data/others/liisa.json\"],\n",
    "    \"luiza\": [\"selected-data/others/luiza.json\"],\n",
    "    \"marcus\": [\"selected-data/others/marcus.json\"],\n",
    "}\n",
    "\n",
    "\n",
    "et = persona.PersonaEncoder()\n",
    "et.parse_fb_messages(texts_with_elias, \"elias\")\n",
    "et.parse_fb_messages(texts_with_petyo, \"petyo\")\n",
    "for name, texts in texts_with_others_dict.items():\n",
    "    et.parse_fb_messages(texts, name)\n",
    "\n",
    "# et.parse_rosebud_entries(\"selected-data/rosebud.md\", \"rosebud\")\n",
    "et.filter_chats_empty()\n",
    "et.filter_chats_regex(utils.BLACKLIST_CHAT_REGEX_FILTERS)\n",
    "\n",
    "\n",
    "for nameid, chat in et.chats.items():\n",
    "    for msg in chat:  \n",
    "        msg.sender = \"Persona\" if msg.sender == \"Airidas Brikas\" else \"Friend\"\n",
    "\n",
    "# Start all chats from 2/3rds\n",
    "# for nameid, chat in et.chats.items():\n",
    "#     et.chats[nameid] = chat[int(len(chat)/3 * 2):]\n",
    "\n",
    "# et.select_chat_limited_by_tokens(\"elias\", 6000)\n",
    "# et.select_chat_limited_by_tokens(\"petyo\", 6000)\n",
    "et.select_chat_full(\"elias\")\n",
    "et.select_chat_full(\"petyo\")\n",
    "\n",
    "for name in texts_with_others_dict.keys():\n",
    "    et.select_chat_full(name)\n",
    "\n",
    "# et.select_nonChat_module_full(\"rosebud\")\n",
    "\n",
    "token_counts = et.count_all_selected_chat_tokens() # token_counts used later for statistics\n",
    "print(f\"Combined tokens: {sum(token_counts.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Boss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gen embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 929/929"
     ]
    }
   ],
   "source": [
    "CHUNK_SIZES = [75]\n",
    "chunk_size = CHUNK_SIZES[0]\n",
    "EMBED_MODEL = \"nomic-embed-text\"\n",
    "OVERLAP_SIZES = [3]\n",
    "overlap_size = OVERLAP_SIZES[0]\n",
    "\n",
    "chunks = []\n",
    "chunk_token_counts = []\n",
    "for chat in et.selectedChats.values():\n",
    "    messages = list(chat)  # Convert chat iterator to list for easier slicing\n",
    "    num_messages = len(messages)\n",
    "    for i in range(0, num_messages - chunk_size + 1, chunk_size - overlap_size):\n",
    "        chunk = messages[i:i + chunk_size]  # Extract chunk of messages\n",
    "        chunk_text = \"\\\\n\".join(str(msg) for msg in chunk)  # Concatenate msgs into a single string\n",
    "        chunks.append(chunk_text)  # Append chunk to list of chunks\n",
    "        chunk_token_counts.append(utils.count_tokens(chunk_text))  # Append token count of the chunk\n",
    "avg_chunk_token_count = sum(chunk_token_counts) / len(chunk_token_counts)\n",
    "embeddings = []\n",
    "progress, chunks_len = 0, len(chunks)\n",
    "for chunk_text in chunks:\n",
    "    progress += 1\n",
    "    print(f\"\\rChunk {progress}/{chunks_len}\", end=\"\")\n",
    "    embedding = ollama.embeddings(model=EMBED_MODEL, prompt=chunk_text)[\"embedding\"]\n",
    "    embeddings.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_NAMEID = \"airidas_finalboss_1\"\n",
    "AUTO_INFO = {\n",
    "    \"model\": EMBED_MODEL,\n",
    "    \"CHUNK_SIZE\": chunk_size,\n",
    "    \"OVERLAP_SIZE\": overlap_size,\n",
    "    \"chunks_count\": len(chunks),\n",
    "    \"modules_chat\": token_counts,\n",
    "    \"overlap_size\": overlap_size,\n",
    "}\n",
    "\n",
    "bu.quickJSON(AUTO_INFO, f\"embeddings/{EMBEDDING_NAMEID}_info.json\")\n",
    "bu.quickJSON({\"chunks\": chunks, \"embeddings\": embeddings}, f\"embeddings/{EMBEDDING_NAMEID}_embeddings.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load embeddings from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks:929, embeds:929\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_NAMEID = \"airidas_finalboss_1\"\n",
    "\n",
    "import json\n",
    "with open(f\"embeddings/{EMBEDDING_NAMEID}_embeddings.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    chunks = data[\"chunks\"]\n",
    "    embeddings = data[\"embeddings\"]\n",
    "\n",
    "try:\n",
    "    with open(f\"embeddings/{EMBEDDING_NAMEID}_info.json\", \"r\") as f:\n",
    "        AUTO_INFO = json.load(f)\n",
    "        try:\n",
    "            EMBED_MODEL = AUTO_INFO[\"model\"]\n",
    "            chunk_size = AUTO_INFO[\"CHUNK_SIZE\"]\n",
    "        except KeyError:\n",
    "            print(\"WARNING: Info text does not contain model information\")\n",
    "except:\n",
    "    print(\"WARNING: No Info file found. Make sure embedding model is matching.\")\n",
    "\n",
    "print(f\"Chunks:{len(chunks)}, embeds:{len(embeddings)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The actual boss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default Kano Survey CSV file: surveys/survey_kano-model.csv\n",
      "Using default Personality Survey CSV file: surveys/survey_personality-test.csv\n",
      "Tokens in average chunk group: 834.0\n",
      "Tokens in average chunk group: 872.0\n",
      "Tokens in average chunk group: 988.2857142857143\n",
      "Tokens in average chunk group: 950.0\n",
      "Tokens in average chunk group: 1332.6666666666667\n",
      "Tokens in average chunk group: 1195.5\n"
     ]
    }
   ],
   "source": [
    "PROMPT_METHOD =\"IMPERSONATE\"\n",
    "SUBJECT = \"airidas\"\n",
    "RETRIAVAL_METHODS = [\"static\"]#, \"hybrid\", \"dynamic\", \n",
    "NUM_RUNS = 3\n",
    "# MODEL = \"llama3-70b\"\n",
    "# MODEL = \"llama3-8b\"\n",
    "MODEL = \"mixtral-8x22b\"\n",
    "# max_tokens = [7500]\n",
    "max_tokens = [0, 4000, 7500]\n",
    "\n",
    "survs = [survey.KanoSurvey(), survey.PersonalitySurvey()]\n",
    "for surv in survs:\n",
    "    if isinstance(surv, survey.KanoSurvey):\n",
    "        DYNAMIC_RETRIEVAL_PROMPTS = list(surv.questions)\n",
    "        PROMPT_COUNT = 40\n",
    "        SURVEY_TYPE = \"KanoSurvey\",\n",
    "        WHICH_SURVEY = \"kano\"\n",
    "        RETRIEVAL_PROMPT = \"video game features\"\n",
    "        SURVEY = \"video game preferences\"\n",
    "        METHOD = \"a Kano survey\"\n",
    "    elif isinstance(surv, survey.PersonalitySurvey):\n",
    "        DYNAMIC_RETRIEVAL_PROMPTS = list(surv.questions)\n",
    "        PROMPT_COUNT = 50\n",
    "        SURVEY_TYPE = \"PersonalitySurvey\",\n",
    "        WHICH_SURVEY = \"pers\"\n",
    "        RETRIEVAL_PROMPT = \"openess conciousness extrovert aggreableness neuroticism\"\n",
    "        SURVEY = \"personality traits\"\n",
    "        METHOD = \"an OCEAN test\"\n",
    "\n",
    "    for max_token in max_tokens:\n",
    "        for RETRIAVAL_METHOD in RETRIAVAL_METHODS:\n",
    "            if RETRIAVAL_METHOD == \"dynamic\":\n",
    "                dynamic_retrieval_prompts = list(surv.questions)\n",
    "                dynamic_chunks_most_similar: List[List[str]] = [] \n",
    "                progress = 0\n",
    "                lenn = len(dynamic_retrieval_prompts)\n",
    "\n",
    "                # Dynamic Retrieval\n",
    "                for prompt in dynamic_retrieval_prompts:\n",
    "                    progress += 1\n",
    "                    print(f\"\\rPrompt {progress}/{lenn}\", end=\"\")\n",
    "\n",
    "                    prompt_embedding = ollama.embeddings(model=EMBED_MODEL, prompt=prompt)[\"embedding\"]\n",
    "\n",
    "                    ## Chunking and Cosine Similarity\n",
    "                    max_chunks_count = int((max_token / avg_chunk_token_count))\n",
    "                    chunks_most_similar_embeddings = utils.find_most_similar(prompt_embedding, embeddings)\n",
    "                    chunks_most_similar = [chunks[embedding[1]] for embedding in chunks_most_similar_embeddings]\n",
    "\n",
    "                    # Limit chunks by tokens\n",
    "                    if max_token == 0:\n",
    "                        chunks_most_similar = chunks_most_similar[:1]\n",
    "                    else:\n",
    "                        # Limit chunks by man token count\n",
    "                        cur_tc = 0 # current token count\n",
    "                        selected_chunks = []\n",
    "                        for chunk in chunks_most_similar:\n",
    "                            tk_in_chunk = utils.count_tokens(chunk)\n",
    "                            if cur_tc + tk_in_chunk >= max_token:\n",
    "                                break\n",
    "                            cur_tc += tk_in_chunk\n",
    "                            selected_chunks.append(chunk)\n",
    "                        chunks_most_similar = selected_chunks\n",
    "                    \n",
    "                    # Finalize chunks\n",
    "                    tokens_in_chunks = sum(utils.count_tokens(chunk) for chunk in chunks_most_similar)\n",
    "                    dynamic_chunks_most_similar.append(chunks_most_similar)\n",
    "                print(end=\"\\n\")\n",
    "\n",
    "                # Count total tokens in all chunks\n",
    "                tokens_in_chunks = 0\n",
    "                for chunks_most_similar in dynamic_chunks_most_similar:\n",
    "                    for chunk in chunks_most_similar:\n",
    "                        tokens_in_chunks += utils.count_tokens(chunk)\n",
    "                # del chunks_most_similar_embeddings  # free memory\n",
    "                print(f\"Tokens in average chunk group: {tokens_in_chunks/len(dynamic_chunks_most_similar)}\")\n",
    "                final_prompts = []\n",
    "                prompt_template = \"\"\"\n",
    "for question, chunks_most_similar in zip(surv.questions, dynamic_chunks_most_similar):\n",
    "    p = [\n",
    "        systemMsg(\"\\\\n\".join([\n",
    "            f\"You are an expert actor, specializing in impersonation of non-famous people. You will be presented to the subject through explicit datapoints of their digital footprint. In addition, you will deduct their implicit {SURVEY} by shadowing chats between the subject and friends. You will be asked to fully immerse yourself in the role, and answer questions from the point of view of the persona. \\\\n#Context \\\\n##Chat conversations between the subject and their friends:\\\\n\",\n",
    "            \"\\\\n\\\\nNEW CONVERSATION:\\\\n\".join(chunks_most_similar)\n",
    "        ])),     \n",
    "        assistantMsg(\"Understood. I will answer from the point of view of the persona, based on what I could the deduct from the text provided.\"),\n",
    "        userMsg(\"\\\\n\".join([\n",
    "            f\"Persona is questioned about their {SURVEY} in {METHOD}. The persona must choose an appropriate answer to the question below with one of these five given options: {', '.join(surv.POSSIBLE_ANSWERS)}. Persona's answer must only contain the chosen option, without any elaboration, nor introduction.\\\\n\\\\n**Your question is:**\\\\n\",\n",
    "            question,\n",
    "            \"\\\\nThe persona chooses:\"\n",
    "        ]))]\n",
    "    final_prompts.append(p)\n",
    "                \"\"\"\n",
    "\n",
    "            elif RETRIAVAL_METHOD == \"static\":\n",
    "                prompt_embedding = ollama.embeddings(model=EMBED_MODEL, prompt=RETRIEVAL_PROMPT)[\"embedding\"]\n",
    "                chunks_most_similar_embeddings = utils.find_most_similar(prompt_embedding, embeddings)\n",
    "                chunks_most_similar = [chunks[embedding[1]] for embedding in chunks_most_similar_embeddings]\n",
    "                ## Chunking\n",
    "                if max_token == 0:\n",
    "                    chunks_most_similar = chunks_most_similar[:1]\n",
    "                else:\n",
    "                    # Limit chunks by man token count\n",
    "                    cur_tc = 0 # current token count\n",
    "                    selected_chunks = []\n",
    "                    for chunk in chunks_most_similar:\n",
    "                        tk_in_chunk = utils.count_tokens(chunk)\n",
    "                        if cur_tc + tk_in_chunk >= max_token:\n",
    "                            break\n",
    "                        cur_tc += tk_in_chunk\n",
    "                        selected_chunks.append(chunk)\n",
    "                    chunks_most_similar = selected_chunks\n",
    "                \n",
    "                # Count total tokens in all chunks\n",
    "                tokens_in_chunks = sum(utils.count_tokens(chunk) for chunk in chunks_most_similar)\n",
    "                print(f\"Tokens in average chunk group: {tokens_in_chunks/len(chunks_most_similar)}\")\n",
    "\n",
    "                del chunks_most_similar_embeddings # free memory\n",
    "                #Retrieve^\n",
    "                final_prompts = []\n",
    "                prompt_template = \"\"\"\n",
    "for question in surv.questions:\n",
    "    p = [\n",
    "        systemMsg(\"\\\\n\".join([\n",
    "            f\"You are an expert actor, specializing in impersonation of non-famous people. You will be presented to the subject through explicit datapoints of their digital footprint. In addition, you will deduct their implicit {SURVEY} by shadowing chats between the subject and friends. You will be asked to fully immerse yourself in the role, and answer questions from the point of view of the persona. \\\\n#Context \\\\n##Chat conversations between the subject and their friends:\\\\n\",\n",
    "            \"\\\\n\\\\nNEW CONVERSATION:\\\\n\".join(chunks_most_similar)\n",
    "        ])),     \n",
    "        assistantMsg(\"Understood. I will answer from the point of view of the persona, based on what I could the deduct from the text provided.\"),\n",
    "        userMsg(\"\\\\n\".join([\n",
    "            f\"Persona is questioned about their {SURVEY} in {METHOD}. The persona must choose an appropriate answer to the question below with one of these five given options: {', '.join(surv.POSSIBLE_ANSWERS)}. Persona's answer must only contain the chosen option, without any elaboration, nor introduction.\\\\n\\\\n**Your question is:**\\\\n\",\n",
    "            question,\n",
    "            \"\\\\nThe persona chooses:\"\n",
    "        ]))]\n",
    "    final_prompts.append(p)\n",
    "                \"\"\"\n",
    "            else: raise ValueError(f\"Unknown retrieval method: {RETRIAVAL_METHOD}\")\n",
    "            exec(prompt_template)\n",
    "            SIM_ID = f\"{SUBJECT}-{WHICH_SURVEY}-{RETRIAVAL_METHOD}-{max_token}_{MODEL}_V8\"\n",
    "            bu.quickJSON(final_prompts, f\"temp/{SIM_ID}_prompts.json\")\n",
    "            for num_run in range(NUM_RUNS):\n",
    "                instructions = {\n",
    "                    \"prompt_file\": f\"batch/prompts/{SIM_ID}_prompts.json\",\n",
    "                    \"survey_type\": f\"{SURVEY_TYPE[0]}\",\n",
    "                    \"isLocal\": True,\n",
    "                    \"LIMIT\": None\n",
    "                }\n",
    "                settings = {\n",
    "                    \"model\": MODEL,\n",
    "                    \"timeout\": 300}\n",
    "                AUTO_INFO = {\n",
    "                    \"CHUNK_SIZE\": chunk_size,\n",
    "                    \"OVERLAP_SIZE\": overlap_size,\n",
    "                    \"CTX_limit\": max_token,\n",
    "                    \"chunk_count\": len(chunks_most_similar),\n",
    "                    \"EMBED_MODEL\": EMBED_MODEL,\n",
    "                    \"prompt method\": PROMPT_METHOD,\n",
    "                    \"retrieval method\": RETRIAVAL_METHOD,\n",
    "                    \"RETRIEVAL_PROMPT\": RETRIEVAL_PROMPT,\n",
    "                    \"prompt_count\": PROMPT_COUNT,\n",
    "                    \"survey\": WHICH_SURVEY,\n",
    "                    \"SUBJECT\": SUBJECT,\n",
    "                    \"prompt_template\": prompt_template,\n",
    "                    **utils.describe_prompts(final_prompts)\n",
    "                }\n",
    "                bu.quickJSON({\"instructions\": instructions, \"settings\": settings, \"info\": AUTO_INFO}, f\"temp/runs/{SIM_ID}_{num_run}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_METHOD =\"IMPERSONATE\"\n",
    "SUBJECT = \"airidas\"\n",
    "RETRIAVAL_METHODS = [\"dynamic\"]#, \"hybrid\"]\n",
    "NUM_RUNS = 3\n",
    "# MODEL = \"llama3-70b\"\n",
    "# MODEL = \"llama3-8b\"\n",
    "MODEL = \"mixtral-8x22b\"\n",
    "max_tokens = [0, 4000, 7500]\n",
    "# max_tokens = [0, 4000, 15500]\n",
    "\n",
    "survs = [survey.KanoSurvey(), survey.PersonalitySurvey()]\n",
    "for surv in survs:\n",
    "    if isinstance(surv, survey.KanoSurvey):\n",
    "        DYNAMIC_RETRIEVAL_PROMPTS = list(surv.questions)\n",
    "        PROMPT_COUNT = 40\n",
    "        SURVEY_TYPE = \"KanoSurvey\",\n",
    "        WHICH_SURVEY = \"kano\"\n",
    "        RETRIEVAL_PROMPT = \"video game features\"\n",
    "        SURVEY = \"video game preferences\"\n",
    "        METHOD = \"a Kano survey\"\n",
    "    elif isinstance(surv, survey.PersonalitySurvey):\n",
    "        DYNAMIC_RETRIEVAL_PROMPTS = list(surv.questions)\n",
    "        PROMPT_COUNT = 50\n",
    "        SURVEY_TYPE = \"PersonalitySurvey\",\n",
    "        WHICH_SURVEY = \"pers\"\n",
    "        RETRIEVAL_PROMPT = \"openess conciousness extrovert aggreableness neuroticism\"\n",
    "        SURVEY = \"personality traits\"\n",
    "        METHOD = \"an OCEAN test\"\n",
    "\n",
    "    for max_token in max_tokens:\n",
    "\n",
    "        for RETRIAVAL_METHOD in RETRIAVAL_METHODS:\n",
    "            if RETRIAVAL_METHOD == \"dynamic\":\n",
    "                dynamic_retrieval_prompts = list(surv.questions)\n",
    "                dynamic_chunks_most_similar: List[List[str]] = [] \n",
    "                progress = 0\n",
    "                lenn = len(dynamic_retrieval_prompts)\n",
    "                for prompt in dynamic_retrieval_prompts:\n",
    "                    progress += 1\n",
    "                    print(f\"\\rPrompt {progress}/{lenn}\", end=\"\")\n",
    "                    prompt_embedding = ollama.embeddings(model=EMBED_MODEL, prompt=prompt)[\"embedding\"]\n",
    "\n",
    "                    ## Chunking\n",
    "                    chunks_most_similar_embeddings = utils.find_most_similar(prompt_embedding, embeddings)\n",
    "                    chunks_most_similar = [chunks[embedding[1]] for embedding in chunks_most_similar_embeddings]\n",
    "\n",
    "                    if max_token == 0:\n",
    "                        chunks_most_similar = chunks_most_similar[:1]\n",
    "                    else:\n",
    "                        # Limit chunks by man token count\n",
    "                        cur_tc = 0 # current token count\n",
    "                        selected_chunks = []\n",
    "                        for chunk in chunks_most_similar:\n",
    "                            tk_in_chunk = utils.count_tokens(chunk)\n",
    "                            if cur_tc + tk_in_chunk >= max_token:\n",
    "                                break\n",
    "                            cur_tc += tk_in_chunk\n",
    "                            selected_chunks.append(chunk)\n",
    "                        chunks_most_similar = selected_chunks\n",
    "                    ##\n",
    "\n",
    "                    tokens_in_chunks = sum(utils.count_tokens(chunk) for chunk in chunks_most_similar)\n",
    "                    dynamic_chunks_most_similar.append(chunks_most_similar)\n",
    "                print(end=\"\\n\")\n",
    "                tokens_in_chunks = 0\n",
    "                for chunks_most_similar in dynamic_chunks_most_similar:\n",
    "                    for chunk in chunks_most_similar:\n",
    "                        tokens_in_chunks += utils.count_tokens(chunk)\n",
    "                del chunks_most_similar_embeddings  # free memory\n",
    "                print(f\"Tokens in average chunk group: {tokens_in_chunks/len(dynamic_chunks_most_similar)}\")\n",
    "                final_prompts = []\n",
    "                prompt_template = \"\"\"\n",
    "for question, chunks_most_similar in zip(surv.questions, dynamic_chunks_most_similar):\n",
    "    p = [\n",
    "        systemMsg(\"\\\\n\".join([\n",
    "            f\"You are an expert actor, specializing in impersonation of non-famous people. You will be presented to the subject through explicit datapoints of their digital footprint. In addition, you will deduct their implicit {SURVEY} by shadowing chats between the subject and friends. You will be asked to fully immerse yourself in the role, and answer questions from the point of view of the persona. \\\\n#Context \\\\n##Chat conversations between the subject and their friends:\\\\n\",\n",
    "            \"\\\\n\\\\nNEW CONVERSATION:\\\\n\".join(chunks_most_similar)\n",
    "        ])),     \n",
    "        assistantMsg(\"Understood. I will answer from the point of view of the persona, based on what I could the deduct from the text provided.\"),\n",
    "        userMsg(\"\\\\n\".join([\n",
    "            f\"Persona is questioned about their {SURVEY} in {METHOD}. The persona must choose an appropriate answer to the question below with one of these five given options: {', '.join(surv.POSSIBLE_ANSWERS)}. Persona's answer must only contain the chosen option, without any elaboration, nor introduction.\\\\n\\\\n**Your question is:**\\\\n\",\n",
    "            question,\n",
    "            \"\\\\nThe persona chooses:\"\n",
    "        ]))]\n",
    "    final_prompts.append(p)\n",
    "                \"\"\"\n",
    "            else: print(\"not dynamic\")\n",
    "            exec(prompt_template)\n",
    "\n",
    "            # Save prompts\n",
    "            SIM_ID = f\"{SUBJECT}-{WHICH_SURVEY}-{str(max_token)}_{MODEL}_V7\"\n",
    "            bu.quickJSON(final_prompts, f\"temp/{SIM_ID}_prompts.json\")\n",
    "\n",
    "            \n",
    "            for num_run in range(NUM_RUNS):\n",
    "                instructions = {\n",
    "                    \"prompt_file\": f\"batch/prompts/{SIM_ID}_prompts.json\",\n",
    "                    \"survey_type\": f\"{SURVEY_TYPE[0]}\",\n",
    "                    \"isLocal\": True,\n",
    "                    \"LIMIT\": None\n",
    "                }\n",
    "                settings = {\n",
    "                    \"model\": MODEL,\n",
    "                    \"timeout\": 300}\n",
    "                AUTO_INFO = {\n",
    "                    \"CHUNK_SIZE\": chunk_size,\n",
    "                    \"OVERLAP_SIZE\": overlap_size,\n",
    "                    \"CTX_limit\": max_token,\n",
    "                    \"chunk_count\": len(chunks_most_similar),\n",
    "                    \"EMBED_MODEL\": EMBED_MODEL,\n",
    "                    \"prompt method\": PROMPT_METHOD,\n",
    "                    \"retrieval method\": RETRIAVAL_METHOD,\n",
    "                    \"RETRIEVAL_PROMPT\": RETRIEVAL_PROMPT,\n",
    "                    \"prompt_count\": PROMPT_COUNT,\n",
    "                    \"survey\": WHICH_SURVEY,\n",
    "                    \"SUBJECT\": SUBJECT,\n",
    "                    \"prompt_template\": prompt_template,\n",
    "                    **utils.describe_prompts(final_prompts)\n",
    "                }\n",
    "                bu.quickJSON({\"instructions\": instructions, \"settings\": settings, \"info\": AUTO_INFO}, f\"temp/runs/{SIM_ID}_{num_run}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_most_similar_embeddings = utils.find_most_similar(prompt_embedding, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens in chunks: 6947\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tokens = 8000\n",
    "cur_tc = 0 # current token count\n",
    "selected_chunks = []\n",
    "for chunk in chunks_most_similar:\n",
    "    tk_in_chunk = utils.count_tokens(chunk)\n",
    "    if cur_tc + tk_in_chunk >= max_tokens:\n",
    "        break\n",
    "    cur_tc += tk_in_chunk\n",
    "    selected_chunks.append(chunk)\n",
    "print(f\"Tokens in chunks: {cur_tc}\")\n",
    "len(selected_chunks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
