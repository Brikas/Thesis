{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial setup: **INPUT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import brikasutils as bu\n",
    "importlib.reload(bu)\n",
    "import shared_utils as utils\n",
    "from shared_utils import systemMsg, userMsg, assistantMsg\n",
    "importlib.reload(utils)\n",
    "import survey\n",
    "importlib.reload(survey)\n",
    "import persona\n",
    "importlib.reload(persona)\n",
    "\n",
    "import ollama\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "from typing import List\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et = persona.PersonaEncoder()\n",
    "\n",
    "# ==== FB messages ====\n",
    "et.parse_fb_messages([\"data/1_raw/1_airidas.json\"], \"airidas\")\n",
    "et.parse_fb_messages([\"data/1_raw/2_christian.json\"], \"christian\")\n",
    "et.parse_fb_messages([\"data/1_raw/1_nikolay.json\"], \"nikolay\")\n",
    "et.parse_fb_messages([\"data/1_raw/2_mathis.json\"], \"mathis\")\n",
    "et.parse_fb_messages([\"data/1_raw/2_jacob.json\"], \"jacob\")\n",
    "et.parse_fb_messages([\"data/1_raw/2_chris.json\"], \"chris\")\n",
    "et.parse_fb_messages([\"data/1_raw/3_aziz.json\"], \"aziz\")\n",
    "et.parse_fb_messages([\"data/1_raw/3_daniela.json\"], \"daniela\")\n",
    "et.parse_fb_messages([\"data/1_raw/3_mihi.json\"], \"mihi\")\n",
    "et.parse_fb_messages([\"data/1_raw/3_viktoria.json\"], \"viktoria\")\n",
    "et.parse_fb_messages([\"data/1_raw/4_diba.json\"], \"diba\")\n",
    "et.parse_fb_messages([\"data/1_raw/6_filip.json\"], \"filip\")\n",
    "et.parse_wa_messages([\"data/1_raw/messages_1000.json\"], \"rebecca\")\n",
    "# for name, texts in texts_with_others_dict.items():\n",
    "#     et.parse_fb_messages(texts, name)\n",
    "\n",
    "# Regex cleaning\n",
    "et.filter_chats_empty()\n",
    "et.filter_chats_regex(utils.BLACKLIST_CHAT_REGEX_FILTERS)\n",
    "\n",
    "# Compress names\n",
    "for nameid, chat in et.chats.items():\n",
    "    for msg in chat:  \n",
    "        msg.sender = \"Persona\" if msg.sender == \"Elias Salvador Smidt Torjani\"  else \"Friend\"\n",
    "\n",
    "# Start all chats from 2/3rds\n",
    "# for name, chat in et.chats.items():\n",
    "#     et.chats[name] = chat[int(len(chat)/3 * 2):]\n",
    "# Select the final modules\n",
    "# et.select_chat_limited_by_tokens(\"airidas\", 10000)\n",
    "et.select_chat_full(\"rebecca\")\n",
    "et.select_chat_full(\"airidas\")\n",
    "et.select_chat_full(\"christian\")\n",
    "et.select_chat_full(\"nikolay\")\n",
    "et.select_chat_full(\"mathis\") \n",
    "et.select_chat_full(\"daniela\")\n",
    "et.select_chat_full(\"diba\")\n",
    "et.select_chat_full(\"aziz\")\n",
    "et.select_chat_full(\"jacob\")  \n",
    "et.select_chat_full(\"chris\")\n",
    "et.select_chat_full(\"filip\")\n",
    "et.select_chat_full(\"mihi\")\n",
    "et.select_chat_full(\"viktoria\")\n",
    "\n",
    "# save\n",
    "BIG_MODULE=et.output()\n",
    "bu.quickTXT(BIG_MODULE, filename=f\"data/2_modules/big_{bu.get_timestamp()}\")\n",
    "\n",
    "# stats\n",
    "token_counts = et.count_all_selected_chat_tokens() # token_counts used later for statistics\n",
    "print(f\"Combined tokens: {sum(token_counts.values())}\")\n",
    "# utils.count_tokens(BIG_MODULE) \n",
    "# or list(et.selectedChats.keys()) --> et.count_chat_tokens(\"{friend}\")\n",
    "# et.selectedChats[\"{friend}\"][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# surv = survey.PersonalitySurvey()\n",
    "surv = survey.KanoSurvey()\n",
    "# surv = survey.buildFairnessPrompts()\n",
    "# surv = survey.DictatorGameSurvey()\n",
    "surv.questions[:2]#.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "EMBED_MODEL = \"nomic-embed-text\"        # nomic-embed-text = long ctx / mxbai-embed-large = big\n",
    "CHUNK_SIZE = 50                         # Number of messages per chunk\n",
    "OVERLAP_SIZE = 10                       # Number of overlapping messages between consecutive chunks\n",
    "# COMMENT 04-16, perhaps we could try 5x retrievals with isolated semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists for storing chunks â€“ and embeddings later\n",
    "# different chunk size\n",
    "chunks = []\n",
    "stat_total_msgs_in_chunks = 0 # for statistics\n",
    "\n",
    "# different chunk size\n",
    "# Iterate over chats and messages to create chunks\n",
    "for chat in et.selectedChats.values():\n",
    "    messages = list(chat)  # Convert chat iterator to list for easier slicing\n",
    "    num_messages = len(messages)\n",
    "\n",
    "    # Create overlapping chunks of messages\n",
    "    for i in range(0, num_messages - CHUNK_SIZE + 1, CHUNK_SIZE - OVERLAP_SIZE):\n",
    "        chunk = messages[i:i + CHUNK_SIZE]  # Extract chunk of messages\n",
    "        chunk_text = \"\\n\".join(str(msg) for msg in chunk)  # Concatenate messages into a single string\n",
    "        chunks.append(chunk_text)  # Append chunk to list of chunks\n",
    "\n",
    "        stat_total_msgs_in_chunks += len(chunk) # For statistics\n",
    "\n",
    "##### Display Info\n",
    "total_messages = sum(len(chat) for chat in et.selectedChats.values())\n",
    "chunks_count = len(chunks)\n",
    "avg_chunk_char_len = np.mean([len(chunk) for chunk in chunks])\n",
    "\n",
    "print(\n",
    "    f\"Chunk count: {chunks_count}\",\n",
    "    f\"Average chunk character length: {round( avg_chunk_char_len)}\",\n",
    "    f\"Rough estimate of tokens per chunk: {round(avg_chunk_char_len / 4)} (4 characters per token)\",\n",
    "    f\"Messagees in input count: {total_messages}\",\n",
    "    f\"Messages in chunks count: {stat_total_msgs_in_chunks}\",\n",
    "    f\"Chunk \\ Input ratio: {round(stat_total_msgs_in_chunks / total_messages,2)} (OVERLAP_SIZE={OVERLAP_SIZE})\",\n",
    "    f\"Chunk Python type: {type(chunks[0])}\",\n",
    "    sep=\"\\n\"\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generaterating embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Serialization ########\n",
    "EMBEDDING_NAMEID = \"game_batch-B\"\n",
    "AUTO_INFO = {\n",
    "    \"model\": EMBED_MODEL,\n",
    "    \"CHUNK_SIZE\": CHUNK_SIZE,\n",
    "    \"OVERLAP_SIZE\": OVERLAP_SIZE,\n",
    "    \"chunks_count\": chunks_count,\n",
    "    \"total_messages\": total_messages,\n",
    "    \"stat_total_msgs_in_chunks\": stat_total_msgs_in_chunks,\n",
    "    \"modules_chat\": token_counts,\n",
    "}\n",
    "##################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for each chunk\n",
    "embeddings = []\n",
    "\n",
    "progress, chunks_len = 0, len(chunks) # for progress bar\n",
    "for chunk_text in chunks:\n",
    "    progress += 1\n",
    "    print(f\"\\rChunk {progress}/{chunks_len}\", end=\"\")\n",
    "\n",
    "    embedding = ollama.embeddings(model=EMBED_MODEL, prompt=chunk_text)[\"embedding\"]\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "####################################################\n",
    "# token counts in all similar chunks\n",
    "# tokens_in_chunks = 0\n",
    "# for chunk in chunks_most_similar:\n",
    "#     tokens_in_chunks += utils.count_tokens(chunk)\n",
    "# print(f\"Tokens in chunks: {tokens_in_chunks}\")\n",
    "\n",
    "bu.if_dir_not_exist_make(\"data/3_embeddings\")\n",
    "bu.quickJSON(AUTO_INFO, f\"data/3_embeddings/{EMBEDDING_NAMEID}_info.json\")\n",
    "bu.quickJSON({\"chunks\": chunks, \"embeddings\": embeddings}, f\"data/3_embeddings/{EMBEDDING_NAMEID}_embeddings.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION_ID = \"8k-dynamic\"\n",
    "\n",
    "subject = \"elias\"\n",
    "\n",
    "# persona_small = \"SMALL_MODULE\"\n",
    "# persona_med = \"{MED_MODULE}\"\n",
    "# persona_text = \"Favorite video games are Rimworld, Minecraft, Age of Empires, 7 Days to Die\"\n",
    "\n",
    "# Change below accoring to survey above\n",
    "# RETRIEVAL_PROMPT = \"openess conciousness extrovert aggreableness neuroticism\" #\"personality\"\n",
    "RETRIEVAL_PROMPT = \"video game features\"\n",
    "# q_retrival_prompt =\n",
    "# SURVEY_PROMPT = \"Determine how much {subject} aggree with the statement. Guestimate how {subject} would answer to the question\"\n",
    "\n",
    "CHUNKS_COUNT_IN_CTX = 10 # Number of nearby chunks to put in context window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_embedding = ollama.embeddings(model=EMBED_MODEL, prompt=RETRIEVAL_PROMPT)[\"embedding\"]\n",
    "chunks_most_similar_embeddings  = utils.find_most_similar(prompt_embedding, embeddings)[:CHUNKS_COUNT_IN_CTX]\n",
    "chunks_most_similar = []\n",
    "for embedding in chunks_most_similar_embeddings:\n",
    "    chunks_most_similar.append(chunks[embedding[1]])\n",
    "\n",
    "# token counts in all similar chunks\n",
    "tokens_in_chunks = 0\n",
    "for chunk in chunks_most_similar:\n",
    "    tokens_in_chunks += utils.count_tokens(chunk)\n",
    "print(f\"Tokens in chunks: {tokens_in_chunks}\")\n",
    "####################################################\n",
    "print(f\"Chunks:{len(chunks)}, embeds:{len(embeddings)}\")\n",
    "\n",
    "\n",
    "# Display results\n",
    "bu.quickTXT(\"\\n\\n\".join(chunks_most_similar), filename=f\"data/4_chunks/{EMBEDDING_NAMEID}_{VERSION_ID}_chunks.txt\")\n",
    "\n",
    "bu.if_dir_not_exist_make(\"data/4_chunks\")\n",
    "bu.quickJSON(AUTO_INFO, f\"data/4_chunks/{EMBEDDING_NAMEID}_{VERSION_ID}_info.json\")\n",
    "bu.quickJSON({\"chunks\": chunks, \"embeddings\": embeddings}, f\"data/4_chunks/{EMBEDDING_NAMEID}_{VERSION_ID}_embeddings.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_retrieval_prompts = list(surv.questions)\n",
    "\n",
    "CHUNKS_COUNT_IN_CTX = 5 # Number of nearby chunks to put in context window\n",
    "dynamic_chunks_most_similar: List[List[str]] = [] \n",
    "\n",
    "progress = 0\n",
    "lenn = len(dynamic_retrieval_prompts)\n",
    "for prompt in dynamic_retrieval_prompts:\n",
    "    progress += 1\n",
    "    print(f\"\\rPrompt {progress}/{lenn}\", end=\"\")\n",
    "\n",
    "    prompt_embedding = ollama.embeddings(model=EMBED_MODEL, prompt=prompt)[\"embedding\"]\n",
    "    chunks_most_similar_embeddings = utils.find_most_similar(prompt_embedding, embeddings)[:CHUNKS_COUNT_IN_CTX]\n",
    "    chunks_most_similar = []\n",
    "    for embedding in chunks_most_similar_embeddings:\n",
    "        chunks_most_similar.append(chunks[embedding[1]])\n",
    "\n",
    "    dynamic_chunks_most_similar.append(chunks_most_similar)\n",
    "print(end=\"\\n\")\n",
    "    \n",
    "# VANITY PRINT\n",
    "tokens_in_chunks = 0\n",
    "for chunks_most_similar in dynamic_chunks_most_similar:\n",
    "    for chunk in chunks_most_similar:\n",
    "        tokens_in_chunks += utils.count_tokens(chunk)\n",
    "\n",
    "del chunks_most_similar_embeddings # free memory\n",
    "print(f\"Tokens in average chunk group: {tokens_in_chunks/len(dynamic_chunks_most_similar)}\")\n",
    "bu.quickJSON(dynamic_chunks_most_similar, filename=f\"data/4_chunks/{EMBEDDING_NAMEID}_{VERSION_ID}-chunks.json\")\n",
    "###########################################\n",
    "# Vanity preview\n",
    "preview_text = \"\"\n",
    "PREVIEW_LIMIT = 10\n",
    "\n",
    "for i, chunks_most_similar in enumerate(dynamic_chunks_most_similar):\n",
    "    preview_text += f\"==============Prompt: {dynamic_retrieval_prompts[i]}==============\\n\"\n",
    "    for j, chunk in enumerate(chunks_most_similar):\n",
    "        if j >= PREVIEW_LIMIT:\n",
    "            break\n",
    "        preview_text += f\"=======CHUNK {j}=======\\n{chunk}\\n\\n\"\n",
    "    preview_text += \"\\n\\n\"\n",
    "bu.quickTXT(preview_text, filename=f\"data/4_chunks/{EMBEDDING_NAMEID}_{VERSION_ID}-chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You are {subject} vs you will impersonate {subject}\n",
    "SYS_MSG = {\n",
    "    \"role\": \"system\", \n",
    "    \"content\": \"You are an actor specializing in impersonating non-famouns people. You will be presented to the subject through explicit datapoints of their digital footprint. In addition, you will deduct their implicit personality traits by shadowing chats between the subject and friends. You will be asked to answer questions from the point of view of the persona. The persona you will be impersonating is named Elias. Context:\"\n",
    "    }\n",
    "\n",
    "ASSIST_MSG = {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": \"\"\n",
    "}\n",
    "\n",
    "USER_MSG = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompts = []\n",
    "\n",
    "for question, chunks_most_similar in zip(surv.questions, dynamic_chunks_most_similar):\n",
    "    p = [\n",
    "        systemMsg(\n",
    "            \"You are specialized in impersonating people. You will be presented to the subject through explicit datapoints of their digital footprint. In addition, you will deduct their implicit tastes by shadowing chats between the subject and friends. You will be asked to answer questions from the point of view of the persona. Text below:\",\n",
    "            \"Conversations between persona and friends\",\n",
    "            \"\\nNEW CONVERSATION:\\n\".join(chunks_most_similar)\n",
    "        ),\n",
    "        # Understanding affirmation\n",
    "        assistantMsg('I will answer from the point of view of the persona, based on what I could the deduct from the text provided.'),\n",
    "        # Survey question. With Simulation\n",
    "        userMsg(\"\\n\".join([\n",
    "            f\"Persona is surveyed about their video game survey. The persona must choose answer the question below with one of the given options: {', '.join(surv.POSSIBLE_ANSWERS)}. Persona's answer must only contain the chosen option, without any elaboration, nor introduction. \",\n",
    "            question,\n",
    "            \"Persona chooses: \"\n",
    "        ])),\n",
    "        # assistantMsg(\"\\n\".join([f\"response: \"\n",
    "        # ])),\n",
    "    ]\n",
    "    final_prompts.append(p)\n",
    "\n",
    "prompt_info = utils.describe_prompts_and_print(final_prompts)\n",
    "bu.quickJSON(final_prompts, f\"data/5_prep/{EMBEDDING_NAMEID}_{VERSION_ID}_prompts.json\")\n",
    "print(f\"{len(final_prompts)}\")#,{final_prompts[:1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYS_PROMPT = {\n",
    "    \"role\": \"system\", \n",
    "    \"content\": \"You are an actor specializing in impersonating non-famouns people. You will be presented to the subject through explicit datapoints of their digital footprint. In addition, you will deduct their implicit personality traits by shadowing chats between the subject and friends. You will be asked to answer questions from the point of view of the persona. The persona you will be impersonating is named Elias. Context:\"\n",
    "    }\n",
    "\n",
    "# Load Embeddings From File (optional)\n",
    "EMBEDDING_NAMEID = \"game_batch-A\"\n",
    "VERSION_ID = \"8k-static\"\n",
    "\n",
    "import json\n",
    "with open(f\"data/4_chunks/{EMBEDDING_NAMEID}_{VERSION_ID}_embeddings.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    chunks = data[\"chunks\"]\n",
    "    embeddings = data[\"embeddings\"]\n",
    "\n",
    "print(f\"Chunks:{len(chunks)}, embeds:{len(embeddings)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompts = []\n",
    "\n",
    "for question in surv.questions:\n",
    "    p = [\n",
    "        systemMsg(SYS_PROMPT['content']+\"\\n## chat conversions between subject and friends\\n\".join(chunks_most_similar)),\n",
    "        assistantMsg('Understood. I will answer from the point of view of the persona, {subject}, based on what I could the deduct from the text provided above.'),\n",
    "        userMsg(\"\\n\".join([\n",
    "            f'\\n\\n**Your answer should only contain the chosen option without further explanation!** Reply to the statement below - how {subject} would reply - with one of these five options: {\", \".join(surv.POSSIBLE_ANSWERS)}.',\n",
    "            question,\n",
    "            \"The persona chooses: \"\n",
    "        ])),\n",
    "    ]\n",
    "    final_prompts.append(p)\n",
    "\n",
    "print(f\"{len(final_prompts)}\")#,{final_prompts[:1]}\")\n",
    "prompt_info = utils.describe_prompts_and_print(final_prompts) # Vanity print\n",
    "bu.quickJSON(final_prompts, f\"data/5_prep/{EMBEDDING_NAMEID}_{VERSION_ID}_prompts.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.describe_prompts_and_print(final_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base (no persona)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompts = []\n",
    "\n",
    "for question in surv.questions:\n",
    "    p = [\n",
    "        systemMsg(\n",
    "            \"You are participating in a survey. You will be presented with a series of questions about your video game preferrences.\",\n",
    "            f\"You must choose answer to the question below with one of the five options: {', '.join(surv.POSSIBLE_ANSWERS)}. The answer must only contain the chosen option. \"\n",
    "        ),\n",
    "        # Understanding affirmation\n",
    "        assistantMsg('Understood. I will answer the question below with one of the given options.'),\n",
    "        # Survey question. With Simulation\n",
    "        userMsg(\n",
    "            question,\n",
    "            \"Your choice: \"\n",
    "        ),\n",
    "    ]\n",
    "    final_prompts.append(p)\n",
    "\n",
    "prompt_info = utils.describe_prompts_and_print(final_prompts) # Vanity print\n",
    "bu.quickJSON(final_prompts, \"data/5_prep/game_base_prompts.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json file to dict\n",
    "# with open(\"simulations/toElias/run2-airidas-personality_cv1_prompts.json\", \"r\") as read_file:\n",
    "# with open(\"simulations/toElias/run2-airidas-video-game-cv1_prompts.json\", \"r\") as read_file:\n",
    "# with open(\"simulations/toElias/run2-base-personality-cv1_prompts.json\", \"r\") as read_file:\n",
    "with open(\"simulations/toElias/run2-base-video-game-cv1_prompts.json\", \"r\") as read_file:\n",
    "    pre_final_prompts = json.load(read_file)\n",
    "\n",
    "\n",
    "# pre_final_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Simulation\n",
    "SETTINGS = {\n",
    "     \"model\": \"command-r-plus:104b-q2_K\", # mixtral, command-r-plus:104b-q2_K\n",
    "     # \"temperature\": 0.5,\n",
    "     # best wizard and mixtral try mixtral-8x22b wizard in uCloud\n",
    "}\n",
    "\n",
    "##################################\n",
    "SIM_ID = f\"run2-base-video-game_rplus_cv2\"\n",
    "LIMIT = None # For testing purposes. Set to NONE to run all\n",
    "AUTO_INFO = {\n",
    "    \"date\": bu.get_timestamp(),\n",
    "    # \"EMBEDDING_NAMEID\": EMBEDDING_NAMEID,\n",
    "    # \"RETRIEVAL_PROMPT\": RETRIEVAL_PROMPT,\n",
    "    # \"CHUNKS_COUNT_IN_CTX\": CHUNKS_COUNT_IN_CTX,\n",
    "    # \"survey_type\": str(type(surv)),\n",
    "    # \"prompt_count\": min(len(final_prompts), LIMIT) if LIMIT != None else len(final_prompts),\n",
    "    # \"avg_tokens_in_prompt\": round(prompt_info[\"total_all_prompt_tokens\"]/len(final_prompts)),\n",
    "}\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama', # required, but unused\n",
    ")\n",
    "\n",
    "save = f\"{SETTINGS['model']}_{SIM_ID}\"\n",
    "##################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ==== THE FUNCTIONAL 1!!!! =====\n",
    "SETTINGS = {\n",
    "     \"model\": \"llama3\", # mixtral, command-r-plus:104b-q2_K\n",
    "}\n",
    "##########################################\n",
    "SIM_ID = f\"run2-base-video-game_rplus_cv2\"\n",
    "LIMIT = None # For testing purposes. Set to NONE to run all\n",
    "AUTO_INFO = {\n",
    "    \"date\": bu.get_timestamp(),\n",
    "}\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama', # required, but unused\n",
    ")\n",
    "##########################################\n",
    "\n",
    "save = f\"{SETTINGS['model']}_{SIM_ID}\"\n",
    "completions = []\n",
    "l = len(final_prompts)\n",
    "timer = bu.Benchmarker()\n",
    "for i, (prompt, question) in enumerate(list(zip(final_prompts, surv.questions))):\n",
    "    if LIMIT != None and i > LIMIT:\n",
    "        break\n",
    "    timer.mark()\n",
    "    print(f\"{i}/{l}...\", end=\"\\t\") # Print progress\n",
    "    # Send the Request    \n",
    "    full_response = client.chat.completions.create(\n",
    "        model=SETTINGS[\"model\"],\n",
    "        messages=prompt,\n",
    "        # timeout=120,\n",
    "        # temperature=SETTINGS[\"temperature\"],\n",
    "    )\n",
    "\n",
    "    r = full_response.choices[0].message.content\n",
    "    completions.append({'question': question, 'answer': r})\n",
    "    print(f\"{question}: {r}\")\n",
    "\n",
    "timer.mark()\n",
    "# Save results\n",
    "df = pd.DataFrame(completions)\n",
    "# df.to_csv(f\"results/{save}_simulation.csv\", index=False)\n",
    "df.to_csv(f\"simulations/{SIM_ID}_simulation.csv\", index=False)\n",
    "# bu.quickJSON(final_prompts, f\"results/{save}_prompts.json\")\n",
    "bu.quickJSON(final_prompts, f\"ignorefolder/{SIM_ID}_prompts.json\")\n",
    "# bu.quickJSON(SETTINGS, f\"results/{save}_setings.json\")\n",
    "bu.quickJSON({\"settings\": SETTINGS, \"info\": AUTO_INFO}, f\"simulations/{SIM_ID}_info.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Batch Sim**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "SIMULATION_NAMEID = \"airidas-personality_mixtral_cv1\" #f\"{SIM_ID}\"\n",
    "\n",
    "df = pd.read_csv(f'simulations/local/personality/{SIMULATION_NAMEID}_simulation.csv')\n",
    "with open(f'simulations/local/personality/{SIMULATION_NAMEID}_info.json', 'r') as f:\n",
    "    loaded = json.load(f)\n",
    "try:\n",
    "    AUTO_INFO = loaded[\"info\"]\n",
    "    SETTINGS = loaded[\"settings\"]\n",
    "    print(\"Settings and info loaded:\")\n",
    "    for k, v in AUTO_INFO.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "    for k, v in SETTINGS.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "except:\n",
    "    print(\"No settings and/or info found\")\n",
    "\n",
    "try:\n",
    "    if str(type(surv)) != AUTO_INFO[\"survey_type\"]:\n",
    "        print(f\"WARNING: surv variable is not of the same type. {str(type(surv))} != {AUTO_INFO['survey_type']}\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "df.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bu.if_dir_not_exist_make(\"data/5_sim-clean/results\")\n",
    "res = bu.LiveCSV(f\"data/5_sim-clean/elias_runs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the simulation results\n",
    "# both personality and game WIP\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "dfs = []\n",
    "filenames = []\n",
    "\n",
    "# List of folder paths\n",
    "folder_paths = ['simulations/local/video-game/', 'simulations/local/personality/']\n",
    "\n",
    "# Read the first CSV file from the first folder to get the 'question' column\n",
    "first_folder_path = folder_paths[0]\n",
    "first_file_path = os.path.join(first_folder_path, os.listdir(first_folder_path)[0])\n",
    "first_df = pd.read_csv(first_file_path)\n",
    "questions = first_df['question']\n",
    "\n",
    "for folder_path in folder_paths:\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # remove all characters from a black list from the column answer\n",
    "            df['answer'] = df['answer'].apply(lambda x: x.strip())\n",
    "            for substr in utils.BLACKLIST_ANSWER_SUBSTRINGS:\n",
    "                df['answer'] = df['answer'].apply(lambda x: re.sub(substr, \"\", x))\n",
    "            df['answer'] = df['answer'].str.upper()\n",
    "\n",
    "            filename_without_ext = filename[:-4]  # Remove the '.csv' extension\n",
    "            filename_without_simulation = (\n",
    "                filename_without_ext.replace('_simulation', '')\n",
    "                                    # .replace('video-', '')\n",
    "            )  # Remove '_simulation' from the filename\n",
    "            filenames.append(filename_without_simulation)\n",
    "            dfs.append(df['answer'])\n",
    "\n",
    "answers_df = pd.concat(dfs, axis=1, keys=filenames)\n",
    "answers_df.insert(0, 'question', questions)  # Insert the 'question' column at the beginning\n",
    "answers_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surv = {}\n",
    "surv['POSSIBLE_ANSWERS'] = [\"I LIKE IT\", \"I EXPECT IT\", \"I AM NEUTRAL\", \"I CAN TOLERATE IT\", \"I DISLIKE IT\", \"SOMEWHAT DISAGREE\", \"DISAGREE\", \"NEUTRAL\", \"SOMEWHAT AGREE\", \"AGREE\"]\n",
    "\n",
    "def remove_invalid_answers(value):\n",
    "    if pd.isna(value):\n",
    "        return \"\"\n",
    "    elif value in surv['POSSIBLE_ANSWERS']:\n",
    "        return value\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def extract_possible_answer(value):\n",
    "    for phrase in surv['POSSIBLE_ANSWERS']:\n",
    "        pattern = r'(?i)' + re.escape(phrase)\n",
    "        match = re.search(pattern, value)\n",
    "        if match:\n",
    "            return match.group()\n",
    "    return value  # Return the original value if no possible answer is found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RETRIEVAL_PROMPT = surv['POSSIBLE_ANSWERS']\n",
    "PRE_DEF_ANSWERS = [\"I LIKE IT\", \"I EXPECT IT\", \"I AM NEUTRAL\", \"I CAN TOLERATE IT\", \"I DISLIKE IT\", \"SOMEWHAT DISAGREE\", \"DISAGREE\", \"NEUTRAL\", \"SOMEWHAT AGREE\", \"AGREE\"]\n",
    "\n",
    "embeddings = [ollama.embeddings(model=EMBED_MODEL, prompt=answer)[\"embedding\"] for answer in PRE_DEF_ANSWERS]\n",
    "\n",
    "\n",
    "FINAL_STRINGS_2_CLEAN = df['answer'] #change to answers_df['answer'] for all simulations\n",
    "# utils.find_most_similar(ollama.embeddings(model=EMBED_MODEL, prompt=FINAL_STRINGS_2_CLEAN[0])[\"embedding\"], embeddings)\n",
    "mapped_results = [utils.find_most_similar(string) for string in FINAL_STRINGS_2_CLEAN]\n",
    "print(mapped_results)\n",
    "# Generate embeddings for each chunk\n",
    "embeddings = []\n",
    "\n",
    "progress, chunks_len = 0, len(chunks) # for progress bar\n",
    "for chunk_text in chunks:\n",
    "    progress += 1\n",
    "    print(f\"\\rChunk {progress}/{chunks_len}\", end=\"\")\n",
    "\n",
    "    embedding = ollama.embeddings(model=EMBED_MODEL, prompt=chunk_text)[\"embedding\"]\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "bu.if_dir_not_exist_make(\"data/3_embeddings\")\n",
    "bu.quickJSON(AUTO_INFO, f\"data/3_embeddings/POSSIBLE_ANSWERS_info.json\")\n",
    "bu.quickJSON({\"chunks\": chunks, \"embeddings\": embeddings}, f\"data/3_embeddings/POSSIBLE_ANSWERS_embeddings.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     pattern = r'\\b(?:' + '|'.join(re.escape(phrase) for phrase in phrases_to_extract) + r')\\b'\n",
    "#     matches = re.findall(pattern, text, flags=re.IGNORECASE) \n",
    "#     return ' '.join(matches) if matches else ''\n",
    "\n",
    "\n",
    "# Update isValid\n",
    "df['isValid'] = df['answer'].apply(lambda x: x in surv.POSSIBLE_ANSWERS)\n",
    "\n",
    "# if all values in isValid is true, drop the column, else print a message\n",
    "if df['isValid'].all():\n",
    "    df = df.drop('isValid', axis=1)\n",
    "    print(\"All answers were valid\")\n",
    "else:\n",
    "    print(\"Some answers were not valid\")\n",
    "\n",
    "df.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proces simulation output - KANO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surv = survey.KanoSurvey()\n",
    "# Add airidas and elias answers\n",
    "air = surv.test_answers[\"airidas\"]\n",
    "eli = surv.test_answers[\"elias\"]\n",
    "df.insert(2, \"airidas\", air[:len(df)])\n",
    "df.insert(3, \"elias\", eli[:len(df)])\n",
    "\n",
    "df['answer'] = df['answer'].str.upper()\n",
    "df['airidas'] = df['airidas'].str.upper()\n",
    "df['elias'] = df['elias'].str.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proces simulation output - PERSONALITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surv = survey.PersonalitySurvey()\n",
    "# df = df.dropna()\n",
    "\n",
    "# Add airidas and elias answers\n",
    "air = surv.test_answers[\"airidas\"]\n",
    "eli = surv.test_answers[\"elias\"]\n",
    "df.insert(2, \"airidas\", air[:len(df)])\n",
    "df.insert(3, \"elias\", eli[:len(df)])\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute one number of how the percentage of correct answers\n",
    "result_data = {\n",
    "    \"Exact Matches\": (df['answer'] == df['airidas']).sum() / len(df),\n",
    "    \"Correlation\": df['answer'].corr(df['airidas']),\n",
    "    \"Exact Matches - elias\": (df['answer'] == df['elias']).sum() / len(df),\n",
    "    \"Correlation - elias\": df['answer'].corr(df['elias']),\n",
    "}\n",
    "\n",
    "for k, v in result_data.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(type(surv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_res = {\n",
    "    # \"label\": None,\n",
    "    \"SIMULATION_NAMEID\": SIMULATION_NAMEID,#SIM_ID,\n",
    "    \"timestamp\": bu.get_timestamp(),\n",
    "    \"survey_type\": str(type(surv)),\n",
    "    # \"temperature\": SETTINGS[\"temperature\"],\n",
    "    # \"note\": \"\",\n",
    "    \"exact_matches\": result_data[\"Exact Matches\"],\n",
    "    \"corr\": result_data[\"Correlation\"],\n",
    "    \"exact_matches_elias\": result_data[\"Exact Matches - elias\"],\n",
    "    \"corr_elias\": result_data[\"Correlation - elias\"],\n",
    "}\n",
    "\n",
    "tmp = bu.convert_dicts_to_table([new_res])\n",
    "res.append_data(tmp[1], tmp[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['airidas'] = df['airidas'].str.upper()\n",
    "# df['elias'] = df['elias'].str.upper()\n",
    "# df['answer'] = df['answer'].map(remap_dict)\n",
    "# df['airidas'] = df['airidas'].map(remap_dict)\n",
    "# df['elias'] = df['elias'].map(remap_dict)\n",
    "\n",
    "\n",
    "########################\n",
    "# df = df.drop(columns=['uppercase_text'])\n",
    "# df['CLONE_eli'] = df['answer'].apply(extract_uppercase_text)\n",
    "# df['CLONE_eli'] = df['CLONE_eli'].str.upper()\n",
    "# .str.upper() or .lower()\n",
    "# df['answer'] = df['answer'].map(remap_dict, na_action='ignore')\n",
    "#df['CLONE_eli'] = df['CLONE_eli'].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(surv, survey.KanoSurvey):\n",
    "    remap_dict = {\"I EXPECT IT\": 5, \"I LIKE IT\": 4, \"I AM NEUTRAL\": 3, \"I CAN TOLERATE IT\": 2, \"I DISLIKE IT\": 1}\n",
    "    df['answer'] = df['answer'].map(remap_dict)\n",
    "    df['airidas'] = df['airidas'].map(remap_dict)\n",
    "    df['elias'] = df['elias'].map(remap_dict)\n",
    "elif isinstance(surv, survey.PersonalitySurvey):\n",
    "    remap_dict = {\"AGREE\": 5, \"SOMEWHAT AGREE\": 4, \"NEUTRAL\": 3, \"SOMEWHAT DISAGREE\": 2, \"DISAGREE\": 1}\n",
    "    df['answer'] = df['answer'].map(remap_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remaps - UNIVERSAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute one number of how the percentage of correct answers\n",
    "print(f\"Exact Matches: {(df['CLONE_eli'] == df['IRL_eli']).sum() / len(df)}\")\n",
    "print(f\"Correlation: {df['CLONE_eli'].corr(df['IRL_eli'])}\")\n",
    "df['elias_correct'] = df['CLONE_eli'] == df['IRL_eli']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "import traceback\n",
    "import sys\n",
    "\n",
    "import brikasutils as bu\n",
    "import shared_utils as utils\n",
    "import survey\n",
    "importlib.reload(bu)\n",
    "importlib.reload(utils)\n",
    "importlib.reload(survey)\n",
    "\n",
    "queue = bu.FileRunQueue(queue_folder_path=\"batch/queue\", completed_folder_path=\"batch/done\")\n",
    "report_live_csv = bu.LiveCSV(\"batch/run_reports.csv\")\n",
    "timer = bu.Benchmarker()\n",
    "\n",
    "\n",
    "for filepath in queue:\n",
    "    timer.mark_start(filepath)\n",
    "\n",
    "    try: \n",
    "        ########## Handle batch stuff ########\n",
    "        filename = os.path.splitext(os.path.basename(filepath))[0]\n",
    "        with open(filepath, 'r') as f:\n",
    "            rundata = json.load(f)\n",
    "\n",
    "        # Load prompt file\n",
    "        with open(rundata[\"instructions\"][\"prompt_file\"], 'r') as f:\n",
    "            final_prompts = json.load(f)\n",
    "\n",
    "        # Make the surv\n",
    "        if rundata[\"instructions\"][\"survey_type\"] == \"KanoSurvey\":\n",
    "            surv = survey.KanoSurvey()\n",
    "        elif rundata[\"instructions\"][\"survey_type\"] == \"PersonalitySurvey\":\n",
    "            surv = survey.PersonalitySurvey()\n",
    "        else:\n",
    "            raise Exception(\"Invalid survey type\")\n",
    "\n",
    "        timestamp = bu.get_timestamp()\n",
    "        ######### Run Simulation ########\n",
    "        SIMULATION_NAMEID = filename\n",
    "        LIMIT = rundata[\"instructions\"][\"LIMIT\"] if \"LIMIT\" in rundata[\"instructions\"] else None\n",
    "        AUTO_INFO = {\n",
    "            \"date\": timestamp,\n",
    "            **rundata[\"info\"], # unpacked from rundata\n",
    "            \"limit\": LIMIT,\n",
    "            \"prompt_count\": min(len(final_prompts), LIMIT) if LIMIT != None else len(final_prompts),\n",
    "            \"avg_tokens_in_prompt\": round(utils.describe_prompts(final_prompts)[\"total_all_prompt_tokens\"]/len(final_prompts)),\n",
    "        }\n",
    "        SETTINGS = {\n",
    "            **rundata[\"settings\"], # unpacked from rundata\n",
    "        }\n",
    "\n",
    "        # client depends on if it's local or not\n",
    "        if rundata[\"instructions\"][\"isLocal\"]:\n",
    "            client = OpenAI(\n",
    "                base_url = 'http://localhost:11434/v1',\n",
    "                api_key='ollama', # required, but unused\n",
    "            )\n",
    "        else:\n",
    "            client = OpenAI(\n",
    "                api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "            )\n",
    "\n",
    "        completions = []\n",
    "        l = len(final_prompts)\n",
    "\n",
    "        for i, (prompt, question) in enumerate(list(zip(final_prompts, surv.questions))):\n",
    "            if LIMIT != None and i > LIMIT:\n",
    "                break\n",
    "\n",
    "            print(f\"{i}/{l}...\", end=\"\\t\") # Print progress\n",
    "            # Send the Request\n",
    "            full_response = client.chat.completions.create(\n",
    "                messages=prompt,\n",
    "                **SETTINGS,\n",
    "            )\n",
    "            r = full_response.choices[0].message.content\n",
    "\n",
    "            completions.append({'question': question, 'answer': r})\n",
    "\n",
    "            print(f\"{question}: {r}\")\n",
    "            \n",
    "        ############ Save Important results\n",
    "        df = pd.DataFrame(completions)\n",
    "        df.to_csv(f\"batch/output/{SIMULATION_NAMEID}_simulation.csv\", index=False)\n",
    "        bu.if_dir_not_exist_make(\"batch/output/info\")\n",
    "        bu.quickJSON({\"settings\": SETTINGS, \"info\": AUTO_INFO}, f\"batch/output/info/{SIMULATION_NAMEID}_info.json\")\n",
    "\n",
    "        status = \"OK\"\n",
    "    \n",
    "    except Exception:\n",
    "        print(f\"##### Error while running {filename}.\")\n",
    "        error_string = traceback.format_exc()\n",
    "        print(error_string)\n",
    "        status = \"Failed\"\n",
    "\n",
    "    ########### Time the run\n",
    "    try:\n",
    "        time_taken = timer.mark_end(filepath)\n",
    "    except:\n",
    "        print(\"Error while timing run: \")\n",
    "        print(traceback.format_exc())\n",
    "        time_taken = None\n",
    "\n",
    "    ########### Report the run\n",
    "    try:\n",
    "        new_report = {\n",
    "            \"filename\": filename,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"time_taken\": time_taken,\n",
    "            \"status\": status,\n",
    "            **rundata[\"instructions\"],\n",
    "            \"error\": error_string if status == \"Failed\" else \"\",\n",
    "        }\n",
    "\n",
    "        tmp = bu.convert_dicts_to_table([new_report])\n",
    "        report_live_csv.append_data(tmp[1], tmp[0])\n",
    "    except Exception as e:\n",
    "        print(f\"Error while reporting: \")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    print(f\"Processed {filename}. Stauts: {status}\")\n",
    "\n",
    "timer.print_total_execution_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Force short JSON (markdown) answer\n",
    "\n",
    "Add this to the end of your prompt:\n",
    "> ```json\n",
    "\n",
    "Add this to the \"stop\" sequence:\n",
    ">```\n",
    "\n",
    "----\n",
    "\n",
    "llama3:70b\n",
    "TEMPLATE \"\"\"{{ if .System }}<|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{{ .Response }}<|eot_id|>\"\"\"\n",
    "PARAMETER stop \"<|start_header_id|>\"\n",
    "PARAMETER stop \"<|end_header_id|>\"\n",
    "PARAMETER stop \"<|eot_id|>\"\n",
    "\n",
    "\n",
    "Mixtral\n",
    "TEMPLATE \"\"\" [INST] {{ .System }} {{ .Prompt }} [/INST]\"\"\"\n",
    "PARAMETER stop \"[INST]\"\n",
    "PARAMETER stop \"[/INST]\"\n",
    "\n",
    "----\n",
    "\n",
    "nomic_embed\n",
    "TEMPLATE \"\"\"{{ .Prompt }}\"\"\"\n",
    "PARAMETER num_ctx 8192"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
