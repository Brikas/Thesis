{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'shared_utils' from '/Users/twenythree/Home/Prot/Thesis/shared_utils.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib # Used to add reload functionality, for when we update our own custom libraries\n",
    "from typing import List\n",
    "\n",
    "# Custom libraries\n",
    "import brikasutils as bu\n",
    "importlib.reload(bu)\n",
    "import shared_utils as utils\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'persona' from '/Users/twenythree/Home/Prot/Thesis/persona.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Custom libraries\n",
    "import fb_msg_reader as fb\n",
    "importlib.reload(fb)\n",
    "import persona\n",
    "importlib.reload(persona)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 1946 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-09-13 to 2024-03-06\n",
      "Messages saved to self.chats['elias']\n",
      "Read 40036 messages from 5 files. Failed to read 0 messages.\n",
      "Messages ranged from 2020-08-17 to 2024-03-04\n",
      "Messages saved to self.chats['petyo']\n",
      "Read 7953 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2023-05-12 to 2024-03-04\n",
      "Messages saved to self.chats['anna']\n",
      "Read 5734 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2020-09-18 to 2024-03-02\n",
      "Messages saved to self.chats['patryk']\n",
      "Read 372 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2022-03-19 to 2024-02-24\n",
      "Messages saved to self.chats['andreas']\n",
      "Read 3399 messages from 2 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-08-23 to 2024-03-02\n",
      "Messages saved to self.chats['victoria']\n",
      "Read 2951 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2022-11-13 to 2024-02-19\n",
      "Messages saved to self.chats['joanna']\n",
      "Read 409 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2020-09-07 to 2024-01-24\n",
      "Messages saved to self.chats['antoni']\n",
      "Read 1661 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2020-09-02 to 2023-09-29\n",
      "Messages saved to self.chats['arijan']\n",
      "Read 1350 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2022-11-03 to 2024-02-24\n",
      "Messages saved to self.chats['denis']\n",
      "Read 553 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-01-01 to 2021-04-01\n",
      "Messages saved to self.chats['alexandra']\n",
      "Read 349 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2023-04-21 to 2023-05-17\n",
      "Messages saved to self.chats['FED']\n",
      "Read 661 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-09-03 to 2023-12-13\n",
      "Messages saved to self.chats['filip']\n",
      "Read 1520 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2020-09-11 to 2023-07-23\n",
      "Messages saved to self.chats['kuba']\n",
      "Read 332 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2020-09-01 to 2023-03-19\n",
      "Messages saved to self.chats['laura']\n",
      "Read 812 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2020-09-01 to 2023-08-10\n",
      "Messages saved to self.chats['liisa']\n",
      "Read 753 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2020-09-30 to 2024-01-15\n",
      "Messages saved to self.chats['luiza']\n",
      "Read 2899 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2022-09-06 to 2024-01-22\n",
      "Messages saved to self.chats['marcus']\n",
      "Filtering\n",
      "link-filter: 1354\n",
      "react-filter: 33\n",
      "cookie-data-filter: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/twenythree/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected chat elias for 8022 (557 messages)\n",
      "Selected chat petyo for 169785 (12332 messages)\n",
      "Selected chat anna for 28468 (2450 messages)\n",
      "Selected chat patryk for 16798 (1608 messages)\n",
      "Selected chat andreas for 1259 (113 messages)\n",
      "Selected chat victoria for 14144 (1051 messages)\n",
      "Selected chat joanna for 20464 (936 messages)\n",
      "Selected chat antoni for 1539 (129 messages)\n",
      "Selected chat arijan for 7040 (471 messages)\n",
      "Selected chat denis for 4345 (429 messages)\n",
      "Selected chat alexandra for 1809 (181 messages)\n",
      "Selected chat FED for 1434 (104 messages)\n",
      "Selected chat filip for 2342 (210 messages)\n",
      "Selected chat kuba for 5784 (488 messages)\n",
      "Selected chat laura for 1097 (106 messages)\n",
      "Selected chat liisa for 2596 (254 messages)\n",
      "Selected chat luiza for 3067 (230 messages)\n",
      "Selected chat marcus for 13006 (912 messages)\n",
      "Combined tokens: 302999\n"
     ]
    }
   ],
   "source": [
    "texts_with_elias = [\n",
    "    \"selected-data/elias/message_1.json\",\n",
    "]\n",
    "texts_with_petyo = [\n",
    "    \"selected-data/petyo/message_1.json\",\n",
    "    \"selected-data/petyo/message_2.json\",\n",
    "    \"selected-data/petyo/message_3.json\",\n",
    "    \"selected-data/petyo/message_4.json\",\n",
    "    \"selected-data/petyo/message_5.json\",\n",
    "]\n",
    "texts_with_others_dict = {\n",
    "    \"anna\": [\"selected-data/others/anna.json\"],\n",
    "    \"patryk\": [\"selected-data/others/patryk.json\"],\n",
    "    \"andreas\": [\"selected-data/others/andreas.json\"],\n",
    "    \"victoria\": [\"selected-data/others/victoria.json\", \"selected-data/others/victoria2.json\"],\n",
    "    \"joanna\": [\"selected-data/others/joanna.json\"],\n",
    "    \"antoni\": [\"selected-data/others/antoni.json\"],\n",
    "    \"arijan\": [\"selected-data/others/arijan.json\"],\n",
    "    \"denis\": [\"selected-data/others/denis.json\"],\n",
    "    \"alexandra\": [\"selected-data/others/alexandra.json\"],\n",
    "    \"FED\": [\"selected-data/others/FED.json\"],\n",
    "    \"filip\": [\"selected-data/others/filip.json\"],\n",
    "    \"kuba\": [\"selected-data/others/kuba.json\"],\n",
    "    \"laura\": [\"selected-data/others/laura.json\"],\n",
    "    \"liisa\": [\"selected-data/others/liisa.json\"],\n",
    "    \"luiza\": [\"selected-data/others/luiza.json\"],\n",
    "    \"marcus\": [\"selected-data/others/marcus.json\"],\n",
    "}\n",
    "\n",
    "\n",
    "ab = persona.PersonaEncoder()\n",
    "ab.parse_fb_messages(texts_with_elias, \"elias\")\n",
    "ab.parse_fb_messages(texts_with_petyo, \"petyo\")\n",
    "\n",
    "for name, texts in texts_with_others_dict.items():\n",
    "    ab.parse_fb_messages(texts, name)\n",
    "\n",
    "ab.filter_chats_empty()\n",
    "ab.filter_chats_regex(utils.BLACKLIST_CHAT_REGEX_FILTERS)\n",
    "\n",
    "# Compress names\n",
    "for nameid, chat in ab.chats.items():\n",
    "    for msg in chat:  \n",
    "        msg.sender = \"Persona\" if msg.sender == \"Airidas Brikas\" else \"Friend\"\n",
    "\n",
    "# Start all chats from 2/3rds\n",
    "for nameid, chat in ab.chats.items():\n",
    "    ab.chats[nameid] = chat[int(len(chat)/3 * 2):]\n",
    "\n",
    "# ab.select_chat_limited_by_tokens(\"elias\", 6000)\n",
    "# ab.select_chat_limited_by_tokens(\"petyo\", 6000)\n",
    "ab.select_chat_full(\"elias\")\n",
    "ab.select_chat_full(\"petyo\")\n",
    "\n",
    "for name in texts_with_others_dict.keys():\n",
    "    ab.select_chat_full(name)\n",
    "\n",
    "token_counts = ab.count_all_selected_chat_tokens() # token_counts used later for statistics\n",
    "print(f\"Combined tokens: {sum(token_counts.values())}\")\n",
    "persona_text = ab.output()\n",
    "bu.quickTXT(persona_text, filename=f\"ignorefolder/pt_{bu.get_timestamp()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat petyo has 169785 (12332 messages)\n"
     ]
    }
   ],
   "source": [
    "ab.count_chat_tokens(\"petyo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default Kano Survey CSV file: surveys/survey_kano-model_v1.csv\n"
     ]
    }
   ],
   "source": [
    "import survey\n",
    "importlib.reload(survey)\n",
    "surv = survey.KanoSurvey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & Definitions & Other Static Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import numpy as np\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'texts_with_elias' (list)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "EMBED_MODEL = \"nomic-embed-text\"        # nomic-embed-text = long ctx / mxbai-embed-large = big\n",
    "CHUNK_SIZE = 30                         # Number of messages per chunk\n",
    "OVERLAP_SIZE = 10                       # Number of overlapping messages between consecutive chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk count: 1112\n",
      "Average chunk character length: 1585\n",
      "Rough estimate of tokens per chunk: 396 (4 characters per token)\n",
      "Messagees in input count: 22561\n",
      "Messages in chunks count: 33360\n",
      "Chunk \\ Input ratio: 1.48 (OVERLAP_SIZE=10)\n",
      "Chunk Python type: <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# different chunk size\n",
    "chunks = []\n",
    "stat_total_msgs_in_chunks = 0 # for statistics\n",
    "\n",
    "# Iterate over chats and messages to create chunks\n",
    "for chat in ab.selectedChats.values():\n",
    "    messages = list(chat)  # Convert chat iterator to list for easier slicing\n",
    "    num_messages = len(messages)\n",
    "\n",
    "    # Create overlapping chunks of messages\n",
    "    for i in range(0, num_messages - CHUNK_SIZE + 1, CHUNK_SIZE - OVERLAP_SIZE):\n",
    "        chunk = messages[i:i + CHUNK_SIZE]  # Extract chunk of messages\n",
    "        chunk_text = \"\\n\".join(str(msg) for msg in chunk)  # Concatenate messages into a single string\n",
    "        chunks.append(chunk_text)  # Append chunk to list of chunks\n",
    "\n",
    "        stat_total_msgs_in_chunks += len(chunk) # For statistics\n",
    "\n",
    "\n",
    "##### Display Info\n",
    "total_messages = sum(len(chat) for chat in ab.selectedChats.values())\n",
    "chunks_count = len(chunks)\n",
    "avg_chunk_char_len = np.mean([len(chunk) for chunk in chunks])\n",
    "\n",
    "print(\n",
    "    f\"Chunk count: {chunks_count}\",\n",
    "    f\"Average chunk character length: {round( avg_chunk_char_len)}\",\n",
    "    f\"Rough estimate of tokens per chunk: {round(avg_chunk_char_len / 4)} (4 characters per token)\",\n",
    "    f\"Messagees in input count: {total_messages}\",\n",
    "    f\"Messages in chunks count: {stat_total_msgs_in_chunks}\",\n",
    "    f\"Chunk \\ Input ratio: {round(stat_total_msgs_in_chunks / total_messages,2)} (OVERLAP_SIZE={OVERLAP_SIZE})\",\n",
    "    f\"Chunk Python type: {type(chunks[0])}\",\n",
    "    sep=\"\\n\"\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks:1112, embeds:1112\n"
     ]
    }
   ],
   "source": [
    "########### Serialization ########\n",
    "EMBEDDING_NAMEID = \"test1\"\n",
    "AUTO_INFO = {\n",
    "    \"model\": EMBED_MODEL,\n",
    "    \"CHUNK_SIZE\": CHUNK_SIZE,\n",
    "    \"OVERLAP_SIZE\": OVERLAP_SIZE,\n",
    "    \"chunks_count\": chunks_count,\n",
    "    \"total_messages\": total_messages,\n",
    "    \"stat_total_msgs_in_chunks\": stat_total_msgs_in_chunks,\n",
    "    \"modules_chat\": token_counts,\n",
    "}\n",
    "##################################\n",
    "\n",
    "# Generate embeddings for each chunk\n",
    "embeddings = []\n",
    "\n",
    "progress, chunks_len = 0, len(chunks) # for progress bar\n",
    "for chunk_text in chunks:\n",
    "    progress += 1\n",
    "    print(f\"Chunk {progress}/{chunks_len}\")\n",
    "\n",
    "    embedding = ollama.embeddings(model=EMBED_MODEL, prompt=chunk_text)[\"embedding\"]\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "\n",
    "# Display and save results (if needed later)\n",
    "print(f\"Chunks:{len(chunks)}, embeds:{len(embeddings)}\")\n",
    "\n",
    "bu.if_dir_not_exist_make(\"embeddings\")\n",
    "bu.quickJSON(AUTO_INFO, f\"embeddings/{EMBEDDING_NAMEID}_info.json\")\n",
    "bu.quickJSON({\"chunks\": chunks, \"embeddings\": embeddings}, f\"embeddings/{EMBEDDING_NAMEID}_embeddings.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Embeddings From File (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks:1112, embeds:1112\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_NAMEID = \"test1\"\n",
    "\n",
    "import json\n",
    "with open(f\"embeddings/{EMBEDDING_NAMEID}_embeddings.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    chunks = data[\"chunks\"]\n",
    "    embeddings = data[\"embeddings\"]\n",
    "\n",
    "print(f\"Chunks:{len(chunks)}, embeds:{len(embeddings)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display simulations\n",
    "for item in most_similar_chunks:\n",
    "    print(chunks[item[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find cosine similarity of every chunk to a given embedding\n",
    "def find_most_similar(needle, haystack):\n",
    "    needle_norm = norm(needle)\n",
    "    similarity_scores = [\n",
    "        np.dot(needle, item) / (needle_norm * norm(item)) for item in haystack\n",
    "    ]\n",
    "    return sorted(zip(similarity_scores, range(len(haystack))), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens in chunks: 13347\n"
     ]
    }
   ],
   "source": [
    "RETRIEVAL_PROMPT = \"video game features\"\n",
    "# RETRIEVAL_PROMPT = \"personality\"\n",
    "CHUNKS_COUNT_IN_CTX = 35 # Number of nearby chunks to put in context window\n",
    "# COMMENT 04-16, perhaps we could try 5x retrievals with isolated semantics\n",
    "\n",
    "# Perform similarity search and print simulations\n",
    "prompt_embedding = ollama.embeddings(model=EMBED_MODEL, prompt=RETRIEVAL_PROMPT)[\"embedding\"]\n",
    "chunks_most_similar_embeddings = find_most_similar(prompt_embedding, embeddings)[:CHUNKS_COUNT_IN_CTX]\n",
    "chunks_most_similar = []\n",
    "for embedding in chunks_most_similar_embeddings:\n",
    "    chunks_most_similar.append(chunks[embedding[1]])\n",
    "\n",
    "# token counts in all similar chunks\n",
    "tokens_in_chunks = 0\n",
    "for chunk in chunks_most_similar:\n",
    "    tokens_in_chunks += utils.count_tokens(chunk)\n",
    "print(f\"Tokens in chunks: {tokens_in_chunks}\")\n",
    "\n",
    "bu.quickTXT(\"\\n\\n\".join(chunks_most_similar), filename=\"ignorefolder/chunks.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Embeddings (dynamic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survey Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Builder - Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def userMsg(*args) -> dict:\n",
    "    return {\"role\": \"user\", \"content\": \"\\n\".join(args)}\n",
    "def assistantMsg(*args) -> dict:\n",
    "    return {\"role\": \"assistant\", \"content\": \"\\n\".join(args)}\n",
    "def systemMsg(*args) -> dict:\n",
    "    return {\"role\": \"system\", \"content\": \"\\n\".join(args)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Builder - Persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 50 prompts.\n",
      "Average prompt size: 13706 tokens.\n",
      "Min prompt size: 13703, Max prompt size: 13711\n"
     ]
    }
   ],
   "source": [
    "final_prompts = []\n",
    "\n",
    "for question in surv.questions:\n",
    "    p = [\n",
    "        systemMsg(\n",
    "            \"You are specialized in impersonating people. You will be presented to the subject through explicit datapoints of their digital footprint. In addition, you will deduct their implicit tastes by shadowing chats between the subject and friends. You will be asked to answer questions from the point of view of the persona. Text below:\",\n",
    "            \"Conversations between persona and friends\",\n",
    "            \"\\nNEW CONVERSATION:\\n\".join(chunks_most_similar)\n",
    "        ),\n",
    "        # Understanding affirmation\n",
    "        assistantMsg('I will answer from the point of view of the persona, based on what I could the deduct from the text provided.'),\n",
    "        # Survey question. With Simulation\n",
    "        userMsg(\"\\n\".join([\n",
    "            f\"Persona is surveyed about their video game survey. The persona must choose answer the question below with one of the given options: {', '.join(surv.POSSIBLE_ANSWERS)}. Persona's answer must only contain the chosen option. \",\n",
    "            question,\n",
    "            \"The persona chooses: \"\n",
    "        ])),\n",
    "    ]\n",
    "    final_prompts.append(p)\n",
    "\n",
    "# Report prompt tokens\n",
    "total_all_prompt_tokens = 0 #used elsewhere too\n",
    "prompt_tokens_min = 0\n",
    "prompt_tokens_max = 0\n",
    "for p in final_prompts:\n",
    "    pt = 0 # Prompt tokens\n",
    "    for msg in p:\n",
    "        pt += utils.count_tokens(msg[\"content\"])\n",
    "    if prompt_tokens_min == 0 or pt < prompt_tokens_min:\n",
    "        prompt_tokens_min = pt\n",
    "    if pt > prompt_tokens_max:\n",
    "        prompt_tokens_max = pt\n",
    "\n",
    "    total_all_prompt_tokens += pt\n",
    "\n",
    "print(f\"Created {len(final_prompts)} prompts.\")\n",
    "print(f\"Average prompt size: {round(total_all_prompt_tokens/len(final_prompts))} tokens.\")\n",
    "print(f\"Min prompt size: {prompt_tokens_min}, Max prompt size: {prompt_tokens_max}\")\n",
    "\n",
    "bu.quickJSON(final_prompts, \"ignorefolder/prompts.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Builder - Base case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 40 prompts.\n",
      "Average prompt size: 110 tokens.\n",
      "Min prompt size: 105, Max prompt size: 118\n"
     ]
    }
   ],
   "source": [
    "final_prompts = []\n",
    "\n",
    "for question in surv.questions:\n",
    "    p = [\n",
    "        systemMsg(\n",
    "            \"You are participating in a survey. You will be presented with a series of questions about your video game preferrences.\",\n",
    "            f\"You must choose answer to the question below with one of the five options: {', '.join(surv.POSSIBLE_ANSWERS)}. The answer must only contain the chosen option. \"\n",
    "        ),\n",
    "        # Understanding affirmation\n",
    "        assistantMsg('Understood. I will answer the question below with one of the given options.'),\n",
    "        # Survey question. With Simulation\n",
    "        userMsg(\n",
    "            question,\n",
    "            \"Your choice: \"\n",
    "        ),\n",
    "    ]\n",
    "    final_prompts.append(p)\n",
    "\n",
    "# Report prompt tokens\n",
    "total_all_prompt_tokens = 0\n",
    "prompt_tokens_min = 0\n",
    "prompt_tokens_max = 0\n",
    "for p in final_prompts:\n",
    "    pt = 0 # Prompt tokens\n",
    "    for msg in p:\n",
    "        pt += utils.count_tokens(msg[\"content\"])\n",
    "    if prompt_tokens_min == 0 or pt < prompt_tokens_min:\n",
    "        prompt_tokens_min = pt\n",
    "    if pt > prompt_tokens_max:\n",
    "        prompt_tokens_max = pt\n",
    "\n",
    "    total_all_prompt_tokens += pt\n",
    "\n",
    "print(f\"Created {len(final_prompts)} prompts.\")\n",
    "print(f\"Average prompt size: {round(total_all_prompt_tokens/len(final_prompts))} tokens.\")\n",
    "print(f\"Min prompt size: {prompt_tokens_min}, Max prompt size: {prompt_tokens_max}\")\n",
    "\n",
    "bu.quickJSON(final_prompts, \"ignorefolder/prompts.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Unused / Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "persona_text = \"Favorite video games are Rimworld, Minecraft, Age of Empires, 7 Days to Die\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Survey Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/40...\tWhat would you say if there were options to design your own avatar?: I LIKE IT\n",
      "1/40...\tWhat would you say if there were NO options to design your own avatar?: I LIKE IT\n",
      "2/40...\tWhat would you say if the game had the option to save the game at any time?: I LIKE IT\n",
      "3/40...\tWhat would you say if the game did NOT have the option to save the game at any time?: I EXPECT IT\n",
      "4/40...\tWhat would you say if the game has good graphics?: I LIKE IT\n",
      "5/40...\tWhat would you say if the game had NO good graphics, or rather poor graphics?: I CAN TOLERATE IT\n",
      "6/40...\tWhat would you say if the game had an exciting storyline?: I LIKE IT\n",
      "7/40...\tWhat would you say if the game did NOT have an exciting storyline?: I EXPECT IT\n",
      "8/40...\tWhat would you say if there were rewards such as extra points, in-game currency or coins in the game?: I LIKE IT\n",
      "9/40...\tWhat would you say if there were NO rewards such as extra points, in-game currency or coins in the game?: I EXPECT IT\n",
      "10/40...\tWhat would you say if the game had realistic game physics?: I LIKE IT\n",
      "11/40...\tWhat would you say if the game does NOT have realistic physics?: I EXPECT IT\n",
      "12/40...\tWhat would you say if the game had a relaxed flow rather than being very exciting?: I EXPECT IT\n",
      "13/40...\tWhat would you say if the game DON'T have a relaxed flow?: I LIKE IT\n",
      "14/40...\tWhat would you say if the game had a multiplayer mode?: I LIKE IT\n",
      "15/40...\tWhat would you say if the game does NOT have multiplayer mode?: I CAN TOLERATE IT\n",
      "16/40...\tWhat would you say if you can loot defeated enemies in the game?: I LIKE IT\n",
      "17/40...\tWhat would you say if you CANNOT loot defeated enemies in the game?: I CAN TOLERATE IT\n",
      "18/40...\tWhat would you say if the game had cutscenes?: I LIKE IT\n",
      "19/40...\tWhat would you say if the game DIDN'T have cutscenes?: I LIKE IT\n",
      "20/40...\tWhat would you say if the game had more and increasingly difficult levels?: I LIKE IT\n",
      "21/40...\tWhat would you say if the game had NO levels, or the difficulty didn't increase?: I EXPECT IT\n",
      "22/40...\tWhat would you say if the game had a high score table or hall of fame?: I LIKE IT\n",
      "23/40...\tWhat would you say if the game does NOT have a high score table or hall of fame?: I EXPECT IT\n",
      "24/40...\tWhat would you say if the game contains role-playing elements?: I LIKE IT\n",
      "25/40...\tWhat would you say if the game contained NO role-playing elements?: I EXPECT IT\n",
      "26/40...\tWhat would you say if the game had realistic graphics?: I LIKE IT\n",
      "27/40...\tWhat would you say if the game DON'T have realistic graphics?: I EXPECT IT\n",
      "28/40...\tWhat would you say if the game offered the opportunity to level up the character?: I LIKE IT\n",
      "29/40...\tWhat would you say if the game does NOT provide a way to level up the character?: I EXPECT IT\n",
      "30/40...\tWhat would you say if the game contained violent elements?: I LIKE IT\n",
      "31/40...\tWhat would you say if the game contained NO violent elements?: I LIKE IT\n",
      "32/40...\tWhat would you say if the game gave you creative freedom?: I LIKE IT\n",
      "33/40...\tWhat would you say if the game DIDN'T give you creative freedom?: I LIKE IT\n",
      "34/40...\tWhat would you say if the game had a compelling plot or narrative?: I LIKE IT\n",
      "35/40...\tWhat would you say if the game DIDN'T have a compelling plot or narrative?: I EXPECT IT\n",
      "36/40...\tWhat would you say if the game had quests?: I LIKE IT\n",
      "37/40...\tWhat would you say if the game did NOT have quests?: I LIKE IT\n",
      "38/40...\tWhat would you say if the game had a clear ending or goal?: I LIKE IT\n",
      "39/40...\tWhat would you say if the game does NOT have an ending or clear goal?: I LIKE IT\n"
     ]
    }
   ],
   "source": [
    "# Run Simulation\n",
    "##################################\n",
    "SIMULATION_NAMEID = \"run2-airidas-video-game_smaller-context_cv1\"\n",
    "LIMIT = None # For testing purposes. Set to NONE to run all\n",
    "AUTO_INFO = {\n",
    "    \"date\": bu.get_timestamp(),\n",
    "    \"EMBEDDING_NAMEID\": EMBEDDING_NAMEID,\n",
    "    \"RETRIEVAL_PROMPT\": RETRIEVAL_PROMPT,\n",
    "    \"CHUNKS_COUNT_IN_CTX\": CHUNKS_COUNT_IN_CTX,\n",
    "    \"survey_type\": str(type(surv)),\n",
    "    \"prompt_count\": min(len(final_prompts), LIMIT) if LIMIT != None else len(final_prompts),\n",
    "    \"avg_tokens_in_prompt\": round(total_all_prompt_tokens/len(final_prompts)),\n",
    "}\n",
    "SETTINGS = {\n",
    "    \"model\": \"gpt-3.5-turbo\",\n",
    "    \"temperature\": 0.5,\n",
    "}\n",
    "\n",
    "##################################\n",
    "\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "completions = []\n",
    "l = len(final_prompts)\n",
    "\n",
    "for i, (prompt, question) in enumerate(list(zip(final_prompts, surv.questions))):\n",
    "    if LIMIT != None and i > LIMIT:\n",
    "        break\n",
    "\n",
    "    print(f\"{i}/{l}...\", end=\"\\t\") # Print progress\n",
    "    # Send the Request\n",
    "    full_response = client.chat.completions.create(\n",
    "        messages=prompt,\n",
    "        model=SETTINGS[\"model\"],\n",
    "        temperature=SETTINGS[\"temperature\"],\n",
    "    )\n",
    "    r = full_response.choices[0].message.content\n",
    "\n",
    "    completions.append({'question': question, 'answer': r})\n",
    "\n",
    "    print(f\"{question}: {r}\")\n",
    "\n",
    "# Save results\n",
    "df = pd.DataFrame(completions)\n",
    "df.to_csv(f\"simulations/{SIMULATION_NAMEID}_simulation.csv\", index=False)\n",
    "bu.quickJSON({\"settings\": SETTINGS, \"info\": AUTO_INFO}, f\"simulations/{SIMULATION_NAMEID}_info.json\")\n",
    "bu.quickJSON(final_prompts, f\"ignorefolder/simulations/{SIMULATION_NAMEID}_prompts.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "bu.quickJSON(final_prompts, f\"ignorefolder/simulations/{SIMULATION_NAMEID}_prompts.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis - General"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Simulation File (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings and info loaded:\n",
      "date: 2024-04-17_222912\n",
      "EMBEDDING_NAMEID: test1\n",
      "RETRIEVAL_PROMPT: openess conciousness extrovert aggreableness neuroticism\n",
      "CHUNKS_COUNT_IN_CTX: 30\n",
      "survey_type: <class 'survey.PersonalitySurvey'>\n",
      "prompt_count: 50\n",
      "model: gpt-3.5-turbo\n",
      "temperature: 0.5\n",
      "WARNING: surv variable is not of the same type. <class 'survey.KanoSurvey'> != <class 'survey.PersonalitySurvey'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am the life of the party.</td>\n",
       "      <td>AGREE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I don't talk a lot.</td>\n",
       "      <td>DISAGREE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I feel comfortable around people.</td>\n",
       "      <td>AGREE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I keep in the background.</td>\n",
       "      <td>DISAGREE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I start conversations.</td>\n",
       "      <td>AGREE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I have little to say.</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I talk to a lot of different people at parties.</td>\n",
       "      <td>AGREE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I don't like to draw attention to myself.</td>\n",
       "      <td>DISAGREE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I don't mind being the center of attention.</td>\n",
       "      <td>AGREE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I am quiet around strangers.</td>\n",
       "      <td>SOMEWHAT AGREE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          question          answer\n",
       "0                      I am the life of the party.           AGREE\n",
       "1                              I don't talk a lot.        DISAGREE\n",
       "2                I feel comfortable around people.           AGREE\n",
       "3                        I keep in the background.        DISAGREE\n",
       "4                           I start conversations.           AGREE\n",
       "5                            I have little to say.         NEUTRAL\n",
       "6  I talk to a lot of different people at parties.           AGREE\n",
       "7        I don't like to draw attention to myself.        DISAGREE\n",
       "8      I don't mind being the center of attention.           AGREE\n",
       "9                     I am quiet around strangers.  SOMEWHAT AGREE"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "SIMULATION_NAMEID = \"run2-airidas-personality_cv3\"\n",
    "\n",
    "df = pd.read_csv(f'simulations/{SIMULATION_NAMEID}_simulation.csv')\n",
    "# df = df.drop(df.columns[0], axis=1) #if loaded from csv, drop the added index col\n",
    "\n",
    "with open(f'simulations/{SIMULATION_NAMEID}_info.json', 'r') as f:\n",
    "    loaded = json.load(f)\n",
    "try:\n",
    "    AUTO_INFO = loaded[\"info\"]\n",
    "    SETTINGS = loaded[\"settings\"]\n",
    "    print(\"Settings and info loaded:\")\n",
    "    for k, v in AUTO_INFO.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "    for k, v in SETTINGS.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "except:\n",
    "    print(\"No settings and/or info found\")\n",
    "\n",
    "\n",
    "try:\n",
    "    if str(type(surv)) != AUTO_INFO[\"survey_type\"]:\n",
    "        print(f\"WARNING: surv variable is not of the same type. {str(type(surv))} != {AUTO_INFO['survey_type']}\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "df.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All answers were valid\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What would you say if there were options to de...</td>\n",
       "      <td>I LIKE IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What would you say if there were NO options to...</td>\n",
       "      <td>I LIKE IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What would you say if the game had the option ...</td>\n",
       "      <td>I LIKE IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What would you say if the game did NOT have th...</td>\n",
       "      <td>I EXPECT IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What would you say if the game has good graphics?</td>\n",
       "      <td>I LIKE IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What would you say if the game had NO good gra...</td>\n",
       "      <td>I CAN TOLERATE IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What would you say if the game had an exciting...</td>\n",
       "      <td>I LIKE IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What would you say if the game did NOT have an...</td>\n",
       "      <td>I EXPECT IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What would you say if there were rewards such ...</td>\n",
       "      <td>I LIKE IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What would you say if there were NO rewards su...</td>\n",
       "      <td>I EXPECT IT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question             answer\n",
       "0  What would you say if there were options to de...          I LIKE IT\n",
       "1  What would you say if there were NO options to...          I LIKE IT\n",
       "2  What would you say if the game had the option ...          I LIKE IT\n",
       "3  What would you say if the game did NOT have th...        I EXPECT IT\n",
       "4  What would you say if the game has good graphics?          I LIKE IT\n",
       "5  What would you say if the game had NO good gra...  I CAN TOLERATE IT\n",
       "6  What would you say if the game had an exciting...          I LIKE IT\n",
       "7  What would you say if the game did NOT have an...        I EXPECT IT\n",
       "8  What would you say if there were rewards such ...          I LIKE IT\n",
       "9  What would you say if there were NO rewards su...        I EXPECT IT"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove all characters from a black list from the column answer\n",
    "for substr in utils.BLACKLIST_ANSWER_SUBSTRINGS:\n",
    "    df['answer'] = df['answer'].apply(lambda x: re.sub(substr, \"\", x))\n",
    "\n",
    "df['answer'] = df['answer'].str.upper()\n",
    "# Update isValid\n",
    "df['isValid'] = df['answer'].apply(lambda x: x in surv.POSSIBLE_ANSWERS)\n",
    "\n",
    "# if all values in isValid is true, drop the column, else print a message\n",
    "if df['isValid'].all():\n",
    "    df = df.drop('isValid', axis=1)\n",
    "    print(\"All answers were valid\")\n",
    "else:\n",
    "    print(\"Some answers were not valid\")\n",
    "\n",
    "df.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proces simulation output - KANO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>airidas</th>\n",
       "      <th>elias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What would you say if there were options to de...</td>\n",
       "      <td>I LIKE IT</td>\n",
       "      <td>I AM NEUTRAL</td>\n",
       "      <td>I AM NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What would you say if there were NO options to...</td>\n",
       "      <td>I LIKE IT</td>\n",
       "      <td>I CAN TOLERATE IT</td>\n",
       "      <td>I EXPECT IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What would you say if the game had the option ...</td>\n",
       "      <td>I LIKE IT</td>\n",
       "      <td>I EXPECT IT</td>\n",
       "      <td>I EXPECT IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What would you say if the game did NOT have th...</td>\n",
       "      <td>I EXPECT IT</td>\n",
       "      <td>I DISLIKE IT</td>\n",
       "      <td>I CAN TOLERATE IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What would you say if the game has good graphics?</td>\n",
       "      <td>I LIKE IT</td>\n",
       "      <td>I LIKE IT</td>\n",
       "      <td>I LIKE IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What would you say if the game had NO good gra...</td>\n",
       "      <td>I CAN TOLERATE IT</td>\n",
       "      <td>I AM NEUTRAL</td>\n",
       "      <td>I CAN TOLERATE IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What would you say if the game had an exciting...</td>\n",
       "      <td>I LIKE IT</td>\n",
       "      <td>I LIKE IT</td>\n",
       "      <td>I LIKE IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What would you say if the game did NOT have an...</td>\n",
       "      <td>I EXPECT IT</td>\n",
       "      <td>I CAN TOLERATE IT</td>\n",
       "      <td>I DISLIKE IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What would you say if there were rewards such ...</td>\n",
       "      <td>I LIKE IT</td>\n",
       "      <td>I LIKE IT</td>\n",
       "      <td>I DISLIKE IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What would you say if there were NO rewards su...</td>\n",
       "      <td>I EXPECT IT</td>\n",
       "      <td>I DISLIKE IT</td>\n",
       "      <td>I LIKE IT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question             answer  \\\n",
       "0  What would you say if there were options to de...          I LIKE IT   \n",
       "1  What would you say if there were NO options to...          I LIKE IT   \n",
       "2  What would you say if the game had the option ...          I LIKE IT   \n",
       "3  What would you say if the game did NOT have th...        I EXPECT IT   \n",
       "4  What would you say if the game has good graphics?          I LIKE IT   \n",
       "5  What would you say if the game had NO good gra...  I CAN TOLERATE IT   \n",
       "6  What would you say if the game had an exciting...          I LIKE IT   \n",
       "7  What would you say if the game did NOT have an...        I EXPECT IT   \n",
       "8  What would you say if there were rewards such ...          I LIKE IT   \n",
       "9  What would you say if there were NO rewards su...        I EXPECT IT   \n",
       "\n",
       "             airidas              elias  \n",
       "0       I AM NEUTRAL       I AM NEUTRAL  \n",
       "1  I CAN TOLERATE IT        I EXPECT IT  \n",
       "2        I EXPECT IT        I EXPECT IT  \n",
       "3       I DISLIKE IT  I CAN TOLERATE IT  \n",
       "4          I LIKE IT          I LIKE IT  \n",
       "5       I AM NEUTRAL  I CAN TOLERATE IT  \n",
       "6          I LIKE IT          I LIKE IT  \n",
       "7  I CAN TOLERATE IT       I DISLIKE IT  \n",
       "8          I LIKE IT       I DISLIKE IT  \n",
       "9       I DISLIKE IT          I LIKE IT  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add airidas and elias answers\n",
    "air = surv.test_answers[\"airidas\"]\n",
    "eli = surv.test_answers[\"elias\"]\n",
    "df.insert(2, \"airidas\", air[:len(df)])\n",
    "df.insert(3, \"elias\", eli[:len(df)])\n",
    "\n",
    "df['answer'] = df['answer'].str.upper()\n",
    "df['airidas'] = df['airidas'].str.upper()\n",
    "df['elias'] = df['elias'].str.upper()\n",
    "df.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proces simulation output - PERSONALITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>airidas</th>\n",
       "      <th>elias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am the life of the party.</td>\n",
       "      <td>AGREE</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I don't talk a lot.</td>\n",
       "      <td>SOMEWHAT AGREE</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I feel comfortable around people.</td>\n",
       "      <td>AGREE</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I keep in the background.</td>\n",
       "      <td>SOMEWHAT AGREE</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I start conversations.</td>\n",
       "      <td>AGREE</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I have little to say.</td>\n",
       "      <td>DISAGREE</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I talk to a lot of different people at parties.</td>\n",
       "      <td>AGREE</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I don't like to draw attention to myself.</td>\n",
       "      <td>SOMEWHAT AGREE</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I don't mind being the center of attention.</td>\n",
       "      <td>SOMEWHAT DISAGREE</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I am quiet around strangers.</td>\n",
       "      <td>AGREE</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          question             answer  \\\n",
       "0                      I am the life of the party.              AGREE   \n",
       "1                              I don't talk a lot.     SOMEWHAT AGREE   \n",
       "2                I feel comfortable around people.              AGREE   \n",
       "3                        I keep in the background.     SOMEWHAT AGREE   \n",
       "4                           I start conversations.              AGREE   \n",
       "5                            I have little to say.           DISAGREE   \n",
       "6  I talk to a lot of different people at parties.              AGREE   \n",
       "7        I don't like to draw attention to myself.     SOMEWHAT AGREE   \n",
       "8      I don't mind being the center of attention.  SOMEWHAT DISAGREE   \n",
       "9                     I am quiet around strangers.              AGREE   \n",
       "\n",
       "   airidas  elias  \n",
       "0        3      3  \n",
       "1        2      2  \n",
       "2        4      4  \n",
       "3        2      4  \n",
       "4        4      3  \n",
       "5        1      1  \n",
       "6        5      4  \n",
       "7        2      3  \n",
       "8        4      1  \n",
       "9        1      4  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add airidas and elias answers\n",
    "air = surv.test_answers[\"airidas\"]\n",
    "eli = surv.test_answers[\"elias\"]\n",
    "df.insert(2, \"airidas\", air[:len(df)])\n",
    "df.insert(3, \"elias\", eli[:len(df)])\n",
    "df.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remaps - UNIVERSAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>airidas</th>\n",
       "      <th>elias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What would you say if there were options to de...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What would you say if there were NO options to...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What would you say if the game had the option ...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What would you say if the game did NOT have th...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What would you say if the game has good graphics?</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  answer  airidas  elias\n",
       "0  What would you say if there were options to de...       4        3      3\n",
       "1  What would you say if there were NO options to...       4        2      5\n",
       "2  What would you say if the game had the option ...       4        5      5\n",
       "3  What would you say if the game did NOT have th...       5        1      2\n",
       "4  What would you say if the game has good graphics?       4        4      4"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if isinstance(surv, survey.KanoSurvey):\n",
    "    remap_dict = {\"I EXPECT IT\": 5, \"I LIKE IT\": 4, \"I AM NEUTRAL\": 3, \"I CAN TOLERATE IT\": 2, \"I DISLIKE IT\": 1}\n",
    "    df['answer'] = df['answer'].map(remap_dict)\n",
    "    df['airidas'] = df['airidas'].map(remap_dict)\n",
    "    df['elias'] = df['elias'].map(remap_dict)\n",
    "elif isinstance(surv, survey.PersonalitySurvey):\n",
    "    remap_dict = {\"AGREE\": 5, \"SOMEWHAT AGREE\": 4, \"NEUTRAL\": 3, \"SOMEWHAT DISAGREE\": 2, \"DISAGREE\": 1}\n",
    "    df['answer'] = df['answer'].map(remap_dict)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Matches: 0.325\n",
      "Correlation: -0.06554567164164832\n",
      "Exact Matches - elias: 0.4\n",
      "Correlation - elias: 0.014175914545213615\n"
     ]
    }
   ],
   "source": [
    "# compute one number of how the percentage of correct answers\n",
    "result_data = {\n",
    "    \"Exact Matches\": (df['answer'] == df['airidas']).sum() / len(df),\n",
    "    \"Correlation\": df['answer'].corr(df['airidas']),\n",
    "    \"Exact Matches - elias\": (df['answer'] == df['elias']).sum() / len(df),\n",
    "    \"Correlation - elias\": df['answer'].corr(df['elias']),\n",
    "}\n",
    "\n",
    "for k, v in result_data.items():\n",
    "    print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Results CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognized 0 headers in results/airidas_results.csv\n"
     ]
    }
   ],
   "source": [
    "bu.if_dir_not_exist_make(\"results\")\n",
    "res = bu.LiveCSV(\"results/airidas_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brikasutils.quickCSV: Saved 14 as results/airidas_results.csv\n"
     ]
    }
   ],
   "source": [
    "new_res = {\n",
    "    # \"label\": None,\n",
    "    \"SIMULATION_NAMEID\": SIMULATION_NAMEID,\n",
    "    \"timestamp\": bu.get_timestamp(),\n",
    "    \"survey_type\": str(type(surv)),\n",
    "    \"temperature\": SETTINGS[\"temperature\"],\n",
    "    # \"note\": \"\",\n",
    "    \"exact_matches\": result_data[\"Exact Matches\"],\n",
    "    \"corr\": result_data[\"Correlation\"],\n",
    "    \"exact_matches_elias\": result_data[\"Exact Matches - elias\"],\n",
    "    \"corr_elias\": result_data[\"Correlation - elias\"],\n",
    "}\n",
    "\n",
    "tmp = bu.convert_dicts_to_table([new_res])\n",
    "res.append_data(tmp[1], tmp[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What would you say if there were options to de...</td>\n",
       "      <td>I LIKE IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What would you say if there were NO options to...</td>\n",
       "      <td>I DISLIKE IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What would you say if the game had the option ...</td>\n",
       "      <td>I EXPECT IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What would you say if the game did NOT have th...</td>\n",
       "      <td>I EXPECT IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What would you say if the game has good graphics?</td>\n",
       "      <td>I LIKE IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What would you say if the game had NO good gra...</td>\n",
       "      <td>I CAN TOLERATE IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What would you say if the game had an exciting...</td>\n",
       "      <td>I LIKE IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What would you say if the game did NOT have an...</td>\n",
       "      <td>I CAN TOLERATE IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What would you say if there were rewards such ...</td>\n",
       "      <td>I EXPECT IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What would you say if there were NO rewards su...</td>\n",
       "      <td>I LIKE IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What would you say if the game had realistic g...</td>\n",
       "      <td>I LIKE IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What would you say if the game does NOT have r...</td>\n",
       "      <td>I CAN TOLERATE IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What would you say if the game had a relaxed f...</td>\n",
       "      <td>I LIKE IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What would you say if the game DON'T have a re...</td>\n",
       "      <td>I DISLIKE IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>What would you say if the game had a multiplay...</td>\n",
       "      <td>I LIKE IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>What would you say if the game does NOT have m...</td>\n",
       "      <td>I LIKE IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>What would you say if you can loot defeated en...</td>\n",
       "      <td>I LIKE IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>What would you say if you CANNOT loot defeated...</td>\n",
       "      <td>I DISLIKE IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>What would you say if the game had cutscenes?</td>\n",
       "      <td>I LIKE IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>What would you say if the game DIDN'T have cut...</td>\n",
       "      <td>I LIKE IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>What would you say if the game had more and in...</td>\n",
       "      <td>I EXPECT IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>What would you say if the game had NO levels, ...</td>\n",
       "      <td>I EXPECT IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>What would you say if the game had a high scor...</td>\n",
       "      <td>I LIKE IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>What would you say if the game does NOT have a...</td>\n",
       "      <td>I CAN TOLERATE IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>What would you say if the game contains role-p...</td>\n",
       "      <td>I LIKE IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>What would you say if the game contained NO ro...</td>\n",
       "      <td>I LIKE IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>What would you say if the game had realistic g...</td>\n",
       "      <td>I LIKE IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>What would you say if the game DON'T have real...</td>\n",
       "      <td>I LIKE IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>What would you say if the game offered the opp...</td>\n",
       "      <td>I LIKE IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>What would you say if the game does NOT provid...</td>\n",
       "      <td>I EXPECT IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>What would you say if the game contained viole...</td>\n",
       "      <td>I CAN TOLERATE IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>What would you say if the game contained NO vi...</td>\n",
       "      <td>I LIKE IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>What would you say if the game gave you creati...</td>\n",
       "      <td>I LIKE IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>What would you say if the game DIDN'T give you...</td>\n",
       "      <td>I DISLIKE IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>What would you say if the game had a compellin...</td>\n",
       "      <td>I LIKE IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>What would you say if the game DIDN'T have a c...</td>\n",
       "      <td>I CAN TOLERATE IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>What would you say if the game had quests?</td>\n",
       "      <td>I LIKE IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>What would you say if the game did NOT have qu...</td>\n",
       "      <td>I EXPECT IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>What would you say if the game had a clear end...</td>\n",
       "      <td>I LIKE IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>What would you say if the game does NOT have a...</td>\n",
       "      <td>I DISLIKE IT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question             answer\n",
       "0   What would you say if there were options to de...          I LIKE IT\n",
       "1   What would you say if there were NO options to...       I DISLIKE IT\n",
       "2   What would you say if the game had the option ...        I EXPECT IT\n",
       "3   What would you say if the game did NOT have th...        I EXPECT IT\n",
       "4   What would you say if the game has good graphics?          I LIKE IT\n",
       "5   What would you say if the game had NO good gra...  I CAN TOLERATE IT\n",
       "6   What would you say if the game had an exciting...          I LIKE IT\n",
       "7   What would you say if the game did NOT have an...  I CAN TOLERATE IT\n",
       "8   What would you say if there were rewards such ...        I EXPECT IT\n",
       "9   What would you say if there were NO rewards su...          I LIKE IT\n",
       "10  What would you say if the game had realistic g...          I LIKE IT\n",
       "11  What would you say if the game does NOT have r...  I CAN TOLERATE IT\n",
       "12  What would you say if the game had a relaxed f...          I LIKE IT\n",
       "13  What would you say if the game DON'T have a re...       I DISLIKE IT\n",
       "14  What would you say if the game had a multiplay...          I LIKE IT\n",
       "15  What would you say if the game does NOT have m...          I LIKE IT\n",
       "16  What would you say if you can loot defeated en...          I LIKE IT\n",
       "17  What would you say if you CANNOT loot defeated...       I DISLIKE IT\n",
       "18      What would you say if the game had cutscenes?          I LIKE IT\n",
       "19  What would you say if the game DIDN'T have cut...          I LIKE IT\n",
       "20  What would you say if the game had more and in...        I EXPECT IT\n",
       "21  What would you say if the game had NO levels, ...        I EXPECT IT\n",
       "22  What would you say if the game had a high scor...          I LIKE IT\n",
       "23  What would you say if the game does NOT have a...  I CAN TOLERATE IT\n",
       "24  What would you say if the game contains role-p...          I LIKE IT\n",
       "25  What would you say if the game contained NO ro...          I LIKE IT\n",
       "26  What would you say if the game had realistic g...          I LIKE IT\n",
       "27  What would you say if the game DON'T have real...          I LIKE IT\n",
       "28  What would you say if the game offered the opp...          I LIKE IT\n",
       "29  What would you say if the game does NOT provid...        I EXPECT IT\n",
       "30  What would you say if the game contained viole...  I CAN TOLERATE IT\n",
       "31  What would you say if the game contained NO vi...          I LIKE IT\n",
       "32  What would you say if the game gave you creati...          I LIKE IT\n",
       "33  What would you say if the game DIDN'T give you...       I DISLIKE IT\n",
       "34  What would you say if the game had a compellin...          I LIKE IT\n",
       "35  What would you say if the game DIDN'T have a c...  I CAN TOLERATE IT\n",
       "36         What would you say if the game had quests?          I LIKE IT\n",
       "37  What would you say if the game did NOT have qu...        I EXPECT IT\n",
       "38  What would you say if the game had a clear end...          I LIKE IT\n",
       "39  What would you say if the game does NOT have a...       I DISLIKE IT"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40 entries, 0 to 39\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   question  40 non-null     object\n",
      " 1   answer    40 non-null     int64 \n",
      " 2   airidas   40 non-null     int64 \n",
      " 3   elias     40 non-null     int64 \n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 1.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/twenythree/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/twenythree/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration, RagTokenForGeneration\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n",
      "Some weights of the model checkpoint at facebook/rag-token-base were not used when initializing RagTokenForGeneration: ['rag.question_encoder.question_encoder.bert_model.pooler.dense.bias', 'rag.question_encoder.question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RagTokenForGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RagTokenForGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = \"facebook/rag-token-base\"\n",
    "tokenizer = RagTokenizer.from_pretrained(model_name)\n",
    "model = RagTokenForGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    # For models that return sequence outputs, you might need to aggregate them to get a single vector.\n",
    "    # For example, taking the mean of the output embeddings (across the token dimension).\n",
    "    embeddings = outputs.last_hidden_state\n",
    "    embeddings_mean = torch.mean(embeddings, dim=1).squeeze()  # Take mean across tokens\n",
    "\n",
    "    return embeddings_mean.detach().numpy()  # Convert tensor to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Make sure that `context_input_ids` are passed, if no `retriever` is set. Alternatively, you can set a retriever using the `set_retriever(...)` function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example text\u001b[39;00m\n\u001b[1;32m      2\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlbert Einstein developed the theory of relativity.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(embedding)\n",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m, in \u001b[0;36mgenerate_embedding\u001b[0;34m(text, model, tokenizer)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_embedding\u001b[39m(text, model, tokenizer):\n\u001b[1;32m      2\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 3\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# For models that return sequence outputs, you might need to aggregate them to get a single vector.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# For example, taking the mean of the output embeddings (across the token dimension).\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/rag/modeling_rag.py:1318\u001b[0m, in \u001b[0;36mRagTokenForGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_outputs, decoder_input_ids, decoder_attention_mask, past_key_values, context_input_ids, context_attention_mask, doc_scores, use_cache, output_attentions, output_hidden_states, output_retrieved, do_marginalize, reduce_loss, labels, n_docs, **kwargs)\u001b[0m\n\u001b[1;32m   1315\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m labels\n\u001b[1;32m   1316\u001b[0m     use_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1318\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrag\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoc_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoc_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_retrieved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_retrieved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_docs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_docs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1335\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1336\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/rag/modeling_rag.py:648\u001b[0m, in \u001b[0;36mRagModel.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_outputs, decoder_input_ids, decoder_attention_mask, past_key_values, doc_scores, context_input_ids, context_attention_mask, use_cache, output_attentions, output_hidden_states, output_retrieved, n_docs)\u001b[0m\n\u001b[1;32m    644\u001b[0m         doc_scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(\n\u001b[1;32m    645\u001b[0m             question_encoder_last_hidden_state\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), retrieved_doc_embeds\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    646\u001b[0m         )\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 648\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m context_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n\u001b[1;32m    649\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that `context_input_ids` are passed, if no `retriever` is set. Alternatively, you can\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    650\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m set a retriever using the `set_retriever(...)` function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    651\u001b[0m     )\n\u001b[1;32m    652\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m context_attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n\u001b[1;32m    653\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that `context_attention_mask` are passed, if no `retriever` is set. Alternatively, you\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    654\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m can set a retriever using the `set_retriever(...)` function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    655\u001b[0m     )\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m doc_scores \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n\u001b[1;32m    657\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that `doc_scores` are passed, if no `retriever` is set. Alternatively, you can set a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    658\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m retriever using the `set_retriever(...)` function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    659\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Make sure that `context_input_ids` are passed, if no `retriever` is set. Alternatively, you can set a retriever using the `set_retriever(...)` function."
     ]
    }
   ],
   "source": [
    "# Example text\n",
    "text = \"Albert Einstein developed the theory of relativity.\"\n",
    "embedding = generate_embedding(text, model, tokenizer)\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "def get_model_and_tokenizer(model_name=\"sentence-transformers/bert-base-nli-mean-tokens\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_embedding(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    # For models that return sequence outputs, you might need to aggregate them to get a single vector.\n",
    "    # For example, taking the mean of the output embeddings (across the token dimension).\n",
    "    embeddings = outputs.last_hidden_state\n",
    "    embeddings_mean = torch.mean(embeddings, dim=1).squeeze()  # Take mean across tokens\n",
    "\n",
    "    return embeddings_mean.detach().numpy()  # Convert tensor to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.01518410e-02  6.54714108e-01 -4.37146813e-01  1.72774956e-01\n",
      " -3.68136197e-01 -8.64105225e-01  6.07604980e-01 -6.26230389e-02\n",
      "  1.52332425e-01 -7.37388015e-01 -4.72566485e-02  1.02278665e-01\n",
      "  1.80437833e-01  1.35338879e+00 -4.08144772e-01 -4.66639578e-01\n",
      " -6.70039296e-01 -2.14205027e-01  5.53092778e-01  3.78459930e-01\n",
      " -4.86701429e-01  2.18615621e-01 -1.34859204e+00 -4.33657557e-01\n",
      "  5.13376176e-01 -6.14898086e-01 -4.48532969e-01 -9.60150540e-01\n",
      " -3.07825178e-01  7.88511112e-02 -4.45862383e-01  9.73700881e-01\n",
      " -6.88949347e-01 -1.12216927e-01 -9.81475830e-01  1.92295402e-01\n",
      "  2.37183526e-01  2.27337882e-01  1.46151274e-01  3.22736174e-01\n",
      "  6.77452534e-02 -2.40518183e-01  4.17782545e-01 -3.80316436e-01\n",
      " -1.66326809e+00 -8.97505581e-01 -1.12364724e-01 -1.60608485e-01\n",
      " -6.61322236e-01 -1.43750417e+00 -5.59373379e-01 -1.06027052e-01\n",
      " -8.92154202e-02  6.51685774e-01 -4.02220070e-01  3.21537703e-01\n",
      "  2.26337433e-01 -1.04054475e+00  3.30521345e-01  5.00110030e-01\n",
      " -1.89074576e-01  1.28885418e-01  2.12278455e-01  1.36877209e-01\n",
      "  3.04525942e-01  1.08101293e-02  8.71586323e-01  1.56997472e-01\n",
      " -4.05770630e-01  6.43898249e-02 -1.41043365e-01  1.85680240e-02\n",
      " -1.04143417e+00 -6.35880470e-01 -7.71964490e-02  3.24304774e-02\n",
      "  5.81824720e-01  2.84782976e-01  6.17153227e-01 -1.11369893e-01\n",
      " -5.60882092e-01  5.81789970e-01  8.84416759e-01 -2.72593558e-01\n",
      "  6.37618676e-02  3.26982349e-01  1.11074902e-01 -6.51363358e-02\n",
      " -1.01832974e+00  4.00856256e-01 -1.26206830e-01  4.78444159e-01\n",
      " -2.61541486e-01 -1.74857497e-01  4.53481257e-01 -5.67030072e-01\n",
      "  1.09705889e+00 -7.07785904e-01 -1.14464498e+00 -2.52220690e-01\n",
      "  2.15498004e-02 -6.22356057e-01 -1.81974471e-01  2.22424939e-01\n",
      "  4.70116794e-01 -5.62952876e-01  2.95062214e-01 -1.27365041e+00\n",
      "  5.67645967e-01  7.02033877e-01  4.31704432e-01  8.64254087e-02\n",
      " -4.66644056e-02  1.97586074e-01 -1.03045011e+00 -8.84594917e-01\n",
      "  1.94278613e-01  1.19586661e-01  1.35642558e-01  1.39561844e+00\n",
      " -3.56419152e-03  4.25875485e-01 -3.18651974e-01 -1.48183778e-02\n",
      "  7.22169518e-01  5.87925076e-01  6.63608432e-01  8.38887811e-01\n",
      " -1.28700781e+00  3.90816629e-01  4.22753096e-01  8.60188305e-01\n",
      "  8.41940284e-01 -7.78434455e-01 -4.76509780e-02  1.39124081e-01\n",
      "  4.35822010e-01 -9.00765061e-01 -1.25734591e+00  2.82165587e-01\n",
      "  4.04618651e-01  3.86540651e-01 -1.22575030e-01 -4.55809206e-01\n",
      " -1.46396950e-01  2.17673779e-01  7.52038285e-02  3.93782496e-01\n",
      "  2.76820958e-01  6.54292285e-01 -1.14345860e+00 -2.64940802e-02\n",
      " -3.94143254e-01 -2.56186333e-02  1.42524824e-01 -4.91694540e-01\n",
      " -3.86439145e-01  8.26380730e-01  1.56790510e-01  5.98632932e-01\n",
      "  2.00091243e-01 -6.98216707e-02 -2.38001794e-01  6.76335752e-01\n",
      "  2.40682989e-01  2.02444598e-01  1.31222844e+00 -2.46958017e-01\n",
      " -7.98401952e-01 -4.19287682e-01 -8.58045101e-01 -2.00077623e-01\n",
      "  8.46719265e-01  3.95297864e-03  5.55084348e-01  5.93413770e-01\n",
      "  1.03635252e+00  5.22944272e-01  3.53843361e-01 -2.34054163e-01\n",
      "  1.22035518e-01 -2.70964086e-01 -1.04276739e-01 -5.02153337e-02\n",
      "  5.17570019e-01 -8.74420851e-02 -2.99424469e-01 -9.26229954e-02\n",
      "  6.70439610e-03 -6.50276721e-01 -5.71777336e-02 -3.84483725e-01\n",
      " -8.59793961e-01 -1.54970407e-01  4.78698909e-01  2.85991311e-01\n",
      " -4.19960111e-01  1.55557466e+00 -4.07048315e-01 -3.16814393e-01\n",
      "  1.65843740e-01 -4.32569206e-01  3.92565280e-01 -1.77621339e-02\n",
      "  2.65851319e-01  3.54086429e-01 -3.27998966e-01 -3.49293016e-02\n",
      " -3.30327034e-01  6.55719638e-01 -6.31546155e-02  7.94434369e-01\n",
      "  2.37264201e-01 -7.11283326e-01 -9.10751522e-01 -6.46476626e-01\n",
      "  1.28533959e-01  2.55593419e-01 -7.85301387e-01  1.25750232e+00\n",
      "  2.97945976e-01  3.98806155e-01  2.38173921e-02  3.19638729e-01\n",
      " -1.11782506e-01  1.16361308e+00  4.15190876e-01  5.55280030e-01\n",
      " -1.96594447e-01  1.25216162e+00  8.00148964e-01 -6.32764697e-01\n",
      "  4.23986524e-01  3.39125931e-01 -6.00172698e-01 -1.89846218e-01\n",
      " -7.87074447e-01 -6.51703477e-01  2.54428476e-01 -2.24130183e-01\n",
      "  4.54358049e-02  3.53454053e-01 -9.42489356e-02  1.36686289e+00\n",
      " -9.42156196e-01 -1.07229911e-01 -8.76144543e-02  2.08109379e-01\n",
      " -8.18385959e-01  8.92214477e-01 -1.50018215e+00 -4.62295204e-01\n",
      "  5.00956714e-01  5.18123340e-03  3.69688094e-01 -2.95552105e-01\n",
      "  7.80996680e-02  4.35457557e-01  6.93395078e-01  3.58831644e-01\n",
      "  3.57774794e-01 -7.19130218e-01 -4.72611129e-01  3.99399549e-01\n",
      " -5.06087124e-01 -1.32829452e+00  4.38246638e-01  4.26193327e-01\n",
      " -7.99021661e-01  7.12672353e-01  2.44529292e-01  2.59324551e-01\n",
      " -3.60406578e-01  7.18160093e-01  2.97375351e-01 -3.27707827e-01\n",
      "  8.52763057e-02  6.27426386e-01 -1.01835227e+00  7.48071074e-01\n",
      "  5.88320613e-01  2.26437375e-01 -1.40548992e+00  7.78743401e-02\n",
      "  7.32438445e-01 -5.63806415e-01 -4.11680400e-01 -7.27673411e-01\n",
      " -3.66341546e-02  4.18421179e-01 -1.17117190e+00 -3.53896677e-01\n",
      " -9.10951316e-01 -6.61195874e-01 -1.72174260e-01  2.55614579e-01\n",
      "  1.25397241e+00 -7.96688616e-01  4.36551750e-01 -8.07423294e-01\n",
      " -2.56231666e-01  3.59428763e-01 -2.24244714e-01  3.36191952e-01\n",
      "  2.47378558e-01 -2.80381769e-01  7.99773335e-01  9.58754867e-02\n",
      "  6.98191166e-01  1.40816569e-01 -6.32551193e-01 -9.67895150e-01\n",
      "  7.11490631e-01  1.44525599e+00 -7.16813207e-02  1.58206850e-01\n",
      " -1.27611804e+00 -5.35760581e-01  3.56702268e-01 -1.22821295e+00\n",
      " -9.85268295e-01  6.76134944e-01 -2.83797354e-01  3.99095893e-01\n",
      "  3.91119868e-01  2.31609493e-01 -2.77092904e-01  9.87329125e-01\n",
      " -3.17567140e-01  1.84287392e-02  8.07205558e-01 -3.09008300e-01\n",
      "  2.56649435e-01 -1.48259312e-01 -2.12089568e-01  3.65708232e-01\n",
      "  8.74317735e-02 -4.99328911e-01  7.95057535e-01  1.65635899e-01\n",
      "  9.62001026e-01 -1.23128855e+00  4.93798256e-02  1.00159597e+00\n",
      "  2.43673269e-02  9.35329497e-03  8.84900689e-01  1.94663435e-01\n",
      " -2.50441849e-01 -2.85286605e-01  4.60232273e-02 -6.28333211e-01\n",
      "  7.53291905e-01  4.61654216e-02  4.51940559e-02  1.13124156e+00\n",
      " -3.00893307e-01  6.67571416e-03 -1.34320315e-02  1.01428509e+00\n",
      " -4.84232813e-01 -1.46877193e+00 -7.05495715e-01  4.40134227e-01\n",
      " -3.19105417e-01  9.21641737e-02 -4.90900815e-01 -7.19598413e-01\n",
      "  1.05296135e+00  2.39072949e-01 -7.16060027e-02  2.05712587e-01\n",
      " -4.02964413e-01 -7.67122030e-01 -4.15202707e-01 -5.08406833e-02\n",
      "  3.28592122e-01 -5.56692593e-02 -2.56342262e-01  9.48227763e-01\n",
      "  2.82942057e-01 -6.98125958e-01 -9.13227871e-02 -2.90775836e-01\n",
      " -3.69714767e-01 -2.18886882e-01  5.47203124e-01  2.68332064e-01\n",
      " -6.04838014e-01 -6.98065937e-01 -1.65904805e-01  5.07840633e-01\n",
      "  1.06047297e+00 -1.05202958e-01 -4.34015095e-01 -1.02592385e+00\n",
      " -3.54697734e-01 -5.04358292e-01 -1.44412398e-01  8.07813466e-01\n",
      "  3.88794154e-01  3.37274194e-01 -7.48724341e-01 -9.06694710e-01\n",
      " -2.39914611e-01  1.35576558e+00  5.92050433e-01 -9.97603871e-03\n",
      "  5.73496509e-04 -5.99621296e-01  6.69288337e-01 -1.18236279e+00\n",
      "  1.78596705e-01 -1.98215574e-01 -8.55272174e-01  4.04699624e-01\n",
      " -3.35555047e-01  2.18585402e-01 -8.96867454e-01  6.76252007e-01\n",
      "  3.77519935e-01 -7.73031473e-01  1.04673147e+00  6.02325387e-02\n",
      " -3.03658664e-01  1.59989476e+00 -5.67249596e-01 -3.39626104e-01\n",
      " -1.48113117e-01  3.29119116e-01 -4.27388579e-01 -7.85930455e-01\n",
      " -1.12641931e-01  5.23221970e-01 -9.15441036e-01  2.98598140e-01\n",
      "  6.21204317e-01  6.38414741e-01 -1.32906437e-03  1.90128177e-01\n",
      "  3.03363174e-01  9.71999764e-02 -8.25923979e-02 -8.76883715e-02\n",
      " -1.17899060e+00  1.84394628e-01 -9.25748125e-02  3.97819817e-01\n",
      "  1.23890781e+00 -5.39206266e-01  7.15468109e-01 -5.22361159e-01\n",
      "  1.04013458e-01 -1.85114145e-01  8.44632804e-01 -2.89486170e-01\n",
      " -8.67880344e-01 -4.67874616e-01  5.87367415e-01 -3.60076010e-01\n",
      " -4.24148947e-01 -5.47998130e-01 -8.33858609e-01 -3.78231294e-02\n",
      "  5.42719424e-01  9.88852233e-02 -3.92683446e-01 -2.40090698e-01\n",
      " -1.16381502e+00  8.04861248e-01 -2.63543785e-01 -2.16495425e-01\n",
      "  1.74188435e-01  2.07275271e-01 -1.73760995e-01 -1.19795978e+00\n",
      "  6.50470018e-01  3.49840134e-01 -4.77045238e-01 -3.80121946e-01\n",
      "  5.97898364e-01 -3.37490350e-01 -4.05182928e-01  1.54870808e+00\n",
      " -6.26609325e-02 -6.79412067e-01 -5.12886122e-02 -5.54928958e-01\n",
      "  6.69327796e-01  1.01151373e-02 -1.50504136e+00 -8.08514595e-01\n",
      " -1.37991261e+00 -1.48308742e+00  5.56776166e-01 -2.74533987e-01\n",
      " -2.59268194e-01 -1.37702882e+00  5.32088995e-01  6.39033839e-02\n",
      " -5.06159902e-01  5.62825978e-01 -7.41768539e-01 -1.30746201e-01\n",
      " -2.05742866e-01  7.05692321e-02  9.75583076e-01  1.21637151e-01\n",
      " -5.56084692e-01  9.41425040e-02 -3.44711095e-01  4.98760551e-01\n",
      " -3.11697304e-01 -6.03903651e-01 -1.26452255e+00  6.66216984e-02\n",
      "  4.96321112e-01  1.04807687e+00 -4.29940283e-01 -7.28985906e-01\n",
      "  5.50010324e-01 -3.72521251e-01  7.23403811e-01 -1.50825232e-01\n",
      "  2.00961307e-01 -4.79841143e-01  3.64390075e-01  3.88905019e-01\n",
      "  4.09328759e-01  1.47017166e-01 -1.28617972e-01  2.71403491e-02\n",
      " -2.14373156e-01 -4.81447607e-01  6.96811795e-01  7.75206864e-01\n",
      " -2.57287651e-01 -8.75369668e-01 -3.51533592e-01  2.49405831e-01\n",
      " -1.55913383e-01  8.70050251e-01  3.28716695e-01 -4.48768318e-01\n",
      " -4.36751068e-01 -3.14004719e-01 -1.20380963e-03 -1.10290885e+00\n",
      "  3.20585012e-01 -8.89851272e-01 -9.82729137e-01 -2.05552846e-01\n",
      " -2.14587182e-01 -8.43107179e-02  8.29289734e-01  3.84700149e-01\n",
      "  5.26013672e-02  9.26460743e-01 -3.51755559e-01  1.68323278e-01\n",
      " -7.28626609e-01 -4.28774744e-01 -6.05166078e-01  2.52795279e-01\n",
      " -5.10344207e-01  1.87353760e-01 -6.37938857e-01 -9.36625779e-01\n",
      "  3.95773947e-01  3.76135051e-01  6.17103696e-01 -2.51747757e-01\n",
      " -9.78579931e-03  1.05984819e+00 -6.90917552e-01 -3.28772843e-01\n",
      "  1.15531683e+00 -8.22524667e-01 -2.71024644e-01 -3.93381059e-01\n",
      " -2.44017512e-01 -3.82233341e-03 -1.95286661e-01  3.44684958e-01\n",
      " -6.31147981e-01 -2.16735914e-01 -2.92640865e-01 -1.08676839e+00\n",
      "  2.57223547e-01  1.38621902e+00  5.71057737e-01 -7.07733452e-01\n",
      "  4.67950016e-01  3.18540454e-01 -6.00983143e-01  4.86209214e-01\n",
      " -3.33046377e-01  6.21733844e-01 -4.06347930e-01 -1.70994714e-01\n",
      "  1.45810485e+00 -2.62698084e-01 -7.56223798e-01  6.18925750e-01\n",
      " -4.26282227e-01  1.02748573e+00 -3.04856896e-01 -2.75388151e-01\n",
      " -1.94143534e-01  4.39865828e-01  5.58777824e-02 -7.64751554e-01\n",
      "  3.01970720e-01 -2.49802738e-01  1.89155266e-01 -1.70724690e-01\n",
      " -2.17687458e-01  7.93619633e-01  1.00591266e+00 -2.20386714e-01\n",
      " -6.55192852e-01  6.34287298e-01 -1.28678000e+00 -4.10353392e-03\n",
      "  6.35380507e-01 -2.67587900e-01 -2.09176540e-03  5.01830697e-01\n",
      " -1.39245975e+00  2.43349075e-01  5.01249969e-01 -3.16341370e-01\n",
      " -1.83319181e-01 -1.23388711e-02  6.09920204e-01  6.33070350e-01\n",
      " -1.30033374e+00 -1.14330634e-01  6.10326409e-01 -3.26874346e-01\n",
      " -4.87521350e-01  2.40235180e-01  1.70267653e-02 -2.69015402e-01\n",
      "  7.85239995e-01  6.39641881e-01 -4.57534380e-02  4.12856251e-01\n",
      "  2.91089892e-01  9.27923098e-02 -4.52965587e-01  1.89171568e-01\n",
      "  3.41953218e-01 -1.15127049e-01  3.53722483e-01  1.29416180e+00\n",
      "  7.62360096e-01  4.03815284e-02 -8.13035518e-02  5.36604047e-01\n",
      "  1.04838991e+00 -8.12114120e-01  4.80675161e-01  4.73839417e-02\n",
      "  1.48138916e+00  8.67077351e-01  6.33206218e-02  3.18016738e-01\n",
      " -4.43220466e-01 -2.74095386e-01  5.43647766e-01 -9.01283443e-01\n",
      "  1.97259009e+00 -2.27528721e-01  5.59149742e-01 -5.72026849e-01\n",
      " -2.07026556e-01 -8.78869295e-02 -2.66510487e-01 -1.31046140e+00\n",
      " -1.59516454e-01  3.04291040e-01 -4.24675703e-01  2.66637415e-01\n",
      " -1.40383616e-01 -2.08652213e-01  5.08856654e-01 -2.05622628e-01\n",
      "  4.62381780e-01 -1.96669087e-01  1.16590753e-01  1.26067385e-01\n",
      " -1.32195067e+00 -9.91378427e-01 -3.52918774e-01  1.03927207e+00\n",
      " -1.42282337e-01 -5.94307303e-01  1.80381507e-01 -5.42300820e-01\n",
      " -7.40790486e-01 -2.12754793e-02 -2.95625836e-01 -1.31635010e+00\n",
      "  6.96730494e-01 -4.05742407e-01 -1.26156896e-01 -8.54437947e-01\n",
      " -5.85132420e-01  6.27283603e-02  4.16076571e-01  4.67398345e-01\n",
      "  5.09373128e-01  6.34048462e-01  1.96628980e-02  1.20537901e+00\n",
      "  1.04854427e-01  4.85935450e-01 -9.92169201e-01 -7.33368516e-01\n",
      " -4.07261461e-01  6.68990314e-01 -4.38021839e-01 -6.01170072e-03\n",
      " -7.76857257e-01  2.22876191e-01  3.44918936e-01  4.52956051e-01\n",
      " -8.13986778e-01 -1.88952118e-01 -8.58276665e-01 -1.36724189e-01\n",
      " -4.73815620e-01  2.92741597e-01  6.16259933e-01  5.81363663e-02\n",
      "  1.75935745e-01 -1.18538894e-01 -1.17753577e+00 -1.00509584e+00\n",
      " -9.81946468e-01 -4.03034836e-01 -2.23877475e-01 -4.83638614e-01\n",
      " -3.30606163e-01  3.72476131e-01 -8.70090842e-01 -4.78194095e-02\n",
      " -4.10057306e-01  8.42046916e-01  3.64740461e-01  4.72613156e-01\n",
      "  1.65704441e+00  3.03966105e-01 -2.40425587e-01  4.87850904e-01\n",
      "  3.27963144e-01 -9.27893966e-02 -9.25967619e-02 -4.35064137e-01\n",
      " -1.23198710e-01  5.87357655e-02  3.03814054e-01 -9.72312242e-02\n",
      "  3.21008623e-01 -6.54154550e-03 -3.59073691e-02 -3.96915555e-01]\n"
     ]
    }
   ],
   "source": [
    "# Usage Example\n",
    "model_name = \"sentence-transformers/bert-base-nli-mean-tokens\"\n",
    "model, tokenizer = get_model_and_tokenizer(model_name)\n",
    "\n",
    "# Example text\n",
    "text = \"Albert Einstein developed the theory of relativity.\"\n",
    "embedding = generate_embedding(text, model, tokenizer)\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session Saved Things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
