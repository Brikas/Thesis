{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial setup: **INPUT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lib import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import brikasutils as bu\n",
    "importlib.reload(bu)\n",
    "import shared_utils as utils\n",
    "from shared_utils import systemMsg, userMsg, assistantMsg\n",
    "importlib.reload(utils)\n",
    "import survey\n",
    "importlib.reload(survey)\n",
    "import persona\n",
    "importlib.reload(persona)\n",
    "\n",
    "import ollama\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "from typing import List\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 1916 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-09-13 to 2024-03-06\n",
      "Messages saved to self.chats['airidas']\n",
      "Read 618 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-09-10 to 2024-03-03\n",
      "Messages saved to self.chats['christian']\n",
      "Read 297 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2018-07-25 to 2024-01-01\n",
      "Messages saved to self.chats['nikolay']\n",
      "Read 144 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-03-28 to 2021-12-30\n",
      "Messages saved to self.chats['mathis']\n",
      "Read 104 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-08-25 to 2024-03-05\n",
      "Messages saved to self.chats['jacob']\n",
      "Read 159 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-10-12 to 2023-04-30\n",
      "Messages saved to self.chats['chris']\n",
      "Read 161 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-03-28 to 2021-06-06\n",
      "Messages saved to self.chats['aziz']\n",
      "Read 350 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-03-26 to 2022-04-11\n",
      "Messages saved to self.chats['daniela']\n",
      "Read 105 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-03-28 to 2022-07-10\n",
      "Messages saved to self.chats['mihi']\n",
      "Read 117 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-05-17 to 2022-04-11\n",
      "Messages saved to self.chats['viktoria']\n",
      "Read 172 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2019-08-11 to 2023-12-21\n",
      "Messages saved to self.chats['diba']\n",
      "Read 154 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-12-02 to 2023-04-29\n",
      "Messages saved to self.chats['filip']\n",
      "Read 659 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2023-03-12 to 2024-02-16\n",
      "Messages saved to self.chats['rebecca']\n",
      "Filtering\n",
      "link-filter: 133\n",
      "react-filter: 0\n",
      "cookie-data-filter: 12\n",
      "Selected chat rebecca for 16706 (634 messages)\n",
      "Selected chat airidas for 41064 (1671 messages)\n",
      "Selected chat christian for 10435 (576 messages)\n",
      "Selected chat nikolay for 5522 (266 messages)\n",
      "Selected chat mathis for 2531 (134 messages)\n",
      "Selected chat daniela for 5518 (293 messages)\n",
      "Selected chat diba for 3203 (156 messages)\n",
      "Selected chat aziz for 2444 (136 messages)\n",
      "Selected chat jacob for 1961 (93 messages)\n",
      "Selected chat chris for 2893 (151 messages)\n",
      "Selected chat filip for 2619 (146 messages)\n",
      "Selected chat mihi for 2016 (102 messages)\n",
      "Selected chat viktoria for 2131 (107 messages)\n",
      "Combined tokens: 99043\n"
     ]
    }
   ],
   "source": [
    "et = persona.PersonaEncoder()\n",
    "\n",
    "# ==== FB messages ====\n",
    "et.parse_fb_messages([\"data/1_raw/1_airidas.json\"], \"airidas\")\n",
    "et.parse_fb_messages([\"data/1_raw/2_christian.json\"], \"christian\")\n",
    "et.parse_fb_messages([\"data/1_raw/1_nikolay.json\"], \"nikolay\")\n",
    "et.parse_fb_messages([\"data/1_raw/2_mathis.json\"], \"mathis\")\n",
    "et.parse_fb_messages([\"data/1_raw/2_jacob.json\"], \"jacob\")\n",
    "et.parse_fb_messages([\"data/1_raw/2_chris.json\"], \"chris\")\n",
    "et.parse_fb_messages([\"data/1_raw/3_aziz.json\"], \"aziz\")\n",
    "et.parse_fb_messages([\"data/1_raw/3_daniela.json\"], \"daniela\")\n",
    "et.parse_fb_messages([\"data/1_raw/3_mihi.json\"], \"mihi\")\n",
    "et.parse_fb_messages([\"data/1_raw/3_viktoria.json\"], \"viktoria\")\n",
    "et.parse_fb_messages([\"data/1_raw/4_diba.json\"], \"diba\")\n",
    "et.parse_fb_messages([\"data/1_raw/6_filip.json\"], \"filip\")\n",
    "et.parse_wa_messages([\"data/1_raw/messages_1000.json\"], \"rebecca\")\n",
    "# for name, texts in texts_with_others_dict.items():\n",
    "#     et.parse_fb_messages(texts, name)\n",
    "\n",
    "# Regex cleaning\n",
    "et.filter_chats_empty()\n",
    "et.filter_chats_regex(utils.BLACKLIST_CHAT_REGEX_FILTERS)\n",
    "\n",
    "# Compress names\n",
    "for nameid, chat in et.chats.items():\n",
    "    for msg in chat:  \n",
    "        msg.sender = \"Persona\" if msg.sender == \"Elias Salvador Smidt Torjani\"  else \"Friend\"\n",
    "\n",
    "# Start all chats from 2/3rds\n",
    "# for name, chat in et.chats.items():\n",
    "#     et.chats[name] = chat[int(len(chat)/3 * 2):]\n",
    "# Select the final modules\n",
    "# et.select_chat_limited_by_tokens(\"airidas\", 10000)\n",
    "et.select_chat_full(\"rebecca\")\n",
    "et.select_chat_full(\"airidas\")\n",
    "et.select_chat_full(\"christian\")\n",
    "et.select_chat_full(\"nikolay\")\n",
    "et.select_chat_full(\"mathis\") \n",
    "et.select_chat_full(\"daniela\")\n",
    "et.select_chat_full(\"diba\")\n",
    "et.select_chat_full(\"aziz\")\n",
    "et.select_chat_full(\"jacob\")  \n",
    "et.select_chat_full(\"chris\")\n",
    "et.select_chat_full(\"filip\")\n",
    "et.select_chat_full(\"mihi\")\n",
    "et.select_chat_full(\"viktoria\")\n",
    "\n",
    "# save\n",
    "BIG_MODULE=et.output()\n",
    "bu.quickTXT(BIG_MODULE, filename=f\"data/2_modules/big_{bu.get_timestamp()}\")\n",
    "\n",
    "# stats\n",
    "token_counts = et.count_all_selected_chat_tokens() # token_counts used later for statistics\n",
    "print(f\"Combined tokens: {sum(token_counts.values())}\")\n",
    "# utils.count_tokens(BIG_MODULE) \n",
    "# or list(et.selectedChats.keys()) --> et.count_chat_tokens(\"{friend}\")\n",
    "# et.selectedChats[\"{friend}\"][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding **Chunking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "EMBED_MODEL = \"nomic-embed-text\"        # nomic-embed-text = long ctx / mxbai-embed-large = big\n",
    "CHUNK_SIZE = 40                         # N of msgs per chunk: 10-90?\n",
    "OVERLAP_SIZE = 10                       # N of overlapping msgs between consecutive chunks: 5-50?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists for storing chunks â€“ and embeddings later\n",
    "chunks = []\n",
    "stat_total_msgs_in_chunks = 0 # for statistics\n",
    "\n",
    "# different chunk size\n",
    "# Iterate over chats and messages to create chunks\n",
    "for chat in et.selectedChats.values():\n",
    "    messages = list(chat)  # Convert chat iterator to list for easier slicing\n",
    "    num_messages = len(messages)\n",
    "\n",
    "    # Create overlapping chunks of messages\n",
    "    for i in range(0, num_messages - CHUNK_SIZE + 1, CHUNK_SIZE - OVERLAP_SIZE):\n",
    "        chunk = messages[i:i + CHUNK_SIZE]  # Extract chunk of messages\n",
    "        chunk_text = \"\\n\".join(str(msg) for msg in chunk)  # Concatenate messages into a single string\n",
    "        chunks.append(chunk_text)  # Append chunk to list of chunks\n",
    "\n",
    "        stat_total_msgs_in_chunks += len(chunk) # For statistics\n",
    "\n",
    "##### Display Info\n",
    "total_messages = sum(len(chat) for chat in et.selectedChats.values())\n",
    "chunks_count = len(chunks)\n",
    "avg_chunk_char_len = np.mean([len(chunk) for chunk in chunks])\n",
    "\n",
    "print(\n",
    "    f\"Chunk count: {chunks_count}\",\n",
    "    # f\"Average chunk character length: {round( avg_chunk_char_len)}\",\n",
    "    f\"Rough estimate of tokens per chunk: {round(avg_chunk_char_len / 4)} (4 characters per token)\",\n",
    "    f\"Messagees in input count: {total_messages}\",\n",
    "    f\"Messages in chunks count: {stat_total_msgs_in_chunks}\",\n",
    "    f\"Chunk \\ Input ratio: {round(stat_total_msgs_in_chunks / total_messages,2)} (OVERLAP_SIZE={OVERLAP_SIZE})\",\n",
    "    f\"Chunk Python type: {type(chunks[0])}\",\n",
    "    sep=\"\\n\"\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Generaterating** embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Serialization ###########\n",
    "EMBEDDING_ID = f\"{CHUNK_SIZE}-{OVERLAP_SIZE}\"\n",
    "AUTO_INFO = {\n",
    "    \"model\": EMBED_MODEL,\n",
    "    \"CHUNK_SIZE\": CHUNK_SIZE,\n",
    "    \"OVERLAP_SIZE\": OVERLAP_SIZE,\n",
    "    \"chunks_count\": chunks_count,\n",
    "    \"total_messages\": total_messages,\n",
    "    \"stat_total_msgs_in_chunks\": stat_total_msgs_in_chunks,\n",
    "    \"modules_chat\": token_counts,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for each chunk\n",
    "embeddings = []\n",
    "progress, chunks_len = 0, len(chunks) # for progress bar\n",
    "for chunk_text in chunks:\n",
    "    progress += 1\n",
    "    print(f\"\\rChunk {progress}/{chunks_len}\", end=\"\")\n",
    "    embedding = ollama.embeddings(model=EMBED_MODEL, prompt=chunk_text)[\"embedding\"]\n",
    "    embeddings.append(embedding)\n",
    "####################################################\n",
    "# token counts in all similar chunks\n",
    "# tokens_in_chunks = 0\n",
    "# for chunk in chunks_most_similar:\n",
    "#     tokens_in_chunks += utils.count_tokens(chunk)\n",
    "# print(f\"Tokens in chunks: {tokens_in_chunks}\")\n",
    "####################################################\n",
    "bu.if_dir_not_exist_make(\"data/3_embeddings\")\n",
    "bu.quickJSON(AUTO_INFO, f\"data/3_embeddings/{EMBEDDING_ID}_info.json\")\n",
    "bu.quickJSON({\"chunks\": chunks, \"embeddings\": embeddings}, f\"data/3_embeddings/{EMBEDDING_ID}_embeddings.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CTX**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# surv = survey.KanoSurvey()\n",
    "surv = survey.PersonalitySurvey()\n",
    "#The Five Factors of personality are:\n",
    "# Openness - How open a person is to new ideas and experiences\n",
    "# Conscientiousness - How goal-directed, persistent, and organized a person is\n",
    "# Extraversion - How much a person is energized by the outside world\n",
    "# Agreeableness - How much a person puts others' interests and needs ahead of their own\n",
    "# Neuroticism - How sensitive a person is to stress and negative emotional triggers\n",
    "\n",
    "# surv = survey.buildFairnessPrompts()\n",
    "# surv = survey.DictatorGameSurvey()\n",
    "surv.questions[:2]#.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(surv, survey.KanoSurvey):\n",
    "    RETRIEVAL_PROMPT = \"video game features\"\n",
    "    DYNAMIC_RETRIEVAL_PROMPTS = list(surv.questions)\n",
    "    SURVEY_TYPE = \"KanoSurvey\",\n",
    "    SURVEY = \"video game preferences\"\n",
    "    METHOD = \"Kano survey\"\n",
    "    WHICH_SURVEY = \"kano\"\n",
    "    PROMPT_LENGTH = 40\n",
    "elif isinstance(surv, survey.PersonalitySurvey):\n",
    "    RETRIEVAL_PROMPT = \"openess conciousness extrovert aggreableness neuroticism\"\n",
    "    DYNAMIC_RETRIEVAL_PROMPTS = list(surv.questions)\n",
    "    SURVEY_TYPE = \"PersonalitySurvey\",\n",
    "    SURVEY = \"personality traits\"\n",
    "    METHOD = \"OCEAN test\"\n",
    "    WHICH_SURVEY = \"pers\"\n",
    "    PROMPT_LENGTH = 50\n",
    "\n",
    "CHUNKS_COUNT_IN_CTX = 10 # Number of nearby chunks to put in context window\n",
    "\n",
    "########### Serialization ###########\n",
    "# EMBEDDING_ID = f\"{CHUNK_SIZE}-{OVERLAP_SIZE}\"\n",
    "VERSION_ID = f\"8k_{WHICH_SURVEY}\" # pers/kano_{ctx tokens}k\n",
    "CHECKPOINT = f\"{EMBEDDING_ID}-{CHUNKS_COUNT_IN_CTX}-{VERSION_ID}\"\n",
    "AUTO_INFO = {\n",
    "    \"CHUNKS_COUNT_IN_CTX\": CHUNKS_COUNT_IN_CTX,\n",
    "    \"EMBEDDING_ID\": EMBEDDING_ID,\n",
    "    \"VERSION_ID\": VERSION_ID,\n",
    "    \"model\": EMBED_MODEL,\n",
    "    \"CHUNK_SIZE\": CHUNK_SIZE,\n",
    "    \"OVERLAP_SIZE\": OVERLAP_SIZE,\n",
    "    \"chunks_count\": chunks_count,\n",
    "    \"total_messages\": total_messages,\n",
    "    \"stat_total_msgs_in_chunks\": stat_total_msgs_in_chunks,\n",
    "    \"modules_chat\": token_counts,\n",
    "    \"SURVEY and method\": f\"{SURVEY} and {METHOD}\",\n",
    "    \"RETRIEVAL_PROMPT\": RETRIEVAL_PROMPT,\n",
    "    \"DYNAMIC_RETRIEVAL_PROMPTS\": DYNAMIC_RETRIEVAL_PROMPTS,\n",
    "}\n",
    "########### Serialization ###########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 50/50\n",
      "Tokens in chunks: 140550\n",
      "Chunks:138, embeds:138\n",
      "Tokens in average chunk group: 2811.0\n"
     ]
    }
   ],
   "source": [
    "HYBRID_CTX = int(CHUNKS_COUNT_IN_CTX/2)\n",
    "prompt_embedding = ollama.embeddings(model=EMBED_MODEL, prompt=RETRIEVAL_PROMPT)[\"embedding\"]\n",
    "chunks_most_similar_embeddings  = utils.find_most_similar(prompt_embedding, embeddings)[:HYBRID_CTX]\n",
    "\n",
    "# Static part\n",
    "chunks_most_similar = []\n",
    "for embedding in chunks_most_similar_embeddings:\n",
    "    chunks_most_similar.append(chunks[embedding[1]])\n",
    "\n",
    "# Dynamic part\n",
    "DYNAMIC_RETRIEVAL_PROMPTS\n",
    "dynamic_chunks_most_similar: List[List[str]] = [] \n",
    "progress = 0\n",
    "lenn = len(DYNAMIC_RETRIEVAL_PROMPTS)\n",
    "for prompt in DYNAMIC_RETRIEVAL_PROMPTS:\n",
    "    progress += 1\n",
    "    print(f\"\\rPrompt {progress}/{lenn}\", end=\"\")\n",
    "    chunks_most_similar = []\n",
    "    for embedding in chunks_most_similar_embeddings:\n",
    "        chunks_most_similar.append(chunks[embedding[1]])\n",
    "    dynamic_chunks_most_similar.append(chunks_most_similar)\n",
    "print(end=\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### BOTH VANITY PRINT CHECKS SHOULD BE LIKE THIS AFTER THEIR PARTS ###\n",
    "# token counts in all similar chunks\n",
    "tokens_in_chunks = 0\n",
    "for chunks_most_similar in dynamic_chunks_most_similar:\n",
    "    for chunk in chunks_most_similar:\n",
    "        tokens_in_chunks += utils.count_tokens(chunk)\n",
    "print(f\"Tokens in chunks: {tokens_in_chunks}\")\n",
    "print(f\"Chunks:{len(chunks)}, embeds:{len(embeddings)}\")\n",
    "\n",
    "\n",
    "print(f\"Tokens in average chunk group: {tokens_in_chunks/len(dynamic_chunks_most_similar)}\")\n",
    "bu.quickJSON(dynamic_chunks_most_similar, filename=f\"data/4_chunks/{CHECKPOINT}-hybrid_chunks.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Static part\n",
    "prompt_embedding = ollama.embeddings(model=EMBED_MODEL, prompt=RETRIEVAL_PROMPT)[\"embedding\"]\n",
    "chunks_most_similar_embeddings = utils.find_most_similar(prompt_embedding, embeddings)[:HYBRID_CTX]\n",
    "\n",
    "# Static part chunks\n",
    "static_chunks = []\n",
    "for embedding in chunks_most_similar_embeddings:\n",
    "    static_chunks.append(chunks[embedding[1]])\n",
    "\n",
    "# Dynamic part\n",
    "DYNAMIC_RETRIEVAL_PROMPTS = list(surv.questions)\n",
    "hybrid_chunks = []\n",
    "\n",
    "# Add static chunks with markdown separator\n",
    "static_chunk_string = \"### Related to entire survey\\n\\n\" + \"\\n\\n\".join(static_chunks)\n",
    "\n",
    "for prompt in DYNAMIC_RETRIEVAL_PROMPTS:\n",
    "    prompt_embedding = ollama.embeddings(model=EMBED_MODEL, prompt=prompt)[\"embedding\"]\n",
    "    chunks_most_similar_embeddings = utils.find_most_similar(prompt_embedding, embeddings)[:HYBRID_CTX]\n",
    "\n",
    "    dynamic_chunks = []\n",
    "    for embedding in chunks_most_similar_embeddings:\n",
    "        dynamic_chunks.append(chunks[embedding[1]])\n",
    "\n",
    "    # Combine static and dynamic chunks with markdown separators\n",
    "    chunk_string = [static_chunk_string,\n",
    "                    \"### Related to the specific question you will be asked\\n\\n\" + \"\\n\\n\".join(dynamic_chunks)]\n",
    "\n",
    "    hybrid_chunks.append(chunk_string)\n",
    "\n",
    "with open(f\"data/4_chunks/{CHECKPOINT}-hybrid_chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(hybrid_chunks, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Static part\n",
    "prompt_embedding = ollama.embeddings(model=EMBED_MODEL, prompt=RETRIEVAL_PROMPT)[\"embedding\"]\n",
    "chunks_most_similar_embeddings = utils.find_most_similar(prompt_embedding, embeddings)[:HYBRID_CTX]\n",
    "\n",
    "# Static part chunks\n",
    "static_chunks = []\n",
    "for embedding in chunks_most_similar_embeddings:\n",
    "    static_chunks.append(chunks[embedding[1]])\n",
    "\n",
    "# Dynamic part\n",
    "DYNAMIC_RETRIEVAL_PROMPTS = list(surv.questions)\n",
    "hybrid_chunks = []\n",
    "\n",
    "# Add static chunks with markdown separator\n",
    "static_chunk_string = \"### Related to entire survey\\n\\n\" + \"\\n\\n\".join(static_chunks)\n",
    "\n",
    "for prompt in DYNAMIC_RETRIEVAL_PROMPTS:\n",
    "    prompt_embedding = ollama.embeddings(model=EMBED_MODEL, prompt=prompt)[\"embedding\"]\n",
    "    chunks_most_similar_embeddings = utils.find_most_similar(prompt_embedding, embeddings)[:HYBRID_CTX]\n",
    "\n",
    "    dynamic_chunks = []\n",
    "    for embedding in chunks_most_similar_embeddings:\n",
    "        dynamic_chunks.append(chunks[embedding[1]])\n",
    "\n",
    "    # Combine static and dynamic chunks with markdown separators and newline delimiter\n",
    "    chunk_string = static_chunk_string + \"\\n\\n\\n\" + \\\n",
    "                   \"### Related to the specific question you will be asked\\n\\n\" + \"\\n\\n\".join(dynamic_chunks)\n",
    "\n",
    "    hybrid_chunks.append(chunk_string)\n",
    "\n",
    "with open(f\"data/4_chunks/{CHECKPOINT}-hybrid_chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(hybrid_chunks, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Static part\n",
    "prompt_embedding = ollama.embeddings(model=EMBED_MODEL, prompt=RETRIEVAL_PROMPT)[\"embedding\"]\n",
    "chunks_most_similar_embeddings = utils.find_most_similar(prompt_embedding, embeddings)[:HYBRID_CTX]\n",
    "\n",
    "# Static part chunks\n",
    "static_chunks = []\n",
    "for embedding in chunks_most_similar_embeddings:\n",
    "    static_chunks.append(chunks[embedding[1]])\n",
    "\n",
    "# Dynamic part\n",
    "DYNAMIC_RETRIEVAL_PROMPTS = list(surv.questions)\n",
    "hybrid_data = {\"chunks\": [], \"embeddings\": []}\n",
    "\n",
    "# Add static chunks and embeddings\n",
    "hybrid_data[\"chunks\"].extend(static_chunks)\n",
    "hybrid_data[\"embeddings\"].extend(chunks_most_similar_embeddings)\n",
    "\n",
    "for prompt in DYNAMIC_RETRIEVAL_PROMPTS:\n",
    "    prompt_embedding = ollama.embeddings(model=EMBED_MODEL, prompt=prompt)[\"embedding\"]\n",
    "    chunks_most_similar_embeddings = utils.find_most_similar(prompt_embedding, embeddings)[:HYBRID_CTX]\n",
    "\n",
    "    dynamic_chunks = []\n",
    "    for embedding in chunks_most_similar_embeddings:\n",
    "        dynamic_chunks.append(chunks[embedding[1]])\n",
    "\n",
    "    # Append dynamic chunks and embeddings\n",
    "    hybrid_data[\"chunks\"].extend(dynamic_chunks)\n",
    "    hybrid_data[\"embeddings\"].extend(chunks_most_similar_embeddings)\n",
    "\n",
    "# Save to a JSON file\n",
    "with open(f\"data/4_chunks/{CHECKPOINT}-hybrid_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(hybrid_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_embedding = ollama.embeddings(model=EMBED_MODEL, prompt=RETRIEVAL_PROMPT)[\"embedding\"]\n",
    "chunks_most_similar_embeddings  = utils.find_most_similar(prompt_embedding, embeddings)[:CHUNKS_COUNT_IN_CTX]\n",
    "chunks_most_similar = []\n",
    "for embedding in chunks_most_similar_embeddings:\n",
    "    chunks_most_similar.append(chunks[embedding[1]])\n",
    "\n",
    "# token counts in all similar chunks\n",
    "tokens_in_chunks = 0\n",
    "for chunk in chunks_most_similar:\n",
    "    tokens_in_chunks += utils.count_tokens(chunk)\n",
    "print(f\"Tokens in chunks: {tokens_in_chunks}\")\n",
    "print(f\"Chunks:{len(chunks)}, embeds:{len(embeddings)}\")\n",
    "\n",
    "# Display results\n",
    "bu.quickTXT(\"\\n\\n\".join(chunks_most_similar), filename=f\"data/4_chunks/{CHECKPOINT}-static_chunks.txt\")\n",
    "bu.if_dir_not_exist_make(\"data/4_chunks\")\n",
    "bu.quickJSON(AUTO_INFO, f\"data/4_chunks/{CHECKPOINT}-static_info.json\")\n",
    "bu.quickJSON({\"chunks\": chunks, \"embeddings\": embeddings}, f\"data/4_chunks/{CHECKPOINT}-static_embeddings.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_retrieval_prompts = list(surv.questions)\n",
    "dynamic_chunks_most_similar: List[List[str]] = [] \n",
    "\n",
    "progress = 0\n",
    "lenn = len(dynamic_retrieval_prompts)\n",
    "for prompt in dynamic_retrieval_prompts:\n",
    "    progress += 1\n",
    "    print(f\"\\rPrompt {progress}/{lenn}\", end=\"\")\n",
    "\n",
    "    prompt_embedding = ollama.embeddings(model=EMBED_MODEL, prompt=prompt)[\"embedding\"]\n",
    "    chunks_most_similar_embeddings = utils.find_most_similar(prompt_embedding, embeddings)[:CHUNKS_COUNT_IN_CTX]\n",
    "    chunks_most_similar = []\n",
    "    for embedding in chunks_most_similar_embeddings:\n",
    "        chunks_most_similar.append(chunks[embedding[1]])\n",
    "\n",
    "    dynamic_chunks_most_similar.append(chunks_most_similar)\n",
    "print(end=\"\\n\")\n",
    "\n",
    "# Display results\n",
    "bu.if_dir_not_exist_make(\"data/4_chunks\")\n",
    "bu.quickJSON(AUTO_INFO, f\"data/4_chunks/{CHECKPOINT}-dynamic_info.json\")\n",
    "bu.quickJSON({\"chunks\": chunks, \"embeddings\": embeddings}, f\"data/4_chunks/{CHECKPOINT}-dynamic_embeddings.json\")\n",
    "############################################ VANITY BELOW ########################################\n",
    "# VANITY PRINT\n",
    "tokens_in_chunks = 0\n",
    "for chunks_most_similar in dynamic_chunks_most_similar:\n",
    "    for chunk in chunks_most_similar:\n",
    "        tokens_in_chunks += utils.count_tokens(chunk)\n",
    "\n",
    "del chunks_most_similar_embeddings # free memory\n",
    "print(f\"Tokens in average chunk group: {tokens_in_chunks/len(dynamic_chunks_most_similar)}\")\n",
    "bu.quickJSON(dynamic_chunks_most_similar, filename=f\"data/4_chunks/{CHECKPOINT}-dynamic_chunks.json\")\n",
    "###########################################\n",
    "# Vanity preview\n",
    "preview_text = \"\"\n",
    "PREVIEW_LIMIT = 5\n",
    "for i, chunks_most_similar in enumerate(dynamic_chunks_most_similar):\n",
    "    preview_text += f\"==============Prompt: {dynamic_retrieval_prompts[i]}==============\\n\"\n",
    "    for j, chunk in enumerate(chunks_most_similar):\n",
    "        if j >= PREVIEW_LIMIT:\n",
    "            break\n",
    "        preview_text += f\"=======CHUNK {j}=======\\n{chunk}\\n\\n\"\n",
    "    preview_text += \"\\n\\n\"\n",
    "bu.quickTXT(preview_text, filename=f\"data/4_chunks/{CHECKPOINT}-dynamic_chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Builder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go through w/ both static and dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RETRIAVAL_METHOD = \"dynamic\" #static/dynamic/hybrid\n",
    "PROMPT_METHOD = \"IMPERSONATE\" #ARE/IMPERSONATE\n",
    "\n",
    "SUBJECT = \"Elias\"\n",
    "\n",
    "# Load Embeddings From File (optional)\n",
    "import json\n",
    "# with open(f\"data/4_chunks/{EMBEDDING_ID}-{CHUNKS_COUNT_IN_CTX}_{VERSION_ID}-dynamic_embeddings.json\", \"r\") as f:\n",
    "with open(f\"data/4_chunks/{CHECKPOINT}-{RETRIAVAL_METHOD}_embeddings.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    chunks = data[\"chunks\"]\n",
    "    embeddings = data[\"embeddings\"]\n",
    "print(f\"Chunks:{len(chunks)}, embeds:{len(embeddings)}\")\n",
    "\n",
    "# \"PERSONA_TEXT\": \"Favorite video games are Minecraft, Fortnite, and Call of Duty.\",\n",
    "# \"MED_MODULE\": \" \"\n",
    "# SURVEY_PROMPT = \"Determine how much {subject} aggree with the statement. Guestimate how {subject} would answer to the question\"\n",
    "TINY_MODULE = \"You are Elias, a 24 year old business and IT student from Copenhagen, where you now live in a dormatory.\"\n",
    "\n",
    "####################### You are {SUBJECT} vs you will impersonate {SUBJECT} #####################\n",
    "PREP_CHECKPOINT = f\"{CHECKPOINT}-{RETRIAVAL_METHOD}_{SUBJECT}-{PROMPT_METHOD}\"\n",
    "PREP_CHECKPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################### Method A ############################################\n",
    "if PROMPT_METHOD == \"IMPERSONATE\":\n",
    "    pre_prompt_template = \"\"\"\n",
    "SYS_MSG = {\n",
    "    \"role\": \"system\", \n",
    "    \"content\": f\"You are an expert actor, specializing in impersonation of non-famouns people. You will be presented to the subject through explicit datapoints of their digital footprint. In addition, you will deduct their implicit {SURVEY} by shadowing chats between the subject and friends. You will be asked to fully immerse yourself in the role, and answer questions from the point of view of the persona. \\\\n\\\\n**The persona, which you will be tasked to mimick is named '{SUBJECT}'.** \\\\n#Context \\\\n##Chat conversations between the subject and their friends:\\\\n**From most to least related**\\\\n\"\n",
    "}\n",
    "ASSIST_MSG = {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": f\"Understood. I will answer from the point of view of the persona, {SUBJECT}, based on what I could the deduct from the text provided.\"\n",
    "}\n",
    "USER_MSG = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": f\"Persona is questioned about their {SURVEY} in an {METHOD}. The persona must choose an appropriate answer to the question below with one of these five given options: {', '.join(surv.POSSIBLE_ANSWERS)}. Persona's answer must only contain the chosen option, without any elaboration, nor introduction.\"\n",
    "}\n",
    "\"\"\"\n",
    "########################################### Method B ###########################################\n",
    "elif PROMPT_METHOD == \"ARE\":\n",
    "    pre_prompt_template = \"\"\"\n",
    "SYS_MSG = {\n",
    "    \"role\": \"system\", \n",
    "    \"content\": f\"**{TINY_MODULE}**. You have shared your thoughts, feelings, and experiences through text messages with friedns. Answer the following questions honestly and naturally, as you would in everyday conversations. \\\\n\\\\n#Context \\\\n##Conversations between persona and friends:\"\n",
    "}\n",
    "ASSIST_MSG = {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": f\"Understood. I am {SUBJECT}, and I will answer the survey to the best of my ability.\"\n",
    "}\n",
    "USER_MSG = {   \n",
    "    \"role\": \"user\",\n",
    "    \"content\": f\"The survey is about your {SURVEY}. You must choose an appropriate answer to the question below with one of these five given options: {', '.join(surv.POSSIBLE_ANSWERS)}. Your answer must only contain the chosen option, without any elaboration, nor introduction.\\n**From most to least related**\\\\n\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "exec(pre_prompt_template)\n",
    "\n",
    "print(f\"{SYS_MSG['content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIABLES = {\n",
    "    \"Which survey\": surv,\n",
    "    # \"Prompt method\": f\"You are {SUBJECT} vs you will impersonate {SUBJECT}\",\n",
    "    # \"Retrieval method\": \"Dynamic/static/hybrid\",\n",
    "    \"RETRIEVAL_PROMPT\": RETRIEVAL_PROMPT,\n",
    "    \"EMBEDDING_ID\": EMBEDDING_ID,\n",
    "    \"VERSION_ID\": VERSION_ID,\n",
    "    \"CHUNK_SIZE\": CHUNK_SIZE,\n",
    "    \"OVERLAP_SIZE\": OVERLAP_SIZE,\n",
    "    \"EMBED_MODEL\": EMBED_MODEL,\n",
    "    \"CHUNKS_COUNT_IN_CTX\": CHUNKS_COUNT_IN_CTX,\n",
    "    \"EMBEDDING_ID\": EMBEDDING_ID,\n",
    "    \"DYNAMIC_CHUNKS_COUNT\": len(dynamic_chunks_most_similar),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompts = []\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "for question, chunks_most_similar in zip(surv.questions, dynamic_chunks_most_similar):\n",
    "    p = [\n",
    "        systemMsg(\"\\\\n\".join([\n",
    "            f\"{SYS_MSG['content']}\",\n",
    "            \"\\\\nNEW CONVERSATION:\\\\n\".join(chunks_most_similar)\n",
    "        ])),      \n",
    "        assistantMsg(ASSIST_MSG['content']),\n",
    "        userMsg(\"\\\\n\".join([\n",
    "            f\"{USER_MSG['content']}\\\\n\\\\n**Your question is:**\\\\n\\\\n\",\n",
    "            question,\n",
    "            \"\\\\nThe persona chooses:\"\n",
    "        ]))]\n",
    "    final_prompts.append(p)\n",
    "\"\"\"    \n",
    "exec(prompt_template)\n",
    "prompt_info = utils.describe_prompts_and_print(final_prompts)\n",
    "bu.quickJSON(final_prompts, f\"data/5_prep/{PREP_CHECKPOINT}_prompts.json\")\n",
    "print(f\"{len(final_prompts)}\")#,{final_prompts[:1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompts = []\n",
    "prompt_template = \"\"\"\n",
    "for question in surv.questions:\n",
    "    p = [\n",
    "        systemMsg(\"\\\\n\".join([\n",
    "            f\"{SYS_MSG['content']}\",\n",
    "            \"\\\\nNEW CONVERSATION:\\\\n\".join(chunks_most_similar)\n",
    "        ])),  \n",
    "        assistantMsg(ASSIST_MSG['content']),\n",
    "        userMsg(\"\\\\n\".join([\n",
    "            f\"{USER_MSG['content']}\\\\n\\\\n**Your question is:**\\\\n\\\\n\",\n",
    "            question,\n",
    "            \"\\\\nThe persona chooses:\"\n",
    "        ]))]\n",
    "    final_prompts.append(p)\n",
    "\"\"\"\n",
    "exec(prompt_template)\n",
    "prompt_info = utils.describe_prompts_and_print(final_prompts)\n",
    "bu.quickJSON(final_prompts, f\"data/5_prep/{PREP_CHECKPOINT}_prompts.json\")\n",
    "print(f\"{len(final_prompts)}\")#,{final_prompts[:1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base (no persona)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompts = []\n",
    "prompt_template = \"\"\"\n",
    "for question in surv.questions:\n",
    "    p = [\n",
    "        systemMsg(\n",
    "            \"You are participating in a survey. You will be presented with a series of questions about your {SURVEY}.\",\n",
    "            f\"You must choose answer to the question below with one of the five options: {', '.join(surv.POSSIBLE_ANSWERS)}. The answer must only contain the chosen option. \"\n",
    "        ),\n",
    "        assistantMsg('Understood. I will answer the question below with one of the given options.'),\n",
    "        userMsg(\n",
    "            question,\n",
    "            \"Your choice: \"\n",
    "        )]\n",
    "    final_prompts.append(p)\n",
    "\"\"\"\n",
    "exec(prompt_template)\n",
    "prompt_info = utils.describe_prompts_and_print(final_prompts) # Vanity print\n",
    "bu.quickJSON(final_prompts, f\"data/5_prep/{WHICH_SURVEY}_base_prompts.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"llama3\"\n",
    "\n",
    "instructions = {\n",
    "    \"prompt_file\": f\"batch/prompts/{WHICH_SURVEY}_base_prompt.json\",\n",
    "    \"survey_type\": f\"{SURVEY_TYPE}\",\n",
    "    \"isLocal\": True,\n",
    "    \"LIMIT\": None\n",
    "}\n",
    "settings = {\n",
    "    \"model\": MODEL,\n",
    "    \"timeout\": 300\n",
    "}\n",
    "AUTO_INFO = {\n",
    "    \"survey\": WHICH_SURVEY,\n",
    "    \"prompt_template\": prompt_template,\n",
    "    **utils.describe_prompts([])\n",
    "    }\n",
    "bu.quickJSON({\"instructions\": instructions, \"settings\": settings, \"info\": AUTO_INFO}, f\"data/5_prep/{WHICH_SURVEY}_base_batch-schema.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
