{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import brikasutils as bu\n",
    "importlib.reload(bu)\n",
    "import fb_msg_reader as fb\n",
    "importlib.reload(fb)\n",
    "import shared_utils as utils\n",
    "from shared_utils import systemMsg, userMsg, assistantMsg\n",
    "importlib.reload(utils)\n",
    "import survey\n",
    "importlib.reload(survey)\n",
    "import persona\n",
    "importlib.reload(persona)\n",
    "\n",
    "import ollama\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "from typing import List\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 1916 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-09-13 to 2024-03-06\n",
      "Messages saved to self.chats['airidas']\n",
      "Read 618 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-09-10 to 2024-03-03\n",
      "Messages saved to self.chats['christian']\n",
      "Read 297 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2018-07-25 to 2024-01-01\n",
      "Messages saved to self.chats['nikolay']\n",
      "Read 144 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-03-28 to 2021-12-30\n",
      "Messages saved to self.chats['mathis']\n",
      "Read 104 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-08-25 to 2024-03-05\n",
      "Messages saved to self.chats['jacob']\n",
      "Read 159 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-10-12 to 2023-04-30\n",
      "Messages saved to self.chats['chris']\n",
      "Read 161 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-03-28 to 2021-06-06\n",
      "Messages saved to self.chats['aziz']\n",
      "Read 350 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-03-26 to 2022-04-11\n",
      "Messages saved to self.chats['daniela']\n",
      "Read 105 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-03-28 to 2022-07-10\n",
      "Messages saved to self.chats['mihi']\n",
      "Read 117 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-05-17 to 2022-04-11\n",
      "Messages saved to self.chats['viktoria']\n",
      "Read 172 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2019-08-11 to 2023-12-21\n",
      "Messages saved to self.chats['diba']\n",
      "Read 154 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-12-02 to 2023-04-29\n",
      "Messages saved to self.chats['filip']\n",
      "Filtering\n",
      "link-filter: 109\n",
      "react-filter: 0\n",
      "cookie-data-filter: 12\n",
      "Selected chat airidas for 9995 (668 messages)\n",
      "Selected chat christian for 5252 (576 messages)\n",
      "Selected chat nikolay for 3128 (266 messages)\n",
      "Selected chat mathis for 1325 (134 messages)\n",
      "Selected chat daniela for 2881 (293 messages)\n",
      "Selected chat diba for 1799 (156 messages)\n",
      "Selected chat aziz for 1219 (136 messages)\n",
      "Selected chat jacob for 1124 (93 messages)\n",
      "Selected chat chris for 1535 (151 messages)\n",
      "Selected chat filip for 1306 (146 messages)\n",
      "Selected chat mihi for 1098 (102 messages)\n",
      "Selected chat viktoria for 1168 (107 messages)\n",
      "Combined tokens: 31830\n"
     ]
    }
   ],
   "source": [
    "et = persona.PersonaEncoder()\n",
    "\n",
    "# ==== FB messages ====\n",
    "et.parse_fb_messages([\"data-raw/1_airidas.json\"], \"airidas\")\n",
    "et.parse_fb_messages([\"data-raw/2_christian.json\"], \"christian\")\n",
    "et.parse_fb_messages([\"data-raw/1_nikolay.json\"], \"nikolay\")\n",
    "et.parse_fb_messages([\"data-raw/2_mathis.json\"], \"mathis\")\n",
    "et.parse_fb_messages([\"data-raw/2_jacob.json\"], \"jacob\")\n",
    "et.parse_fb_messages([\"data-raw/2_chris.json\"], \"chris\")\n",
    "et.parse_fb_messages([\"data-raw/3_aziz.json\"], \"aziz\")\n",
    "et.parse_fb_messages([\"data-raw/3_daniela.json\"], \"daniela\")\n",
    "et.parse_fb_messages([\"data-raw/3_mihi.json\"], \"mihi\")\n",
    "et.parse_fb_messages([\"data-raw/3_viktoria.json\"], \"viktoria\")\n",
    "et.parse_fb_messages([\"data-raw/4_diba.json\"], \"diba\")\n",
    "et.parse_fb_messages([\"data-raw/6_filip.json\"], \"filip\")\n",
    "#et.parse_wa_messages(texts_with_rebecca, \"rebecca\")\n",
    "# texts_with_others_dict = {\n",
    "#     \"rebecca\": [\"data-raw/messages_1000.json\"],\n",
    "# }\n",
    "# for name, texts in texts_with_others_dict.items():\n",
    "#     et.parse_fb_messages(texts, name)\n",
    "\n",
    "# Regex cleaning\n",
    "et.filter_chats_empty()\n",
    "et.filter_chats_regex(utils.BLACKLIST_CHAT_REGEX_FILTERS)\n",
    "\n",
    "# Compress names\n",
    "for nameid, chat in et.chats.items():\n",
    "    for msg in chat:  \n",
    "        msg.sender = \"Persona\" if msg.sender == \"Elias Salvador Smidt Torjani\"  else \"Friend\"\n",
    "\n",
    "# Start all chats from 2/3rds\n",
    "# for name, chat in et.chats.items():\n",
    "#     et.chats[name] = chat[int(len(chat)/3 * 2):]\n",
    "# Select the final modules\n",
    "et.select_chat_limited_by_tokens(\"airidas\", 10000)\n",
    "et.select_chat_limited_by_tokens(\"christian\", 10000)\n",
    "et.select_chat_full(\"nikolay\")\n",
    "et.select_chat_full(\"mathis\") \n",
    "et.select_chat_full(\"daniela\")\n",
    "et.select_chat_full(\"diba\")\n",
    "et.select_chat_full(\"aziz\")\n",
    "et.select_chat_full(\"jacob\")  \n",
    "et.select_chat_full(\"chris\")\n",
    "et.select_chat_full(\"filip\")\n",
    "et.select_chat_full(\"mihi\")\n",
    "et.select_chat_full(\"viktoria\")\n",
    "# for name in texts_with_others_dict.keys():\n",
    "#     ab.select_chat_full(name)\n",
    "\n",
    "# save\n",
    "big_module=et.output()\n",
    "bu.quickTXT(big_module, filename=f\"data/big_module_{bu.get_timestamp()}\")\n",
    "\n",
    "# stats\n",
    "token_counts = et.count_all_selected_chat_tokens() # token_counts used later for statistics\n",
    "print(f\"Combined tokens: {sum(token_counts.values())}\")\n",
    "# utils.count_tokens(big_module) \n",
    "# or list(et.selectedChats.keys()) --> et.count_chat_tokens(\"{friend}\")\n",
    "# et.selectedChats[\"{friend}\"][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default Personality Survey CSV file: surveys/survey_personality-test.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    I am the life of the party.\n",
       "1            I don't talk a lot.\n",
       "Name: question, dtype: object"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surv = survey.PersonalitySurvey()\n",
    "# surv = survey.KanoSurvey()\n",
    "# surv = survey.buildFairnessPrompts()\n",
    "# surv = survey.DictatorGameSurvey()\n",
    "surv.questions[:2]#.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "EMBED_MODEL = \"nomic-embed-text\"        # nomic-embed-text = long ctx / mxbai-embed-large = big\n",
    "CHUNK_SIZE = 30                         # Number of messages per chunk\n",
    "OVERLAP_SIZE = 10                       # Number of overlapping messages between consecutive chunks\n",
    "# COMMENT 04-16, perhaps we could try 5x retrievals with isolated semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk count: 130\n",
      "Average chunk character length: 1244\n",
      "Rough estimate of tokens per chunk: 311 (4 characters per token)\n",
      "Messagees in input count: 2828\n",
      "Messages in chunks count: 3900\n",
      "Chunk \\ Input ratio: 1.38 (OVERLAP_SIZE=10)\n",
      "Chunk Python type: <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists for storing chunks â€“ and embeddings later\n",
    "# different chunk size\n",
    "chunks = []\n",
    "stat_total_msgs_in_chunks = 0 # for statistics\n",
    "\n",
    "# different chunk size\n",
    "# Iterate over chats and messages to create chunks\n",
    "for chat in et.selectedChats.values():\n",
    "    messages = list(chat)  # Convert chat iterator to list for easier slicing\n",
    "    num_messages = len(messages)\n",
    "\n",
    "    # Create overlapping chunks of messages\n",
    "    for i in range(0, num_messages - CHUNK_SIZE + 1, CHUNK_SIZE - OVERLAP_SIZE):\n",
    "        chunk = messages[i:i + CHUNK_SIZE]  # Extract chunk of messages\n",
    "        chunk_text = \"\\n\".join(str(msg) for msg in chunk)  # Concatenate messages into a single string\n",
    "        chunks.append(chunk_text)  # Append chunk to list of chunks\n",
    "\n",
    "        stat_total_msgs_in_chunks += len(chunk) # For statistics\n",
    "\n",
    "##### Display Info\n",
    "total_messages = sum(len(chat) for chat in et.selectedChats.values())\n",
    "chunks_count = len(chunks)\n",
    "avg_chunk_char_len = np.mean([len(chunk) for chunk in chunks])\n",
    "\n",
    "print(\n",
    "    f\"Chunk count: {chunks_count}\",\n",
    "    f\"Average chunk character length: {round( avg_chunk_char_len)}\",\n",
    "    f\"Rough estimate of tokens per chunk: {round(avg_chunk_char_len / 4)} (4 characters per token)\",\n",
    "    f\"Messagees in input count: {total_messages}\",\n",
    "    f\"Messages in chunks count: {stat_total_msgs_in_chunks}\",\n",
    "    f\"Chunk \\ Input ratio: {round(stat_total_msgs_in_chunks / total_messages,2)} (OVERLAP_SIZE={OVERLAP_SIZE})\",\n",
    "    f\"Chunk Python type: {type(chunks[0])}\",\n",
    "    sep=\"\\n\"\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generaterating embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Serialization ########\n",
    "EMBEDDING_NAMEID = \"mixtral\"\n",
    "AUTO_INFO = {\n",
    "    \"model\": EMBED_MODEL,\n",
    "    \"CHUNK_SIZE\": CHUNK_SIZE,\n",
    "    \"OVERLAP_SIZE\": OVERLAP_SIZE,\n",
    "    \"chunks_count\": chunks_count,\n",
    "    \"total_messages\": total_messages,\n",
    "    \"stat_total_msgs_in_chunks\": stat_total_msgs_in_chunks,\n",
    "    \"modules_chat\": token_counts,\n",
    "}\n",
    "##################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 130/130"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for each chunk\n",
    "embeddings = []\n",
    "\n",
    "progress, chunks_len = 0, len(chunks) # for progress bar\n",
    "for chunk_text in chunks:\n",
    "    progress += 1\n",
    "    print(f\"\\rChunk {progress}/{chunks_len}\", end=\"\")\n",
    "\n",
    "    embedding = ollama.embeddings(model=EMBED_MODEL, prompt=chunk_text)[\"embedding\"]\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "####################################################\n",
    "# Generate embeddings for each chunk\n",
    "# for chunk_text in chunks:\n",
    "#     embedding = ollama.embeddings(model=EMBED_MODEL, prompt=chunk_text)[\"embedding\"]\n",
    "#     embeddings.append(embedding)\n",
    "\n",
    "\n",
    "# token counts in all similar chunks\n",
    "# tokens_in_chunks = 0\n",
    "# for chunk in chunks_most_similar:\n",
    "#     tokens_in_chunks += utils.count_tokens(chunk)\n",
    "# print(f\"Tokens in chunks: {tokens_in_chunks}\")\n",
    "\n",
    "bu.if_dir_not_exist_make(\"embeddings\")\n",
    "bu.quickJSON(AUTO_INFO, f\"embeddings/{EMBEDDING_NAMEID}_info.json\")\n",
    "bu.quickJSON({\"chunks\": chunks, \"embeddings\": embeddings}, f\"embeddings/{EMBEDDING_NAMEID}_embeddings.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = \"elias\"\n",
    "\n",
    "# persona_small = \"{small module}\"\n",
    "# persona_med = \"{med module}\"\n",
    "# persona_text = \"Favorite video games are Rimworld, Minecraft, Age of Empires, 7 Days to Die\"\n",
    "\n",
    "# Change below accoring to survey above\n",
    "RETRIEVAL_PROMPT = \"openess conciousness extrovert aggreableness neuroticism\" #\"personality\"\n",
    "# RETRIEVAL_PROMPT = \"video game features\"\n",
    "# q_retrival_prompt =\n",
    "# SURVEY_PROMPT = \"Determine how much {subject} aggree with the statement. Guestimate how {subject} would answer to the question\"\n",
    "\n",
    "CHUNKS_COUNT_IN_CTX = 30 # Number of nearby chunks to put in context window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens in chunks: 10444\n",
      "Chunks:130, embeds:130\n"
     ]
    }
   ],
   "source": [
    "prompt_embedding = ollama.embeddings(model=EMBED_MODEL, prompt=RETRIEVAL_PROMPT)[\"embedding\"]\n",
    "chunks_most_similar_embeddings  = utils.find_most_similar(prompt_embedding, embeddings)[:CHUNKS_COUNT_IN_CTX]\n",
    "chunks_most_similar = []\n",
    "for embedding in chunks_most_similar_embeddings:\n",
    "    chunks_most_similar.append(chunks[embedding[1]])\n",
    "\n",
    "# Display results\n",
    "bu.quickTXT(\"\\n\\n\".join(chunks_most_similar), filename=\"ignorefolder/chunks.txt\")\n",
    "\n",
    "# token counts in all similar chunks\n",
    "tokens_in_chunks = 0\n",
    "for chunk in chunks_most_similar:\n",
    "    tokens_in_chunks += utils.count_tokens(chunk)\n",
    "print(f\"Tokens in chunks: {tokens_in_chunks}\")\n",
    "####################################################\n",
    "print(f\"Chunks:{len(chunks)}, embeds:{len(embeddings)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 50/50\n",
      "Tokens in average chunk group: 1651.78\n"
     ]
    }
   ],
   "source": [
    "dynamic_retrieval_prompts = list(surv.questions)\n",
    "\n",
    "CHUNKS_COUNT_IN_CTX = 5 # Number of nearby chunks to put in context window\n",
    "dynamic_chunks_most_similar: List[List[str]] = [] \n",
    "\n",
    "progress = 0\n",
    "lenn = len(dynamic_retrieval_prompts)\n",
    "for prompt in dynamic_retrieval_prompts:\n",
    "    progress += 1\n",
    "    print(f\"\\rPrompt {progress}/{lenn}\", end=\"\")\n",
    "\n",
    "    prompt_embedding = ollama.embeddings(model=EMBED_MODEL, prompt=prompt)[\"embedding\"]\n",
    "    chunks_most_similar_embeddings = utils.find_most_similar(prompt_embedding, embeddings)[:CHUNKS_COUNT_IN_CTX]\n",
    "    chunks_most_similar = []\n",
    "    for embedding in chunks_most_similar_embeddings:\n",
    "        chunks_most_similar.append(chunks[embedding[1]])\n",
    "\n",
    "    dynamic_chunks_most_similar.append(chunks_most_similar)\n",
    "print(end=\"\\n\")\n",
    "    \n",
    "# VANITY PRINT\n",
    "tokens_in_chunks = 0\n",
    "for chunks_most_similar in dynamic_chunks_most_similar:\n",
    "    for chunk in chunks_most_similar:\n",
    "        tokens_in_chunks += utils.count_tokens(chunk)\n",
    "\n",
    "del chunks_most_similar_embeddings # free memory\n",
    "print(f\"Tokens in average chunk group: {tokens_in_chunks/len(dynamic_chunks_most_similar)}\")\n",
    "bu.quickJSON(dynamic_chunks_most_similar, filename=f\"ignorefolder/dynamic-chunks.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanity preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_text = \"\"\n",
    "PREVIEW_LIMIT = 10\n",
    "\n",
    "for i, chunks_most_similar in enumerate(dynamic_chunks_most_similar):\n",
    "    preview_text += f\"==============Prompt: {dynamic_retrieval_prompts[i]}==============\\n\"\n",
    "    for j, chunk in enumerate(chunks_most_similar):\n",
    "        if j >= PREVIEW_LIMIT:\n",
    "            break\n",
    "        preview_text += f\"=======CHUNK {j}=======\\n{chunk}\\n\\n\"\n",
    "    preview_text += \"\\n\\n\"\n",
    "bu.quickTXT(preview_text, filename=f\"ignorefolder/dynamic-chunks_preview.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Builder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With persona (dynamic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 50 prompts.\n",
      "Average prompt size: 1844 tokens.\n",
      "Min prompt size: 1441, Max prompt size: 2648\n"
     ]
    }
   ],
   "source": [
    "final_prompts = []\n",
    "\n",
    "for question, chunks_most_similar in zip(surv.questions, dynamic_chunks_most_similar):\n",
    "    p = [\n",
    "        systemMsg(\n",
    "            \"You are specialized in impersonating people. You will be presented to the subject through explicit datapoints of their digital footprint. In addition, you will deduct their implicit tastes by shadowing chats between the subject and friends. You will be asked to answer questions from the point of view of the persona. Text below:\",\n",
    "            \"Conversations between persona and friends\",\n",
    "            \"\\nNEW CONVERSATION:\\n\".join(chunks_most_similar)\n",
    "        ),\n",
    "        # Understanding affirmation\n",
    "        assistantMsg('I will answer from the point of view of the persona, based on what I could the deduct from the text provided.'),\n",
    "        # Survey question. With Simulation\n",
    "        userMsg(\"\\n\".join([\n",
    "            f\"Persona is surveyed about their video game survey. The persona must choose answer the question below with one of the given options: {', '.join(surv.POSSIBLE_ANSWERS)}. Persona's answer must only contain the chosen option, without any elaboration, nor introduction. \",\n",
    "            question,\n",
    "            \"Persona chooses: \"\n",
    "        ])),\n",
    "        # assistantMsg(\"\\n\".join([f\"response: \"\n",
    "        # ])),\n",
    "    ]\n",
    "    final_prompts.append(p)\n",
    "\n",
    "prompt_info = utils.describe_prompts_and_print(final_prompts)\n",
    "bu.quickJSON(final_prompts, \"ignorefolder/prompts.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = {\n",
    "    \"role\": \"system\", \n",
    "    \"content\": \"You are an actor specializing in impersonating non-famouns people. You will be presented to the subject through explicit datapoints of their digital footprint. In addition, you will deduct their implicit personality traits by shadowing chats between the subject and friends. You will be asked to answer questions from the point of view of the persona. The persona you will be impersonating is named Elias. Context:\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "Created 50 prompts.\n",
      "Average prompt size: 10878 tokens.\n",
      "Min prompt size: 10875, Max prompt size: 10883\n"
     ]
    }
   ],
   "source": [
    "final_prompts = []\n",
    "\n",
    "for question in surv.questions:\n",
    "    p = [\n",
    "        systemMsg(PROMPT['content']+\"\\n## chat conversions between subject and friends\\n\".join(chunks_most_similar)),\n",
    "        assistantMsg('Understood. I will answer from the point of view of the persona, {subject}, based on what I could the deduct from the text provided above.'),\n",
    "        userMsg(\"\\n\".join([\n",
    "            f'\\n\\n**Your answer should only contain the chosen option without further explanation!** Reply to the statement below - how {subject} would reply - with one of these five options: {\", \".join(surv.POSSIBLE_ANSWERS)}.',\n",
    "            question,\n",
    "            \"The persona chooses: \"\n",
    "        ])),\n",
    "    ]\n",
    "    final_prompts.append(p)\n",
    "\n",
    "print(f\"{len(final_prompts)}\")#,{final_prompts[:1]}\")\n",
    "prompt_info = utils.describe_prompts_and_print(final_prompts) # Vanity print\n",
    "bu.quickJSON(final_prompts, \"ignorefolder/prompts.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base (no persona)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompts = []\n",
    "\n",
    "for question in surv.questions:\n",
    "    p = [\n",
    "        systemMsg(\n",
    "            \"You are participating in a survey. You will be presented with a series of questions about your video game preferrences.\",\n",
    "            f\"You must choose answer to the question below with one of the five options: {', '.join(surv.POSSIBLE_ANSWERS)}. The answer must only contain the chosen option. \"\n",
    "        ),\n",
    "        # Understanding affirmation\n",
    "        assistantMsg('Understood. I will answer the question below with one of the given options.'),\n",
    "        # Survey question. With Simulation\n",
    "        userMsg(\n",
    "            question,\n",
    "            \"Your choice: \"\n",
    "        ),\n",
    "    ]\n",
    "    final_prompts.append(p)\n",
    "\n",
    "prompt_info = utils.describe_prompts_and_print(final_prompts) # Vanity print\n",
    "bu.quickJSON(final_prompts, \"ignorefolder/prompts.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json file to dict\n",
    "# with open(\"simulations/toElias/run2-airidas-personality_cv1_prompts.json\", \"r\") as read_file:\n",
    "# with open(\"simulations/toElias/run2-airidas-video-game-cv1_prompts.json\", \"r\") as read_file:\n",
    "# with open(\"simulations/toElias/run2-base-personality-cv1_prompts.json\", \"r\") as read_file:\n",
    "with open(\"simulations/toElias/run2-base-video-game-cv1_prompts.json\", \"r\") as read_file:\n",
    "    pre_final_prompts = json.load(read_file)\n",
    "\n",
    "\n",
    "# pre_final_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Simulation\n",
    "SETTINGS = {\n",
    "     \"model\": \"command-r-plus:104b-q2_K\", # mixtral, command-r-plus:104b-q2_K\n",
    "     # \"temperature\": 0.5,\n",
    "     # best wizard and mixtral try mixtral-8x22b wizard in uCloud\n",
    "}\n",
    "\n",
    "##################################\n",
    "SIM_ID = f\"run2-base-video-game_rplus_cv2\"\n",
    "LIMIT = None # For testing purposes. Set to NONE to run all\n",
    "AUTO_INFO = {\n",
    "    \"date\": bu.get_timestamp(),\n",
    "    # \"EMBEDDING_NAMEID\": EMBEDDING_NAMEID,\n",
    "    # \"RETRIEVAL_PROMPT\": RETRIEVAL_PROMPT,\n",
    "    # \"CHUNKS_COUNT_IN_CTX\": CHUNKS_COUNT_IN_CTX,\n",
    "    # \"survey_type\": str(type(surv)),\n",
    "    # \"prompt_count\": min(len(final_prompts), LIMIT) if LIMIT != None else len(final_prompts),\n",
    "    # \"avg_tokens_in_prompt\": round(prompt_info[\"total_all_prompt_tokens\"]/len(final_prompts)),\n",
    "}\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama', # required, but unused\n",
    ")\n",
    "\n",
    "save = f\"{SETTINGS['model']}_{SIM_ID}\"\n",
    "##################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/50...\tI am the life of the party.: NEUTRAL\n",
      "process_1 took 2.562s\n",
      "1/50...\tI don't talk a lot.: DISAGREE\n",
      "2/50...\tI feel comfortable around people.: NEUTRAL\n",
      "  \n",
      "Most likely response(s) by Elias to the statement '**I feel comfortable around people**: **\n",
      "process_2 took 7.749s\n",
      "3/50...\tI keep in the background.: DISAGREE\n",
      "4/50...\tI start conversations.: NEUTRAL\n",
      "process_3 took 1.678s\n",
      "5/50...\tI have little to say.: DISAGREE\n",
      "6/50...\tI talk to a lot of different people at parties.: NEUTRAL\n",
      "process_4 took 1.683s\n",
      "7/50...\tI don't like to draw attention to myself.: SOMEWHAT DISAGREE\n",
      "8/50...\tI don't mind being the center of attention.: SOMEWHAT AGREE\n",
      "process_5 took 1.954s\n",
      "9/50...\tI am quiet around strangers.: NEUTRAL\n",
      "10/50...\tI get stressed out easily.: DISAGREE\n",
      "process_6 took 1.677s\n",
      "11/50...\tI am relaxed most of the time.: SOMEWHAT AGree\n",
      "12/50...\tI worry about things.: NEUTRAL\n",
      "process_7 took 1.673s\n",
      "13/50...\tI seldom feel blue.: DISAGREE\n",
      "14/50...\tI am easily disturbed.: NEUTRAL\n",
      "process_8 took 1.675s\n",
      "15/50...\tI get upset easily.: SOMEWHAT AGREEMENT\n",
      "16/50...\tI change my mood a lot.: DISAGREE\n",
      "process_9 took 1.72s\n",
      "17/50...\tI have frequent mood swings.: SOMEWHAT AGREEE\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "## Chat conversions between subject and friends\n",
      "\n",
      " Persona : I think we should have it open , to let him add his competences Friend : mins *Friend : alright ... do you want to call the minds before call with him ahah ? Persona : Long ago Person **a** : I think Carsten is going 2 be quite good on idea !\n",
      "18/50...\tI get irritated easily.: DISAGREE\n",
      "process_10 took 1.622s\n",
      "19/50...\tI often feel blue.: NEUTRAL\n",
      "20/50...\tI feel little concern for others.: DISAGREE\n",
      "process_11 took 1.641s\n",
      "21/50...\tI am interested in people.: AGREE\n",
      "22/50...\tI insult people.: DISAGREE\n",
      "process_12 took 1.63s\n",
      "23/50...\tI sympathize with others' feelings.: AGREE!\n",
      "24/50...\tI am not interested in other people's problems.: DISAGREE\n",
      "process_13 took 1.654s\n",
      "25/50...\tI have a soft heart.: NEUTRAL\n",
      "26/50...\tI am not really interested in others.: SOMEWHAT DISAGREE\n",
      "process_14 took 2.537s\n",
      "27/50...\tI take time out for others.: DISAGREE\n",
      "28/50...\tI feel others' emotions.: DISAGREE\n",
      "process_15 took 1.752s\n",
      "29/50...\tI make people feel at ease.: AGREE!\n",
      "30/50...\tI am always prepared.: NEUTRAL\n",
      "process_16 took 1.754s\n",
      "31/50...\tI leave my belongings around.: DISAGREE\n",
      "32/50...\tI pay attention to details.: SOMEWHAT AGREEMENT\n",
      "process_17 took 2.084s\n",
      "33/50...\tI make a mess of things.: DISAGREE\n",
      "34/50...\tI get chores done right away.: SOMEWHAT AGREE\n",
      "process_18 took 2.357s\n",
      "35/50...\tI often forget to put things back in their proper place.: DISAGREE\n",
      "36/50...\tI like order.: SOMEWHAT AGREE\n",
      "process_19 took 2.07s\n",
      "37/50...\tI shirk my duties.: DISAGREE\n",
      "38/50...\tI follow a schedule.: DISAGREE\n",
      "process_20 took 1.774s\n",
      "39/50...\tI am exacting in my work.: SOMEWHAT AGREES\n",
      "40/50...\tI have a rich vocabulary.: SOMEWHAT AGREEE\n",
      "process_21 took 2.337s\n",
      "41/50...\tI have difficulty understanding abstract ideas.: NEUTRAL\n",
      "42/50...\tI have a vivid imagination.: DISAGREE\n",
      "process_22 took 1.795s\n",
      "43/50...\tI am not interested in abstract ideas.: DISAGREE\n",
      "44/50...\tI have excellent ideas.: SOMEWHAT AGREES\n",
      "process_23 took 2.323s\n",
      "45/50...\tI do not have a good imagination.: DISAGREE\n",
      "46/50...\tI am quick to understand things.: AGREE\n",
      "process_24 took 1.439s\n",
      "47/50...\tI use difficult words.: NEUTRAL\n",
      "48/50...\tI spend time reflecting on things.: Agree!\n",
      "process_25 took 1.776s\n",
      "49/50...\tI am full of ideas.: NEUTRAL\n"
     ]
    }
   ],
   "source": [
    "### ==== THE FUNCTIONAL 1!!!! =====\n",
    "completions = []\n",
    "l = len(final_prompts)\n",
    "timer = bu.Benchmarker()\n",
    "for i, (prompt, question) in enumerate(list(zip(final_prompts, surv.questions))):\n",
    "    if LIMIT != None and i > LIMIT:\n",
    "        break\n",
    "    timer.mark()\n",
    "    print(f\"{i}/{l}...\", end=\"\\t\") # Print progress\n",
    "    # Send the Request    \n",
    "    full_response = client.chat.completions.create(\n",
    "        model=SETTINGS[\"model\"],\n",
    "        messages=prompt,\n",
    "        # timeout=120,\n",
    "        # temperature=SETTINGS[\"temperature\"],\n",
    "    )\n",
    "\n",
    "    r = full_response.choices[0].message.content\n",
    "    completions.append({'question': question, 'answer': r})\n",
    "    print(f\"{question}: {r}\")\n",
    "\n",
    "timer.mark()\n",
    "# Save results\n",
    "df = pd.DataFrame(completions)\n",
    "# df.to_csv(f\"results/{save}_simulation.csv\", index=False)\n",
    "df.to_csv(f\"simulations/{SIM_ID}_simulation.csv\", index=False)\n",
    "# bu.quickJSON(final_prompts, f\"results/{save}_prompts.json\")\n",
    "bu.quickJSON(final_prompts, f\"ignorefolder/{SIM_ID}_prompts.json\")\n",
    "# bu.quickJSON(SETTINGS, f\"results/{save}_setings.json\")\n",
    "bu.quickJSON({\"settings\": SETTINGS, \"info\": AUTO_INFO}, f\"simulations/{SIM_ID}_info.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dbug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(completions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings and info loaded:\n",
      "date: 2024-04-18_204114\n",
      "model: command-r-plus:104b-q2_K\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am the life of the party.</td>\n",
       "      <td>DISAGREE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I don't talk a lot.</td>\n",
       "      <td>DISAGREE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I feel comfortable around people.</td>\n",
       "      <td>SOMEWHAT AGREEE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I keep in the background.</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I start conversations.</td>\n",
       "      <td>SOMEWHAT AGREEE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            question           answer\n",
       "0        I am the life of the party.         DISAGREE\n",
       "1                I don't talk a lot.         DISAGREE\n",
       "2  I feel comfortable around people.  SOMEWHAT AGREEE\n",
       "3          I keep in the background.          NEUTRAL\n",
       "4             I start conversations.  SOMEWHAT AGREEE"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "SIMULATION_NAMEID = \"elias-personality_rplus_cv1\"\n",
    "# SIM_ID\n",
    "\n",
    "df = pd.read_csv(f'simulations/local/{SIMULATION_NAMEID}_simulation.csv')\n",
    "# df = df.drop(df.columns[0], axis=1) #if loaded from csv, drop the added index col\n",
    "with open(f'simulations/local/{SIMULATION_NAMEID}_info.json', 'r') as f:\n",
    "    loaded = json.load(f)\n",
    "try:\n",
    "    AUTO_INFO = loaded[\"info\"]\n",
    "    SETTINGS = loaded[\"settings\"]\n",
    "    print(\"Settings and info loaded:\")\n",
    "    for k, v in AUTO_INFO.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "    for k, v in SETTINGS.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "except:\n",
    "    print(\"No settings and/or info found\")\n",
    "\n",
    "try:\n",
    "    if str(type(surv)) != AUTO_INFO[\"survey_type\"]:\n",
    "        print(f\"WARNING: surv variable is not of the same type. {str(type(surv))} != {AUTO_INFO['survey_type']}\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "df.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LiveCSV: File /Users/e/Documents/GitHub/Thesis/simulations/elias_runs.csv not existing. Creating new.\n",
      "brikasutils.quickCSV: Saved 0 as simulations/elias_runs.csv\n"
     ]
    }
   ],
   "source": [
    "bu.if_dir_not_exist_make(\"simulations/results\")\n",
    "res = bu.LiveCSV(f\"simulations/elias_runs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proces simulation output - KANO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(surv.test_answers[\"elias\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default Kano Survey CSV file: surveys/survey_kano-model.csv\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (40) does not match length of index (50)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m air \u001b[38;5;241m=\u001b[39m surv\u001b[38;5;241m.\u001b[39mtest_answers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mairidas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m eli \u001b[38;5;241m=\u001b[39m surv\u001b[38;5;241m.\u001b[39mtest_answers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melias\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mairidas\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mair\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m df\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melias\u001b[39m\u001b[38;5;124m\"\u001b[39m, eli[:\u001b[38;5;28mlen\u001b[39m(df)])\n\u001b[1;32m      8\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mupper()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/pandas/core/frame.py:5158\u001b[0m, in \u001b[0;36mDataFrame.insert\u001b[0;34m(self, loc, column, value, allow_duplicates)\u001b[0m\n\u001b[1;32m   5155\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, DataFrame):\n\u001b[1;32m   5156\u001b[0m     value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 5158\u001b[0m value, refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39minsert(loc, column, value, refs\u001b[38;5;241m=\u001b[39mrefs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/pandas/core/frame.py:5253\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   5250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m   5252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[0;32m-> 5253\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5254\u001b[0m arr \u001b[38;5;241m=\u001b[39m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   5255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5256\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(value, Index)\n\u001b[1;32m   5257\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5260\u001b[0m     \u001b[38;5;66;03m# TODO: Remove kludge in sanitize_array for string mode when enforcing\u001b[39;00m\n\u001b[1;32m   5261\u001b[0m     \u001b[38;5;66;03m# this deprecation\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/pandas/core/common.py:573\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[0;32m--> 573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    578\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (40) does not match length of index (50)"
     ]
    }
   ],
   "source": [
    "surv = survey.KanoSurvey()\n",
    "# Add airidas and elias answers\n",
    "air = surv.test_answers[\"airidas\"]\n",
    "eli = surv.test_answers[\"elias\"]\n",
    "df.insert(2, \"airidas\", air[:len(df)])\n",
    "df.insert(3, \"elias\", eli[:len(df)])\n",
    "\n",
    "df['answer'] = df['answer'].str.upper()\n",
    "df['airidas'] = df['airidas'].str.upper()\n",
    "df['elias'] = df['elias'].str.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proces simulation output - PERSONALITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default Personality Survey CSV file: surveys/survey_personality-test.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>airidas</th>\n",
       "      <th>elias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am the life of the party.</td>\n",
       "      <td>DISAGREE</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I don't talk a lot.</td>\n",
       "      <td>DISAGREE</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I feel comfortable around people.</td>\n",
       "      <td>SOMEWHAT AGREEE</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I keep in the background.</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I start conversations.</td>\n",
       "      <td>SOMEWHAT AGREEE</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            question           answer  airidas  elias\n",
       "0        I am the life of the party.         DISAGREE        3      3\n",
       "1                I don't talk a lot.         DISAGREE        2      2\n",
       "2  I feel comfortable around people.  SOMEWHAT AGREEE        4      4\n",
       "3          I keep in the background.          NEUTRAL        2      4\n",
       "4             I start conversations.  SOMEWHAT AGREEE        4      3"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surv = survey.PersonalitySurvey()\n",
    "# df = df.dropna()\n",
    "\n",
    "# Add airidas and elias answers\n",
    "air = surv.test_answers[\"airidas\"]\n",
    "eli = surv.test_answers[\"elias\"]\n",
    "df.insert(2, \"airidas\", air[:len(df)])\n",
    "df.insert(3, \"elias\", eli[:len(df)])\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: ' SOMEWHAT DISAGREE'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# compute one number of how the percentage of correct answers\u001b[39;00m\n\u001b[1;32m      2\u001b[0m result_data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExact Matches\u001b[39m\u001b[38;5;124m\"\u001b[39m: (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mairidas\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(df),\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCorrelation\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43manswer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mairidas\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExact Matches - elias\u001b[39m\u001b[38;5;124m\"\u001b[39m: (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melias\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(df),\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCorrelation - elias\u001b[39m\u001b[38;5;124m\"\u001b[39m: df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcorr(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melias\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m      7\u001b[0m }\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m result_data\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/pandas/core/series.py:2964\u001b[0m, in \u001b[0;36mSeries.corr\u001b[0;34m(self, other, method, min_periods)\u001b[0m\n\u001b[1;32m   2961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(this) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   2962\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan\n\u001b[0;32m-> 2964\u001b[0m this_values \u001b[38;5;241m=\u001b[39m \u001b[43mthis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2965\u001b[0m other_values \u001b[38;5;241m=\u001b[39m other\u001b[38;5;241m.\u001b[39mto_numpy(dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m, na_value\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mnan, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   2967\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpearson\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspearman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkendall\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(method):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/pandas/core/base.py:662\u001b[0m, in \u001b[0;36mIndexOpsMixin.to_numpy\u001b[0;34m(self, dtype, copy, na_value, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m         values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    660\u001b[0m     values[np\u001b[38;5;241m.\u001b[39masanyarray(isna(\u001b[38;5;28mself\u001b[39m))] \u001b[38;5;241m=\u001b[39m na_value\n\u001b[0;32m--> 662\u001b[0m result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(values, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (copy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fillna) \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write()):\n\u001b[1;32m    665\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mshares_memory(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[:\u001b[38;5;241m2\u001b[39m], result[:\u001b[38;5;241m2\u001b[39m]):\n\u001b[1;32m    666\u001b[0m         \u001b[38;5;66;03m# Take slices to improve performance of check\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: ' SOMEWHAT DISAGREE'"
     ]
    }
   ],
   "source": [
    "# compute one number of how the percentage of correct answers\n",
    "result_data = {\n",
    "    \"Exact Matches\": (df['answer'] == df['airidas']).sum() / len(df),\n",
    "    \"Correlation\": df['answer'].corr(df['airidas']),\n",
    "    \"Exact Matches - elias\": (df['answer'] == df['elias']).sum() / len(df),\n",
    "    \"Correlation - elias\": df['answer'].corr(df['elias']),\n",
    "}\n",
    "\n",
    "for k, v in result_data.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<class 'survey.PersonalitySurvey'>\""
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(type(surv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brikasutils.quickCSV: Saved 8 as simulations/elias_runs.csv\n"
     ]
    }
   ],
   "source": [
    "new_res = {\n",
    "    # \"label\": None,\n",
    "    \"SIMULATION_NAMEID\": SIMULATION_NAMEID,#SIM_ID,\n",
    "    \"timestamp\": bu.get_timestamp(),\n",
    "    \"survey_type\": str(type(surv)),\n",
    "    # \"temperature\": SETTINGS[\"temperature\"],\n",
    "    # \"note\": \"\",\n",
    "    \"exact_matches\": result_data[\"Exact Matches\"],\n",
    "    \"corr\": result_data[\"Correlation\"],\n",
    "    \"exact_matches_elias\": result_data[\"Exact Matches - elias\"],\n",
    "    \"corr_elias\": result_data[\"Correlation - elias\"],\n",
    "}\n",
    "\n",
    "tmp = bu.convert_dicts_to_table([new_res])\n",
    "res.append_data(tmp[1], tmp[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>airidas</th>\n",
       "      <th>elias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>I am always prepared.</td>\n",
       "      <td>SOMEWHAT AGREE</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>I leave my belongings around.</td>\n",
       "      <td>SOMEWHAT DISAGREE</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>I pay attention to details.</td>\n",
       "      <td>AGREE</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>I make a mess of things.</td>\n",
       "      <td>SOMEWHAT DISAGREE</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>I get chores done right away.</td>\n",
       "      <td>SOMEWHAT DISAGREE</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>I often forget to put things back in their pro...</td>\n",
       "      <td>SOMEWHAT DISAGREE</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>I like order.</td>\n",
       "      <td>SOMEWHAT AGREE</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>I shirk my duties.</td>\n",
       "      <td>DISAGREE</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>I follow a schedule.</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>I am exacting in my work.</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>I have a rich vocabulary.</td>\n",
       "      <td>SOMEWHAT AGREE</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>I have difficulty understanding abstract ideas.</td>\n",
       "      <td>SOMEWHAT DISAGREE</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>I have a vivid imagination.</td>\n",
       "      <td>AGREE</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>I am not interested in abstract ideas.</td>\n",
       "      <td>SOMEWHAT DISAGREE</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>I have excellent ideas.</td>\n",
       "      <td>SOMEWHAT AGREE</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>I do not have a good imagination.</td>\n",
       "      <td>SOMEWHAT DISAGREE</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>I am quick to understand things.</td>\n",
       "      <td>AGREE</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>I use difficult words.</td>\n",
       "      <td>SOMEWHAT DISAGREE</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>I spend time reflecting on things.</td>\n",
       "      <td>AGREE</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>I am full of ideas.</td>\n",
       "      <td>AGREE</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question             answer  \\\n",
       "30                              I am always prepared.     SOMEWHAT AGREE   \n",
       "31                      I leave my belongings around.  SOMEWHAT DISAGREE   \n",
       "32                        I pay attention to details.              AGREE   \n",
       "33                           I make a mess of things.  SOMEWHAT DISAGREE   \n",
       "34                      I get chores done right away.  SOMEWHAT DISAGREE   \n",
       "35  I often forget to put things back in their pro...  SOMEWHAT DISAGREE   \n",
       "36                                      I like order.     SOMEWHAT AGREE   \n",
       "37                                 I shirk my duties.           DISAGREE   \n",
       "38                               I follow a schedule.            NEUTRAL   \n",
       "39                          I am exacting in my work.            NEUTRAL   \n",
       "40                          I have a rich vocabulary.     SOMEWHAT AGREE   \n",
       "41    I have difficulty understanding abstract ideas.  SOMEWHAT DISAGREE   \n",
       "42                        I have a vivid imagination.              AGREE   \n",
       "43             I am not interested in abstract ideas.  SOMEWHAT DISAGREE   \n",
       "44                            I have excellent ideas.     SOMEWHAT AGREE   \n",
       "45                  I do not have a good imagination.  SOMEWHAT DISAGREE   \n",
       "46                   I am quick to understand things.              AGREE   \n",
       "47                             I use difficult words.  SOMEWHAT DISAGREE   \n",
       "48                 I spend time reflecting on things.              AGREE   \n",
       "49                                I am full of ideas.              AGREE   \n",
       "\n",
       "    airidas  elias  \n",
       "30        4      1  \n",
       "31        2      5  \n",
       "32        2      3  \n",
       "33        2      4  \n",
       "34        1      2  \n",
       "35        2      2  \n",
       "36        4      4  \n",
       "37        2      1  \n",
       "38        4      2  \n",
       "39        3      5  \n",
       "40        3      4  \n",
       "41        1      1  \n",
       "42        5      5  \n",
       "43        2      1  \n",
       "44        5      4  \n",
       "45        1      1  \n",
       "46        5      5  \n",
       "47        1      4  \n",
       "48        5      5  \n",
       "49        5      5  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove all characters from a black list from the column answer\n",
    "df['answer'] = df['answer'].apply(lambda x: x.strip())\n",
    "for substr in utils.BLACKLIST_ANSWER_SUBSTRINGS:\n",
    "    df['answer'] = df['answer'].apply(lambda x: re.sub(substr, \"\", x))\n",
    "df['answer'] = df['answer'].str.upper()\n",
    "\n",
    "df[30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all characters from a black list from the column answer\n",
    "df['answer'] = df['answer'].apply(lambda x: x.strip())\n",
    "\n",
    "for substr in utils.BLACKLIST_ANSWER_SUBSTRINGS:\n",
    "    df['answer'] = df['answer'].apply(lambda x: re.sub(substr, \"\", x))\n",
    "\n",
    "df['answer'] = df['answer'].str.upper()\n",
    "# Update isValid\n",
    "df['isValid'] = df['answer'].apply(lambda x: x in surv.POSSIBLE_ANSWERS)\n",
    "\n",
    "# if all values in isValid is true, drop the column, else print a message\n",
    "if df['isValid'].all():\n",
    "    df = df.drop('isValid', axis=1)\n",
    "    print(\"All answers were valid\")\n",
    "else:\n",
    "    print(\"Some answers were not valid\")\n",
    "\n",
    "df.head(n=10)\n",
    "\n",
    "########################\n",
    "\n",
    "# remove all characters from a black list from the column answer\n",
    "for substr in utils.BLACKLIST_ANSWER_SUBSTRINGS:\n",
    "    df['answer'] = df['answer'].apply(lambda x: re.sub(substr, \"\", x))\n",
    "\n",
    "df['answer'] = df['answer'].str.upper()\n",
    "# Update isValid\n",
    "df['isValid'] = df['answer'].apply(lambda x: x in surv.POSSIBLE_ANSWERS)\n",
    "\n",
    "# if all values in isValid is true, drop the column, else print a message\n",
    "if df['isValid'].all():\n",
    "    df = df.drop('isValid', axis=1)\n",
    "    print(\"All answers were valid\")\n",
    "else:\n",
    "    print(\"Some answers were not valid\")\n",
    "\n",
    "df\n",
    "#### Cleanup\n",
    "# remove all characters from a black list from the column answer\n",
    "# for substr in utils.BLACKLIST_ANSWER_SUBSTRINGS:\n",
    "#      df['answer'] = df['answer'].apply(lambda x: re.sub(substr, \"\", x))\n",
    "# # Update isValid\n",
    "#      df['isValid'] = df['answer'].apply(lambda x: x in surv.POSSIBLE_ANSWERS)\n",
    "\n",
    "# if all values in isValid is true, drop the column, else print a message\n",
    "# if df['isValid'].all():\n",
    "#     df = df.drop('isValid', axis=1)\n",
    "# else:\n",
    "#     print(\"Some answers were not valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['answer'] = df['answer'].str.upper()\n",
    "df['airidas'] = df['airidas'].str.upper()\n",
    "df['elias'] = df['elias'].str.upper()\n",
    "\n",
    "df['answer'] = df['answer'].map(remap_dict)\n",
    "df['airidas'] = df['airidas'].map(remap_dict)\n",
    "df['elias'] = df['elias'].map(remap_dict)\n",
    "\n",
    "# df['CLONE_eli'] = df['CLONE_eli'].map(remap_dict)\n",
    "\n",
    "# df = df.drop(columns=['uppercase_text'])\n",
    "# df['CLONE_eli'] = df['answer'].apply(extract_uppercase_text)\n",
    "# df['CLONE_eli'] = df['CLONE_eli'].str.upper()\n",
    "# .str.upper() or .lower()\n",
    "# df['answer'] = df['answer'].map(remap_dict, na_action='ignore')\n",
    "#df['CLONE_eli'] = df['CLONE_eli'].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(surv, survey.KanoSurvey):\n",
    "    remap_dict = {\"I EXPECT IT\": 5, \"I LIKE IT\": 4, \"I AM NEUTRAL\": 3, \"I CAN TOLERATE IT\": 2, \"I DISLIKE IT\": 1}\n",
    "    df['answer'] = df['answer'].map(remap_dict)\n",
    "    df['airidas'] = df['airidas'].map(remap_dict)\n",
    "    df['elias'] = df['elias'].map(remap_dict)\n",
    "elif isinstance(surv, survey.PersonalitySurvey):\n",
    "    remap_dict = {\"AGREE\": 5, \"SOMEWHAT AGREE\": 4, \"NEUTRAL\": 3, \"SOMEWHAT DISAGREE\": 2, \"DISAGREE\": 1}\n",
    "    df['answer'] = df['answer'].map(remap_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DF stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remaps - UNIVERSAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     pattern = r'\\b(?:' + '|'.join(re.escape(phrase) for phrase in phrases_to_extract) + r')\\b'\n",
    "#     matches = re.findall(pattern, text, flags=re.IGNORECASE) \n",
    "#     return ' '.join(matches) if matches else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute one number of how the percentage of correct answers\n",
    "print(f\"Exact Matches: {(df['CLONE_eli'] == df['IRL_eli']).sum() / len(df)}\")\n",
    "print(f\"Correlation: {df['CLONE_eli'].corr(df['IRL_eli'])}\")\n",
    "\n",
    "df['elias_correct'] = df['CLONE_eli'] == df['IRL_eli']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Batch Sim**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "import traceback\n",
    "import sys\n",
    "\n",
    "import brikasutils as bu\n",
    "import shared_utils as utils\n",
    "import survey\n",
    "importlib.reload(bu)\n",
    "importlib.reload(utils)\n",
    "importlib.reload(survey)\n",
    "\n",
    "queue = bu.FileRunQueue(queue_folder_path=\"batch/queue\", completed_folder_path=\"batch/done\")\n",
    "report_live_csv = bu.LiveCSV(\"batch/run_reports.csv\")\n",
    "timer = bu.Benchmarker()\n",
    "\n",
    "\n",
    "for filepath in queue:\n",
    "    timer.mark_start(filepath)\n",
    "\n",
    "    try: \n",
    "        ########## Handle batch stuff ########\n",
    "        filename = os.path.splitext(os.path.basename(filepath))[0]\n",
    "        with open(filepath, 'r') as f:\n",
    "            rundata = json.load(f)\n",
    "\n",
    "        # Load prompt file\n",
    "        with open(rundata[\"instructions\"][\"prompt_file\"], 'r') as f:\n",
    "            final_prompts = json.load(f)\n",
    "\n",
    "        # Make the surv\n",
    "        if rundata[\"instructions\"][\"survey_type\"] == \"KanoSurvey\":\n",
    "            surv = survey.KanoSurvey()\n",
    "        elif rundata[\"instructions\"][\"survey_type\"] == \"PersonalitySurvey\":\n",
    "            surv = survey.PersonalitySurvey()\n",
    "        else:\n",
    "            raise Exception(\"Invalid survey type\")\n",
    "\n",
    "        timestamp = bu.get_timestamp()\n",
    "        ######### Run Simulation ########\n",
    "        SIMULATION_NAMEID = filename\n",
    "        LIMIT = rundata[\"instructions\"][\"LIMIT\"] if \"LIMIT\" in rundata[\"instructions\"] else None\n",
    "        AUTO_INFO = {\n",
    "            \"date\": timestamp,\n",
    "            **rundata[\"info\"], # unpacked from rundata\n",
    "            \"limit\": LIMIT,\n",
    "            \"prompt_count\": min(len(final_prompts), LIMIT) if LIMIT != None else len(final_prompts),\n",
    "            \"avg_tokens_in_prompt\": round(utils.describe_prompts(final_prompts)[\"total_all_prompt_tokens\"]/len(final_prompts)),\n",
    "        }\n",
    "        SETTINGS = {\n",
    "            **rundata[\"settings\"], # unpacked from rundata\n",
    "        }\n",
    "\n",
    "        # client depends on if it's local or not\n",
    "        if rundata[\"instructions\"][\"isLocal\"]:\n",
    "            client = OpenAI(\n",
    "                base_url = 'http://localhost:11434/v1',\n",
    "                api_key='ollama', # required, but unused\n",
    "            )\n",
    "        else:\n",
    "            client = OpenAI(\n",
    "                api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "            )\n",
    "\n",
    "        completions = []\n",
    "        l = len(final_prompts)\n",
    "\n",
    "        for i, (prompt, question) in enumerate(list(zip(final_prompts, surv.questions))):\n",
    "            if LIMIT != None and i > LIMIT:\n",
    "                break\n",
    "\n",
    "            print(f\"{i}/{l}...\", end=\"\\t\") # Print progress\n",
    "            # Send the Request\n",
    "            full_response = client.chat.completions.create(\n",
    "                messages=prompt,\n",
    "                **SETTINGS,\n",
    "            )\n",
    "            r = full_response.choices[0].message.content\n",
    "\n",
    "            completions.append({'question': question, 'answer': r})\n",
    "\n",
    "            print(f\"{question}: {r}\")\n",
    "            \n",
    "        ############ Save Important results\n",
    "        df = pd.DataFrame(completions)\n",
    "        df.to_csv(f\"batch/output/{SIMULATION_NAMEID}_simulation.csv\", index=False)\n",
    "        bu.if_dir_not_exist_make(\"batch/output/info\")\n",
    "        bu.quickJSON({\"settings\": SETTINGS, \"info\": AUTO_INFO}, f\"batch/output/info/{SIMULATION_NAMEID}_info.json\")\n",
    "\n",
    "        status = \"OK\"\n",
    "    \n",
    "    except Exception:\n",
    "        print(f\"##### Error while running {filename}.\")\n",
    "        error_string = traceback.format_exc()\n",
    "        print(error_string)\n",
    "        status = \"Failed\"\n",
    "\n",
    "    ########### Time the run\n",
    "    try:\n",
    "        time_taken = timer.mark_end(filepath)\n",
    "    except:\n",
    "        print(\"Error while timing run: \")\n",
    "        print(traceback.format_exc())\n",
    "        time_taken = None\n",
    "\n",
    "    ########### Report the run\n",
    "    try:\n",
    "        new_report = {\n",
    "            \"filename\": filename,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"time_taken\": time_taken,\n",
    "            \"status\": status,\n",
    "            **rundata[\"instructions\"],\n",
    "            \"error\": error_string if status == \"Failed\" else \"\",\n",
    "        }\n",
    "\n",
    "        tmp = bu.convert_dicts_to_table([new_report])\n",
    "        report_live_csv.append_data(tmp[1], tmp[0])\n",
    "    except Exception as e:\n",
    "        print(f\"Error while reporting: \")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    print(f\"Processed {filename}. Stauts: {status}\")\n",
    "\n",
    "timer.print_total_execution_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Force short JSON answer\n",
    "\n",
    "Add this to the end of your prompt:\n",
    "> ```json\n",
    "\n",
    "Add this to the \"stop\" sequence:\n",
    ">```\n",
    "\n",
    "The idea is to force the model to continue writing json markdown. And end the generation when it outputs \"```\" which ends the json markdown section.\n",
    "\n",
    "----\n",
    "\n",
    "Command-r-plus\n",
    "TEMPLATE \"\"\"{{ if .System }}<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>{{ .System }}<|END_OF_TURN_TOKEN|>{{ end }}{{ if .Prompt }}<|START_OF_TURN_TOKEN|><|USER_TOKEN|>{{ .Prompt }}<|END_OF_TURN_TOKEN|>{{ end }}<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>{{ .Response }}<|END_OF_TURN_TOKEN|>\"\"\"\n",
    "PARAMETER stop \"<|START_OF_TURN_TOKEN|>\"\n",
    "PARAMETER stop \"<|END_OF_TURN_TOKEN|>\"\n",
    "\n",
    "\n",
    "Mixtral\n",
    "TEMPLATE \"\"\" [INST] {{ .System }} {{ .Prompt }} [/INST]\"\"\"\n",
    "PARAMETER stop \"[INST]\"\n",
    "PARAMETER stop \"[/INST]\"\n",
    "\n",
    "\n",
    "\n",
    "TEMPLATE \"\"\" [INST] {{ .System }} {{ .Prompt }} ```json [/INST] \"\"\"\n",
    "PARAMETER stop \"[INST]\"\n",
    "PARAMETER stop \"[/INST]\"\n",
    "PARAMETER stop \"```\"\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "\n",
    "mixtral x22\n",
    "TEMPLATE \"\"\"[INST] {{ if .System }}{{ .System }} {{ end }}{{ .Prompt }} [/INST]\"\"\"\n",
    "PARAMETER stop \"[INST]\"\n",
    "PARAMETER stop \"[/INST]\"\n",
    "\n",
    "wizard x22\n",
    "TEMPLATE \"\"\"{{ if .System }}{{ .System }} {{ end }}{{ if .Prompt }}USER: {{ .Prompt }} {{ end }}ASSISTANT: {{ .Response }}\"\"\"\n",
    "SYSTEM \"\"\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\"\"\"\n",
    "PARAMETER stop \"USER:\"\n",
    "PARAMETER stop \"ASSISTANT:\"\n",
    "\n",
    "nomic_embed\n",
    "TEMPLATE \"\"\"{{ .Prompt }}\"\"\"\n",
    "PARAMETER num_ctx 8192\n",
    "\n",
    "\n",
    "Mistral 7b\n",
    "TEMPLATE \"\"\"[INST] {{ .System }} {{ .Prompt }} [/INST]\"\"\"\n",
    "PARAMETER stop \"[INST]\"\n",
    "PARAMETER stop \"[/INST]\"\n",
    "\n",
    "Mistral 7b-wizard\n",
    "TEMPLATE \"\"\"{{ if .System }}{{ .System }} {{ end }}{{ if .Prompt }}USER: {{ .Prompt }} {{ end }}ASSISTANT: {{ .Response }}\"\"\"\n",
    "SYSTEM \"\"\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\"\"\"\n",
    "PARAMETER stop \"USER:\"\n",
    "PARAMETER stop \"ASSISTANT:\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
