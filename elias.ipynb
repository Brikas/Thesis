{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import brikasutils as bu\n",
    "importlib.reload(bu)\n",
    "import fb_msg_reader as fb\n",
    "importlib.reload(fb)\n",
    "import shared_utils as utils\n",
    "from shared_utils import systemMsg, userMsg, assistantMsg\n",
    "importlib.reload(utils)\n",
    "import survey\n",
    "importlib.reload(survey)\n",
    "import persona\n",
    "importlib.reload(persona)\n",
    "\n",
    "import ollama\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "from typing import List\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 1916 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-09-13 to 2024-03-06\n",
      "Messages saved to self.chats['airidas']\n",
      "Read 618 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-09-10 to 2024-03-03\n",
      "Messages saved to self.chats['christian']\n",
      "Read 297 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2018-07-25 to 2024-01-01\n",
      "Messages saved to self.chats['nikolay']\n",
      "Read 144 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-03-28 to 2021-12-30\n",
      "Messages saved to self.chats['mathis']\n",
      "Read 104 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-08-25 to 2024-03-05\n",
      "Messages saved to self.chats['jacob']\n",
      "Read 159 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-10-12 to 2023-04-30\n",
      "Messages saved to self.chats['chris']\n",
      "Read 161 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-03-28 to 2021-06-06\n",
      "Messages saved to self.chats['aziz']\n",
      "Read 350 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-03-26 to 2022-04-11\n",
      "Messages saved to self.chats['daniela']\n",
      "Read 105 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-03-28 to 2022-07-10\n",
      "Messages saved to self.chats['mihi']\n",
      "Read 117 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-05-17 to 2022-04-11\n",
      "Messages saved to self.chats['viktoria']\n",
      "Read 172 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2019-08-11 to 2023-12-21\n",
      "Messages saved to self.chats['diba']\n",
      "Read 154 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-12-02 to 2023-04-29\n",
      "Messages saved to self.chats['filip']\n",
      "Filtering\n",
      "link-filter: 109\n",
      "react-filter: 0\n",
      "cookie-data-filter: 12\n",
      "Selected chat airidas for 9995 (668 messages)\n",
      "Selected chat christian for 5252 (576 messages)\n",
      "Selected chat nikolay for 3128 (266 messages)\n",
      "Selected chat mathis for 1325 (134 messages)\n",
      "Selected chat daniela for 2881 (293 messages)\n",
      "Selected chat diba for 1799 (156 messages)\n",
      "Selected chat aziz for 1219 (136 messages)\n",
      "Selected chat jacob for 1124 (93 messages)\n",
      "Selected chat chris for 1535 (151 messages)\n",
      "Selected chat filip for 1306 (146 messages)\n",
      "Selected chat mihi for 1098 (102 messages)\n",
      "Selected chat viktoria for 1168 (107 messages)\n",
      "Combined tokens: 31830\n"
     ]
    }
   ],
   "source": [
    "et = persona.PersonaEncoder()\n",
    "\n",
    "# ==== FB messages ====\n",
    "et.parse_fb_messages([\"data-raw/1_airidas.json\"], \"airidas\")\n",
    "et.parse_fb_messages([\"data-raw/2_christian.json\"], \"christian\")\n",
    "et.parse_fb_messages([\"data-raw/1_nikolay.json\"], \"nikolay\")\n",
    "et.parse_fb_messages([\"data-raw/2_mathis.json\"], \"mathis\")\n",
    "et.parse_fb_messages([\"data-raw/2_jacob.json\"], \"jacob\")\n",
    "et.parse_fb_messages([\"data-raw/2_chris.json\"], \"chris\")\n",
    "et.parse_fb_messages([\"data-raw/3_aziz.json\"], \"aziz\")\n",
    "et.parse_fb_messages([\"data-raw/3_daniela.json\"], \"daniela\")\n",
    "et.parse_fb_messages([\"data-raw/3_mihi.json\"], \"mihi\")\n",
    "et.parse_fb_messages([\"data-raw/3_viktoria.json\"], \"viktoria\")\n",
    "et.parse_fb_messages([\"data-raw/4_diba.json\"], \"diba\")\n",
    "et.parse_fb_messages([\"data-raw/6_filip.json\"], \"filip\")\n",
    "#et.parse_wa_messages(texts_with_rebecca, \"rebecca\")\n",
    "# texts_with_others_dict = {\n",
    "#     \"rebecca\": [\"data-raw/messages_1000.json\"],\n",
    "# }\n",
    "# for name, texts in texts_with_others_dict.items():\n",
    "#     et.parse_fb_messages(texts, name)\n",
    "\n",
    "# Regex cleaning\n",
    "et.filter_chats_empty()\n",
    "et.filter_chats_regex(utils.BLACKLIST_CHAT_REGEX_FILTERS)\n",
    "\n",
    "# Compress names\n",
    "for nameid, chat in et.chats.items():\n",
    "    for msg in chat:  \n",
    "        msg.sender = \"Persona\" if msg.sender == \"Elias Salvador Smidt Torjani\"  else \"Friend\"\n",
    "\n",
    "# Start all chats from 2/3rds\n",
    "# for name, chat in et.chats.items():\n",
    "#     et.chats[name] = chat[int(len(chat)/3 * 2):]\n",
    "# Select the final modules\n",
    "et.select_chat_limited_by_tokens(\"airidas\", 10000)\n",
    "et.select_chat_limited_by_tokens(\"christian\", 10000)\n",
    "et.select_chat_full(\"nikolay\")\n",
    "et.select_chat_full(\"mathis\") \n",
    "et.select_chat_full(\"daniela\")\n",
    "et.select_chat_full(\"diba\")\n",
    "et.select_chat_full(\"aziz\")\n",
    "et.select_chat_full(\"jacob\")  \n",
    "et.select_chat_full(\"chris\")\n",
    "et.select_chat_full(\"filip\")\n",
    "et.select_chat_full(\"mihi\")\n",
    "et.select_chat_full(\"viktoria\")\n",
    "# for name in texts_with_others_dict.keys():\n",
    "#     ab.select_chat_full(name)\n",
    "\n",
    "# save\n",
    "big_module=et.output()\n",
    "bu.quickTXT(big_module, filename=f\"data/big_module_{bu.get_timestamp()}\")\n",
    "\n",
    "# stats\n",
    "token_counts = et.count_all_selected_chat_tokens() # token_counts used later for statistics\n",
    "print(f\"Combined tokens: {sum(token_counts.values())}\")\n",
    "# utils.count_tokens(big_module) \n",
    "# or list(et.selectedChats.keys()) --> et.count_chat_tokens(\"{friend}\")\n",
    "# et.selectedChats[\"{friend}\"][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default Personality Survey CSV file: surveys/survey_personality-test.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    I am the life of the party.\n",
       "1            I don't talk a lot.\n",
       "Name: question, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surv = survey.PersonalitySurvey()\n",
    "# surv = survey.KanoSurvey()\n",
    "# surv = survey.buildFairnessPrompts()\n",
    "# surv = survey.DictatorGameSurvey()\n",
    "surv.questions[:2]#.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "EMBED_MODEL = \"nomic-embed-text\"        # nomic-embed-text = long ctx / mxbai-embed-large = big\n",
    "CHUNK_SIZE = 30                         # Number of messages per chunk\n",
    "OVERLAP_SIZE = 10                       # Number of overlapping messages between consecutive chunks\n",
    "# COMMENT 04-16, perhaps we could try 5x retrievals with isolated semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk count: 130\n",
      "Average chunk character length: 1244\n",
      "Rough estimate of tokens per chunk: 311 (4 characters per token)\n",
      "Messagees in input count: 2828\n",
      "Messages in chunks count: 3900\n",
      "Chunk \\ Input ratio: 1.38 (OVERLAP_SIZE=10)\n",
      "Chunk Python type: <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists for storing chunks â€“ and embeddings later\n",
    "# different chunk size\n",
    "chunks = []\n",
    "stat_total_msgs_in_chunks = 0 # for statistics\n",
    "\n",
    "# different chunk size\n",
    "# Iterate over chats and messages to create chunks\n",
    "for chat in et.selectedChats.values():\n",
    "    messages = list(chat)  # Convert chat iterator to list for easier slicing\n",
    "    num_messages = len(messages)\n",
    "\n",
    "    # Create overlapping chunks of messages\n",
    "    for i in range(0, num_messages - CHUNK_SIZE + 1, CHUNK_SIZE - OVERLAP_SIZE):\n",
    "        chunk = messages[i:i + CHUNK_SIZE]  # Extract chunk of messages\n",
    "        chunk_text = \"\\n\".join(str(msg) for msg in chunk)  # Concatenate messages into a single string\n",
    "        chunks.append(chunk_text)  # Append chunk to list of chunks\n",
    "\n",
    "        stat_total_msgs_in_chunks += len(chunk) # For statistics\n",
    "\n",
    "##### Display Info\n",
    "total_messages = sum(len(chat) for chat in et.selectedChats.values())\n",
    "chunks_count = len(chunks)\n",
    "avg_chunk_char_len = np.mean([len(chunk) for chunk in chunks])\n",
    "\n",
    "print(\n",
    "    f\"Chunk count: {chunks_count}\",\n",
    "    f\"Average chunk character length: {round( avg_chunk_char_len)}\",\n",
    "    f\"Rough estimate of tokens per chunk: {round(avg_chunk_char_len / 4)} (4 characters per token)\",\n",
    "    f\"Messagees in input count: {total_messages}\",\n",
    "    f\"Messages in chunks count: {stat_total_msgs_in_chunks}\",\n",
    "    f\"Chunk \\ Input ratio: {round(stat_total_msgs_in_chunks / total_messages,2)} (OVERLAP_SIZE={OVERLAP_SIZE})\",\n",
    "    f\"Chunk Python type: {type(chunks[0])}\",\n",
    "    sep=\"\\n\"\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generaterating embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Serialization ########\n",
    "EMBEDDING_NAMEID = \"mixtral\"\n",
    "AUTO_INFO = {\n",
    "    \"model\": EMBED_MODEL,\n",
    "    \"CHUNK_SIZE\": CHUNK_SIZE,\n",
    "    \"OVERLAP_SIZE\": OVERLAP_SIZE,\n",
    "    \"chunks_count\": chunks_count,\n",
    "    \"total_messages\": total_messages,\n",
    "    \"stat_total_msgs_in_chunks\": stat_total_msgs_in_chunks,\n",
    "    \"modules_chat\": token_counts,\n",
    "}\n",
    "##################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 130/130"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for each chunk\n",
    "embeddings = []\n",
    "\n",
    "progress, chunks_len = 0, len(chunks) # for progress bar\n",
    "for chunk_text in chunks:\n",
    "    progress += 1\n",
    "    print(f\"\\rChunk {progress}/{chunks_len}\", end=\"\")\n",
    "\n",
    "    embedding = ollama.embeddings(model=EMBED_MODEL, prompt=chunk_text)[\"embedding\"]\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "####################################################\n",
    "# Generate embeddings for each chunk\n",
    "# for chunk_text in chunks:\n",
    "#     embedding = ollama.embeddings(model=EMBED_MODEL, prompt=chunk_text)[\"embedding\"]\n",
    "#     embeddings.append(embedding)\n",
    "\n",
    "\n",
    "# token counts in all similar chunks\n",
    "# tokens_in_chunks = 0\n",
    "# for chunk in chunks_most_similar:\n",
    "#     tokens_in_chunks += utils.count_tokens(chunk)\n",
    "# print(f\"Tokens in chunks: {tokens_in_chunks}\")\n",
    "\n",
    "bu.if_dir_not_exist_make(\"embeddings\")\n",
    "bu.quickJSON(AUTO_INFO, f\"embeddings/{EMBEDDING_NAMEID}_info.json\")\n",
    "bu.quickJSON({\"chunks\": chunks, \"embeddings\": embeddings}, f\"embeddings/{EMBEDDING_NAMEID}_embeddings.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = \"elias\"\n",
    "\n",
    "# persona_small = \"{small module}\"\n",
    "# persona_med = \"{med module}\"\n",
    "# persona_text = \"Favorite video games are Rimworld, Minecraft, Age of Empires, 7 Days to Die\"\n",
    "\n",
    "# Change below accoring to survey above\n",
    "RETRIEVAL_PROMPT = \"openess conciousness extrovert aggreableness neuroticism\" #\"personality\"\n",
    "# RETRIEVAL_PROMPT = \"video game features\"\n",
    "# q_retrival_prompt =\n",
    "# SURVEY_PROMPT = \"Determine how much {subject} aggree with the statement. Guestimate how {subject} would answer to the question\"\n",
    "\n",
    "CHUNKS_COUNT_IN_CTX = 15 # Number of nearby chunks to put in context window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens in chunks: 5797\n",
      "Chunks:130, embeds:130\n"
     ]
    }
   ],
   "source": [
    "prompt_embedding = ollama.embeddings(model=EMBED_MODEL, prompt=RETRIEVAL_PROMPT)[\"embedding\"]\n",
    "chunks_most_similar_embeddings  = utils.find_most_similar(prompt_embedding, embeddings)[:CHUNKS_COUNT_IN_CTX]\n",
    "chunks_most_similar = []\n",
    "for embedding in chunks_most_similar_embeddings:\n",
    "    chunks_most_similar.append(chunks[embedding[1]])\n",
    "\n",
    "# Display results\n",
    "# bu.quickTXT(\"\\n\\n\".join(chunks_most_similar), filename=\"ignorefolder/chunks.txt\")\n",
    "\n",
    "# token counts in all similar chunks\n",
    "tokens_in_chunks = 0\n",
    "for chunk in chunks_most_similar:\n",
    "    tokens_in_chunks += utils.count_tokens(chunk)\n",
    "print(f\"Tokens in chunks: {tokens_in_chunks}\")\n",
    "####################################################\n",
    "print(f\"Chunks:{len(chunks)}, embeds:{len(embeddings)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_retrieval_prompts = list(surv.questions)\n",
    "\n",
    "CHUNKS_COUNT_IN_CTX = 5 # Number of nearby chunks to put in context window\n",
    "dynamic_chunks_most_similar: List[List[str]] = [] \n",
    "\n",
    "progress = 0\n",
    "lenn = len(dynamic_retrieval_prompts)\n",
    "for prompt in dynamic_retrieval_prompts:\n",
    "    progress += 1\n",
    "    print(f\"\\rPrompt {progress}/{lenn}\", end=\"\")\n",
    "\n",
    "    prompt_embedding = ollama.embeddings(model=EMBED_MODEL, prompt=prompt)[\"embedding\"]\n",
    "    chunks_most_similar_embeddings = utils.find_most_similar(prompt_embedding, embeddings)[:CHUNKS_COUNT_IN_CTX]\n",
    "    chunks_most_similar = []\n",
    "    for embedding in chunks_most_similar_embeddings:\n",
    "        chunks_most_similar.append(chunks[embedding[1]])\n",
    "\n",
    "    dynamic_chunks_most_similar.append(chunks_most_similar)\n",
    "print(end=\"\\n\")\n",
    "    \n",
    "# VANITY PRINT\n",
    "tokens_in_chunks = 0\n",
    "for chunks_most_similar in dynamic_chunks_most_similar:\n",
    "    for chunk in chunks_most_similar:\n",
    "        tokens_in_chunks += utils.count_tokens(chunk)\n",
    "\n",
    "del chunks_most_similar_embeddings # free memory\n",
    "print(f\"Tokens in average chunk group: {tokens_in_chunks/len(dynamic_chunks_most_similar)}\")\n",
    "bu.quickJSON(dynamic_chunks_most_similar, filename=f\"ignorefolder/dynamic-chunks.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanity preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_text = \"\"\n",
    "PREVIEW_LIMIT = 10\n",
    "\n",
    "for i, chunks_most_similar in enumerate(dynamic_chunks_most_similar):\n",
    "    preview_text += f\"==============Prompt: {dynamic_retrieval_prompts[i]}==============\\n\"\n",
    "    for j, chunk in enumerate(chunks_most_similar):\n",
    "        if j >= PREVIEW_LIMIT:\n",
    "            break\n",
    "        preview_text += f\"=======CHUNK {j}=======\\n{chunk}\\n\\n\"\n",
    "    preview_text += \"\\n\\n\"\n",
    "bu.quickTXT(preview_text, filename=f\"ignorefolder/dynamic-chunks_preview.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Builder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With persona (dynamic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompts = []\n",
    "\n",
    "for question, chunks_most_similar in zip(surv.questions, dynamic_chunks_most_similar):\n",
    "    p = [\n",
    "        systemMsg(\n",
    "            \"You are specialized in impersonating people. You will be presented to the subject through explicit datapoints of their digital footprint. In addition, you will deduct their implicit tastes by shadowing chats between the subject and friends. You will be asked to answer questions from the point of view of the persona. Text below:\",\n",
    "            \"Conversations between persona and friends\",\n",
    "            \"\\nNEW CONVERSATION:\\n\".join(chunks_most_similar)\n",
    "        ),\n",
    "        # Understanding affirmation\n",
    "        assistantMsg('I will answer from the point of view of the persona, based on what I could the deduct from the text provided.'),\n",
    "        # Survey question. With Simulation\n",
    "        userMsg(\"\\n\".join([\n",
    "            f\"Persona is surveyed about their video game survey. The persona must choose answer the question below with one of the given options: {', '.join(surv.POSSIBLE_ANSWERS)}. Persona's answer must only contain the chosen option, without any elaboration, nor introduction. \",\n",
    "            question,\n",
    "            \"Persona chooses: \"\n",
    "        ])),\n",
    "        # assistantMsg(\"\\n\".join([f\"response: \"\n",
    "        # ])),\n",
    "    ]\n",
    "    final_prompts.append(p)\n",
    "\n",
    "prompt_info = utils.describe_prompts_and_print(final_prompts)\n",
    "bu.quickJSON(final_prompts, \"ignorefolder/prompts.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = {\n",
    "    \"role\": \"system\", \n",
    "    \"content\": \"You are an actor specializing in impersonating non-famouns people. You will be presented to the subject through explicit datapoints of their digital footprint. In addition, you will deduct their implicit personality traits by shadowing chats between the subject and friends. You will be asked to answer questions from the point of view of the persona. The persona you will be impersonating is named Elias. Context:\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "Created 50 prompts.\n",
      "Average prompt size: 6100 tokens.\n",
      "Min prompt size: 6097, Max prompt size: 6105\n"
     ]
    }
   ],
   "source": [
    "final_prompts = []\n",
    "\n",
    "for question in surv.questions:\n",
    "    p = [\n",
    "        systemMsg(PROMPT['content']+\"\\n## chat conversions between subject and friends\\n\".join(chunks_most_similar)),\n",
    "        assistantMsg('Understood. I will answer from the point of view of the persona, {subject}, based on what I could the deduct from the text provided above.'),\n",
    "        userMsg(\"\\n\".join([\n",
    "            f'\\n\\n**Your answer should only contain the chosen option without further explanation!** Reply to the statement below - how {subject} would reply - with one of these five options: {\", \".join(surv.POSSIBLE_ANSWERS)}.',\n",
    "            question,\n",
    "            \"The persona chooses: \"\n",
    "        ])),\n",
    "    ]\n",
    "    final_prompts.append(p)\n",
    "\n",
    "print(f\"{len(final_prompts)}\")#,{final_prompts[:1]}\")\n",
    "prompt_info = utils.describe_prompts_and_print(final_prompts) # Vanity print\n",
    "bu.quickJSON(final_prompts, \"ignorefolder/prompts.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base (no persona)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompts = []\n",
    "\n",
    "for question in surv.questions:\n",
    "    p = [\n",
    "        systemMsg(\n",
    "            \"You are participating in a survey. You will be presented with a series of questions about your video game preferrences.\",\n",
    "            f\"You must choose answer to the question below with one of the five options: {', '.join(surv.POSSIBLE_ANSWERS)}. The answer must only contain the chosen option. \"\n",
    "        ),\n",
    "        # Understanding affirmation\n",
    "        assistantMsg('Understood. I will answer the question below with one of the given options.'),\n",
    "        # Survey question. With Simulation\n",
    "        userMsg(\n",
    "            question,\n",
    "            \"Your choice: \"\n",
    "        ),\n",
    "    ]\n",
    "    final_prompts.append(p)\n",
    "\n",
    "prompt_info = utils.describe_prompts_and_print(final_prompts) # Vanity print\n",
    "bu.quickJSON(final_prompts, \"ignorefolder/prompts.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json file to dict\n",
    "# with open(\"simulations/toElias/run2-airidas-personality_cv1_prompts.json\", \"r\") as read_file:\n",
    "# with open(\"simulations/toElias/run2-airidas-video-game-cv1_prompts.json\", \"r\") as read_file:\n",
    "# with open(\"simulations/toElias/run2-base-personality-cv1_prompts.json\", \"r\") as read_file:\n",
    "with open(\"simulations/toElias/run2-base-video-game-cv1_prompts.json\", \"r\") as read_file:\n",
    "    pre_final_prompts = json.load(read_file)\n",
    "\n",
    "\n",
    "# pre_final_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Simulation\n",
    "SETTINGS = {\n",
    "     \"model\": \"command-r-plus:104b-q2_K\", # mixtral, command-r-plus:104b-q2_K\n",
    "     # \"temperature\": 0.5,\n",
    "     # best wizard and mixtral try mixtral-8x22b wizard in uCloud\n",
    "}\n",
    "\n",
    "##################################\n",
    "SIM_ID = f\"run2-base-video-game_rplus_cv2\"\n",
    "LIMIT = None # For testing purposes. Set to NONE to run all\n",
    "AUTO_INFO = {\n",
    "    \"date\": bu.get_timestamp(),\n",
    "    # \"EMBEDDING_NAMEID\": EMBEDDING_NAMEID,\n",
    "    # \"RETRIEVAL_PROMPT\": RETRIEVAL_PROMPT,\n",
    "    # \"CHUNKS_COUNT_IN_CTX\": CHUNKS_COUNT_IN_CTX,\n",
    "    # \"survey_type\": str(type(surv)),\n",
    "    # \"prompt_count\": min(len(final_prompts), LIMIT) if LIMIT != None else len(final_prompts),\n",
    "    # \"avg_tokens_in_prompt\": round(prompt_info[\"total_all_prompt_tokens\"]/len(final_prompts)),\n",
    "}\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama', # required, but unused\n",
    ")\n",
    "\n",
    "save = f\"{SETTINGS['model']}_{SIM_ID}\"\n",
    "##################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/50...\tI am the life of the party.: SOMEWHAT DISAGREE\n",
      "process_1 took 19.532s\n",
      "1/50...\tI don't talk a lot.: SOMEWHAT AGREE\n",
      "2/50...\tI feel comfortable around people.: SOMEWHAT DISAGREE\n",
      "process_2 took 1.142s\n",
      "3/50...\tI keep in the background.: SOMEWHAT AGREE\n",
      "4/50...\tI start conversations.: SOMEWHAT AGREE\n",
      "process_3 took 0.891s\n",
      "5/50...\tI have little to say.: SOMEWHAT DISAGREE\n",
      "6/50...\tI talk to a lot of different people at parties.: SOMEWHAT AGREE\n",
      "process_4 took 0.952s\n",
      "7/50...\tI don't like to draw attention to myself.: SOMEWHAT DISAGREE\n",
      "8/50...\tI don't mind being the center of attention.: SOMEWHAT DISAGREE\n",
      "process_5 took 1.056s\n",
      "9/50...\tI am quiet around strangers.: SOMEWHAT AGREE\n",
      "10/50...\tI get stressed out easily.: SOMEWHAT DISAGREE\n",
      "process_6 took 0.999s\n",
      "11/50...\tI am relaxed most of the time.: SOMEWHAT DISAGREE\n",
      "12/50...\tI worry about things.: AGREE\n",
      "process_7 took 0.651s\n",
      "13/50...\tI seldom feel blue.: SOMEWHAT DISAGREE\n",
      "14/50...\tI am easily disturbed.: SOMEWHAT AGREED\n",
      "process_8 took 0.877s\n",
      "15/50...\tI get upset easily.: SOMEWHAT DISAGREE\n",
      "16/50...\tI change my mood a lot.: SOMEWHAT DISAGREE\n",
      "process_9 took 1.031s\n",
      "17/50...\tI have frequent mood swings.: SOMEWHAT DISAGREE\n",
      "18/50...\tI get irritated easily.: SOMEWHAT DISAGREE\n",
      "process_10 took 1.037s\n",
      "19/50...\tI often feel blue.: SOMEWHAT AGREE\n",
      "20/50...\tI feel little concern for others.: SOMEWHAT AGREE\n",
      "process_11 took 0.93s\n",
      "21/50...\tI am interested in people.: SOMEWHAT AGREE\n",
      "22/50...\tI insult people.: SOMEWHAT DISAGREE\n",
      "process_12 took 0.98s\n",
      "23/50...\tI sympathize with others' feelings.: SOMEWHAT AGREED\n",
      "24/50...\tI am not interested in other people's problems.: SOMEWHAT DISAGREE\n",
      "process_13 took 1.009s\n",
      "25/50...\tI have a soft heart.: SOMEWHAT AGREE\n",
      "26/50...\tI am not really interested in others.: SOMEWHAT DISAGREE\n",
      "process_14 took 0.983s\n",
      "27/50...\tI take time out for others.: SOMEWHAT DISAGREE\n",
      "28/50...\tI feel others' emotions.: SOMEWHAT AGREE\n",
      "process_15 took 0.94s\n",
      "29/50...\tI make people feel at ease.: SOMEWHAT DISAGREE\n",
      "30/50...\tI am always prepared.: SOMEWHAT DISAGREE\n",
      "process_16 took 0.977s\n",
      "31/50...\tI leave my belongings around.: SOMEWHAT AGREES\n",
      "32/50...\tI pay attention to details.: SOMEWHAT DISAGREE\n",
      "process_17 took 1.038s\n",
      "33/50...\tI make a mess of things.: SOMEWHAT DISAGREE\n",
      "34/50...\tI get chores done right away.: SOMEWHAT DISAGREE\n",
      "process_18 took 1.097s\n",
      "35/50...\tI often forget to put things back in their proper place.: SOMEWHAT AGREE\n",
      "36/50...\tI like order.: SOMEWHAT DISAGREE\n",
      "process_19 took 1.068s\n",
      "37/50...\tI shirk my duties.: SOMEWHAT DISAGREE\n",
      "38/50...\tI follow a schedule.: SOMEWHAT DISAGREE\n",
      "process_20 took 1.001s\n",
      "39/50...\tI am exacting in my work.: SOMEWHAT DISAGREE\n",
      "40/50...\tI have a rich vocabulary.: SOMEWHAT AGREE\n",
      "process_21 took 0.889s\n",
      "41/50...\tI have difficulty understanding abstract ideas.: SOMEWHAT DISAGREE\n",
      "42/50...\tI have a vivid imagination.: SOMEWHAT DISAGREE\n",
      "process_22 took 0.959s\n",
      "43/50...\tI am not interested in abstract ideas.: SOMEWHAT DISAGREE\n",
      "44/50...\tI have excellent ideas.: SOMEWHAT DISAGREE\n",
      "process_23 took 0.965s\n",
      "45/50...\tI do not have a good imagination.: SOMEWHAT DISAGREE\n",
      "46/50...\tI am quick to understand things.: SOMEWHAT DISAGREE\n",
      "process_24 took 0.954s\n",
      "47/50...\tI use difficult words.: SOMEWHAT DISAGREE\n",
      "48/50...\tI spend time reflecting on things.: SOMEWHAT AGREE\n",
      "process_25 took 0.884s\n",
      "49/50...\tI am full of ideas.: SOMEWHAT AGREE\n"
     ]
    }
   ],
   "source": [
    "### ==== THE FUNCTIONAL 1!!!! =====\n",
    "SETTINGS = {\n",
    "     \"model\": \"llama3\", # mixtral, command-r-plus:104b-q2_K\n",
    "}\n",
    "##########################################\n",
    "SIM_ID = f\"run2-base-video-game_rplus_cv2\"\n",
    "LIMIT = None # For testing purposes. Set to NONE to run all\n",
    "AUTO_INFO = {\n",
    "    \"date\": bu.get_timestamp(),\n",
    "}\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama', # required, but unused\n",
    ")\n",
    "##########################################\n",
    "\n",
    "save = f\"{SETTINGS['model']}_{SIM_ID}\"\n",
    "completions = []\n",
    "l = len(final_prompts)\n",
    "timer = bu.Benchmarker()\n",
    "for i, (prompt, question) in enumerate(list(zip(final_prompts, surv.questions))):\n",
    "    if LIMIT != None and i > LIMIT:\n",
    "        break\n",
    "    timer.mark()\n",
    "    print(f\"{i}/{l}...\", end=\"\\t\") # Print progress\n",
    "    # Send the Request    \n",
    "    full_response = client.chat.completions.create(\n",
    "        model=SETTINGS[\"model\"],\n",
    "        messages=prompt,\n",
    "        # timeout=120,\n",
    "        # temperature=SETTINGS[\"temperature\"],\n",
    "    )\n",
    "\n",
    "    r = full_response.choices[0].message.content\n",
    "    completions.append({'question': question, 'answer': r})\n",
    "    print(f\"{question}: {r}\")\n",
    "\n",
    "timer.mark()\n",
    "# Save results\n",
    "df = pd.DataFrame(completions)\n",
    "# df.to_csv(f\"results/{save}_simulation.csv\", index=False)\n",
    "df.to_csv(f\"simulations/{SIM_ID}_simulation.csv\", index=False)\n",
    "# bu.quickJSON(final_prompts, f\"results/{save}_prompts.json\")\n",
    "bu.quickJSON(final_prompts, f\"ignorefolder/{SIM_ID}_prompts.json\")\n",
    "# bu.quickJSON(SETTINGS, f\"results/{save}_setings.json\")\n",
    "bu.quickJSON({\"settings\": SETTINGS, \"info\": AUTO_INFO}, f\"simulations/{SIM_ID}_info.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dbug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(completions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings and info loaded:\n",
      "date: 2024-04-18_204114\n",
      "model: command-r-plus:104b-q2_K\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am the life of the party.</td>\n",
       "      <td>DISAGREE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I don't talk a lot.</td>\n",
       "      <td>DISAGREE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I feel comfortable around people.</td>\n",
       "      <td>SOMEWHAT AGREEE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I keep in the background.</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I start conversations.</td>\n",
       "      <td>SOMEWHAT AGREEE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            question           answer\n",
       "0        I am the life of the party.         DISAGREE\n",
       "1                I don't talk a lot.         DISAGREE\n",
       "2  I feel comfortable around people.  SOMEWHAT AGREEE\n",
       "3          I keep in the background.          NEUTRAL\n",
       "4             I start conversations.  SOMEWHAT AGREEE"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "SIMULATION_NAMEID = \"elias-personality_rplus_cv1\"\n",
    "# SIM_ID\n",
    "\n",
    "df = pd.read_csv(f'simulations/local/{SIMULATION_NAMEID}_simulation.csv')\n",
    "# df = df.drop(df.columns[0], axis=1) #if loaded from csv, drop the added index col\n",
    "with open(f'simulations/local/{SIMULATION_NAMEID}_info.json', 'r') as f:\n",
    "    loaded = json.load(f)\n",
    "try:\n",
    "    AUTO_INFO = loaded[\"info\"]\n",
    "    SETTINGS = loaded[\"settings\"]\n",
    "    print(\"Settings and info loaded:\")\n",
    "    for k, v in AUTO_INFO.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "    for k, v in SETTINGS.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "except:\n",
    "    print(\"No settings and/or info found\")\n",
    "\n",
    "try:\n",
    "    if str(type(surv)) != AUTO_INFO[\"survey_type\"]:\n",
    "        print(f\"WARNING: surv variable is not of the same type. {str(type(surv))} != {AUTO_INFO['survey_type']}\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "df.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bu.if_dir_not_exist_make(\"simulations/results\")\n",
    "res = bu.LiveCSV(f\"simulations/elias_runs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proces simulation output - KANO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(surv.test_answers[\"elias\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surv = survey.KanoSurvey()\n",
    "# Add airidas and elias answers\n",
    "air = surv.test_answers[\"airidas\"]\n",
    "eli = surv.test_answers[\"elias\"]\n",
    "df.insert(2, \"airidas\", air[:len(df)])\n",
    "df.insert(3, \"elias\", eli[:len(df)])\n",
    "\n",
    "df['answer'] = df['answer'].str.upper()\n",
    "df['airidas'] = df['airidas'].str.upper()\n",
    "df['elias'] = df['elias'].str.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proces simulation output - PERSONALITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surv = survey.PersonalitySurvey()\n",
    "# df = df.dropna()\n",
    "\n",
    "# Add airidas and elias answers\n",
    "air = surv.test_answers[\"airidas\"]\n",
    "eli = surv.test_answers[\"elias\"]\n",
    "df.insert(2, \"airidas\", air[:len(df)])\n",
    "df.insert(3, \"elias\", eli[:len(df)])\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute one number of how the percentage of correct answers\n",
    "result_data = {\n",
    "    \"Exact Matches\": (df['answer'] == df['airidas']).sum() / len(df),\n",
    "    \"Correlation\": df['answer'].corr(df['airidas']),\n",
    "    \"Exact Matches - elias\": (df['answer'] == df['elias']).sum() / len(df),\n",
    "    \"Correlation - elias\": df['answer'].corr(df['elias']),\n",
    "}\n",
    "\n",
    "for k, v in result_data.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(type(surv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_res = {\n",
    "    # \"label\": None,\n",
    "    \"SIMULATION_NAMEID\": SIMULATION_NAMEID,#SIM_ID,\n",
    "    \"timestamp\": bu.get_timestamp(),\n",
    "    \"survey_type\": str(type(surv)),\n",
    "    # \"temperature\": SETTINGS[\"temperature\"],\n",
    "    # \"note\": \"\",\n",
    "    \"exact_matches\": result_data[\"Exact Matches\"],\n",
    "    \"corr\": result_data[\"Correlation\"],\n",
    "    \"exact_matches_elias\": result_data[\"Exact Matches - elias\"],\n",
    "    \"corr_elias\": result_data[\"Correlation - elias\"],\n",
    "}\n",
    "\n",
    "tmp = bu.convert_dicts_to_table([new_res])\n",
    "res.append_data(tmp[1], tmp[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all characters from a black list from the column answer\n",
    "df['answer'] = df['answer'].apply(lambda x: x.strip())\n",
    "for substr in utils.BLACKLIST_ANSWER_SUBSTRINGS:\n",
    "    df['answer'] = df['answer'].apply(lambda x: re.sub(substr, \"\", x))\n",
    "df['answer'] = df['answer'].str.upper()\n",
    "\n",
    "df[30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all characters from a black list from the column answer\n",
    "df['answer'] = df['answer'].apply(lambda x: x.strip())\n",
    "\n",
    "for substr in utils.BLACKLIST_ANSWER_SUBSTRINGS:\n",
    "    df['answer'] = df['answer'].apply(lambda x: re.sub(substr, \"\", x))\n",
    "\n",
    "df['answer'] = df['answer'].str.upper()\n",
    "# Update isValid\n",
    "df['isValid'] = df['answer'].apply(lambda x: x in surv.POSSIBLE_ANSWERS)\n",
    "\n",
    "# if all values in isValid is true, drop the column, else print a message\n",
    "if df['isValid'].all():\n",
    "    df = df.drop('isValid', axis=1)\n",
    "    print(\"All answers were valid\")\n",
    "else:\n",
    "    print(\"Some answers were not valid\")\n",
    "\n",
    "df.head(n=10)\n",
    "\n",
    "########################\n",
    "\n",
    "# remove all characters from a black list from the column answer\n",
    "for substr in utils.BLACKLIST_ANSWER_SUBSTRINGS:\n",
    "    df['answer'] = df['answer'].apply(lambda x: re.sub(substr, \"\", x))\n",
    "\n",
    "df['answer'] = df['answer'].str.upper()\n",
    "# Update isValid\n",
    "df['isValid'] = df['answer'].apply(lambda x: x in surv.POSSIBLE_ANSWERS)\n",
    "\n",
    "# if all values in isValid is true, drop the column, else print a message\n",
    "if df['isValid'].all():\n",
    "    df = df.drop('isValid', axis=1)\n",
    "    print(\"All answers were valid\")\n",
    "else:\n",
    "    print(\"Some answers were not valid\")\n",
    "\n",
    "df\n",
    "#### Cleanup\n",
    "# remove all characters from a black list from the column answer\n",
    "# for substr in utils.BLACKLIST_ANSWER_SUBSTRINGS:\n",
    "#      df['answer'] = df['answer'].apply(lambda x: re.sub(substr, \"\", x))\n",
    "# # Update isValid\n",
    "#      df['isValid'] = df['answer'].apply(lambda x: x in surv.POSSIBLE_ANSWERS)\n",
    "\n",
    "# if all values in isValid is true, drop the column, else print a message\n",
    "# if df['isValid'].all():\n",
    "#     df = df.drop('isValid', axis=1)\n",
    "# else:\n",
    "#     print(\"Some answers were not valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['answer'] = df['answer'].str.upper()\n",
    "df['airidas'] = df['airidas'].str.upper()\n",
    "df['elias'] = df['elias'].str.upper()\n",
    "\n",
    "df['answer'] = df['answer'].map(remap_dict)\n",
    "df['airidas'] = df['airidas'].map(remap_dict)\n",
    "df['elias'] = df['elias'].map(remap_dict)\n",
    "\n",
    "# df['CLONE_eli'] = df['CLONE_eli'].map(remap_dict)\n",
    "\n",
    "# df = df.drop(columns=['uppercase_text'])\n",
    "# df['CLONE_eli'] = df['answer'].apply(extract_uppercase_text)\n",
    "# df['CLONE_eli'] = df['CLONE_eli'].str.upper()\n",
    "# .str.upper() or .lower()\n",
    "# df['answer'] = df['answer'].map(remap_dict, na_action='ignore')\n",
    "#df['CLONE_eli'] = df['CLONE_eli'].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(surv, survey.KanoSurvey):\n",
    "    remap_dict = {\"I EXPECT IT\": 5, \"I LIKE IT\": 4, \"I AM NEUTRAL\": 3, \"I CAN TOLERATE IT\": 2, \"I DISLIKE IT\": 1}\n",
    "    df['answer'] = df['answer'].map(remap_dict)\n",
    "    df['airidas'] = df['airidas'].map(remap_dict)\n",
    "    df['elias'] = df['elias'].map(remap_dict)\n",
    "elif isinstance(surv, survey.PersonalitySurvey):\n",
    "    remap_dict = {\"AGREE\": 5, \"SOMEWHAT AGREE\": 4, \"NEUTRAL\": 3, \"SOMEWHAT DISAGREE\": 2, \"DISAGREE\": 1}\n",
    "    df['answer'] = df['answer'].map(remap_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DF stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remaps - UNIVERSAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     pattern = r'\\b(?:' + '|'.join(re.escape(phrase) for phrase in phrases_to_extract) + r')\\b'\n",
    "#     matches = re.findall(pattern, text, flags=re.IGNORECASE) \n",
    "#     return ' '.join(matches) if matches else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute one number of how the percentage of correct answers\n",
    "print(f\"Exact Matches: {(df['CLONE_eli'] == df['IRL_eli']).sum() / len(df)}\")\n",
    "print(f\"Correlation: {df['CLONE_eli'].corr(df['IRL_eli'])}\")\n",
    "\n",
    "df['elias_correct'] = df['CLONE_eli'] == df['IRL_eli']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Batch Sim**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "import traceback\n",
    "import sys\n",
    "\n",
    "import brikasutils as bu\n",
    "import shared_utils as utils\n",
    "import survey\n",
    "importlib.reload(bu)\n",
    "importlib.reload(utils)\n",
    "importlib.reload(survey)\n",
    "\n",
    "queue = bu.FileRunQueue(queue_folder_path=\"batch/queue\", completed_folder_path=\"batch/done\")\n",
    "report_live_csv = bu.LiveCSV(\"batch/run_reports.csv\")\n",
    "timer = bu.Benchmarker()\n",
    "\n",
    "\n",
    "for filepath in queue:\n",
    "    timer.mark_start(filepath)\n",
    "\n",
    "    try: \n",
    "        ########## Handle batch stuff ########\n",
    "        filename = os.path.splitext(os.path.basename(filepath))[0]\n",
    "        with open(filepath, 'r') as f:\n",
    "            rundata = json.load(f)\n",
    "\n",
    "        # Load prompt file\n",
    "        with open(rundata[\"instructions\"][\"prompt_file\"], 'r') as f:\n",
    "            final_prompts = json.load(f)\n",
    "\n",
    "        # Make the surv\n",
    "        if rundata[\"instructions\"][\"survey_type\"] == \"KanoSurvey\":\n",
    "            surv = survey.KanoSurvey()\n",
    "        elif rundata[\"instructions\"][\"survey_type\"] == \"PersonalitySurvey\":\n",
    "            surv = survey.PersonalitySurvey()\n",
    "        else:\n",
    "            raise Exception(\"Invalid survey type\")\n",
    "\n",
    "        timestamp = bu.get_timestamp()\n",
    "        ######### Run Simulation ########\n",
    "        SIMULATION_NAMEID = filename\n",
    "        LIMIT = rundata[\"instructions\"][\"LIMIT\"] if \"LIMIT\" in rundata[\"instructions\"] else None\n",
    "        AUTO_INFO = {\n",
    "            \"date\": timestamp,\n",
    "            **rundata[\"info\"], # unpacked from rundata\n",
    "            \"limit\": LIMIT,\n",
    "            \"prompt_count\": min(len(final_prompts), LIMIT) if LIMIT != None else len(final_prompts),\n",
    "            \"avg_tokens_in_prompt\": round(utils.describe_prompts(final_prompts)[\"total_all_prompt_tokens\"]/len(final_prompts)),\n",
    "        }\n",
    "        SETTINGS = {\n",
    "            **rundata[\"settings\"], # unpacked from rundata\n",
    "        }\n",
    "\n",
    "        # client depends on if it's local or not\n",
    "        if rundata[\"instructions\"][\"isLocal\"]:\n",
    "            client = OpenAI(\n",
    "                base_url = 'http://localhost:11434/v1',\n",
    "                api_key='ollama', # required, but unused\n",
    "            )\n",
    "        else:\n",
    "            client = OpenAI(\n",
    "                api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "            )\n",
    "\n",
    "        completions = []\n",
    "        l = len(final_prompts)\n",
    "\n",
    "        for i, (prompt, question) in enumerate(list(zip(final_prompts, surv.questions))):\n",
    "            if LIMIT != None and i > LIMIT:\n",
    "                break\n",
    "\n",
    "            print(f\"{i}/{l}...\", end=\"\\t\") # Print progress\n",
    "            # Send the Request\n",
    "            full_response = client.chat.completions.create(\n",
    "                messages=prompt,\n",
    "                **SETTINGS,\n",
    "            )\n",
    "            r = full_response.choices[0].message.content\n",
    "\n",
    "            completions.append({'question': question, 'answer': r})\n",
    "\n",
    "            print(f\"{question}: {r}\")\n",
    "            \n",
    "        ############ Save Important results\n",
    "        df = pd.DataFrame(completions)\n",
    "        df.to_csv(f\"batch/output/{SIMULATION_NAMEID}_simulation.csv\", index=False)\n",
    "        bu.if_dir_not_exist_make(\"batch/output/info\")\n",
    "        bu.quickJSON({\"settings\": SETTINGS, \"info\": AUTO_INFO}, f\"batch/output/info/{SIMULATION_NAMEID}_info.json\")\n",
    "\n",
    "        status = \"OK\"\n",
    "    \n",
    "    except Exception:\n",
    "        print(f\"##### Error while running {filename}.\")\n",
    "        error_string = traceback.format_exc()\n",
    "        print(error_string)\n",
    "        status = \"Failed\"\n",
    "\n",
    "    ########### Time the run\n",
    "    try:\n",
    "        time_taken = timer.mark_end(filepath)\n",
    "    except:\n",
    "        print(\"Error while timing run: \")\n",
    "        print(traceback.format_exc())\n",
    "        time_taken = None\n",
    "\n",
    "    ########### Report the run\n",
    "    try:\n",
    "        new_report = {\n",
    "            \"filename\": filename,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"time_taken\": time_taken,\n",
    "            \"status\": status,\n",
    "            **rundata[\"instructions\"],\n",
    "            \"error\": error_string if status == \"Failed\" else \"\",\n",
    "        }\n",
    "\n",
    "        tmp = bu.convert_dicts_to_table([new_report])\n",
    "        report_live_csv.append_data(tmp[1], tmp[0])\n",
    "    except Exception as e:\n",
    "        print(f\"Error while reporting: \")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    print(f\"Processed {filename}. Stauts: {status}\")\n",
    "\n",
    "timer.print_total_execution_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Force short JSON answer\n",
    "\n",
    "Add this to the end of your prompt:\n",
    "> ```json\n",
    "\n",
    "Add this to the \"stop\" sequence:\n",
    ">```\n",
    "\n",
    "The idea is to force the model to continue writing json markdown. And end the generation when it outputs \"```\" which ends the json markdown section.\n",
    "\n",
    "----\n",
    "\n",
    "Command-r-plus\n",
    "TEMPLATE \"\"\"{{ if .System }}<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>{{ .System }}<|END_OF_TURN_TOKEN|>{{ end }}{{ if .Prompt }}<|START_OF_TURN_TOKEN|><|USER_TOKEN|>{{ .Prompt }}<|END_OF_TURN_TOKEN|>{{ end }}<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>{{ .Response }}<|END_OF_TURN_TOKEN|>\"\"\"\n",
    "PARAMETER stop \"<|START_OF_TURN_TOKEN|>\"\n",
    "PARAMETER stop \"<|END_OF_TURN_TOKEN|>\"\n",
    "\n",
    "\n",
    "Mixtral\n",
    "TEMPLATE \"\"\" [INST] {{ .System }} {{ .Prompt }} [/INST]\"\"\"\n",
    "PARAMETER stop \"[INST]\"\n",
    "PARAMETER stop \"[/INST]\"\n",
    "\n",
    "\n",
    "\n",
    "TEMPLATE \"\"\" [INST] {{ .System }} {{ .Prompt }} ```json [/INST] \"\"\"\n",
    "PARAMETER stop \"[INST]\"\n",
    "PARAMETER stop \"[/INST]\"\n",
    "PARAMETER stop \"```\"\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "\n",
    "mixtral x22\n",
    "TEMPLATE \"\"\"[INST] {{ if .System }}{{ .System }} {{ end }}{{ .Prompt }} [/INST]\"\"\"\n",
    "PARAMETER stop \"[INST]\"\n",
    "PARAMETER stop \"[/INST]\"\n",
    "\n",
    "wizard x22\n",
    "TEMPLATE \"\"\"{{ if .System }}{{ .System }} {{ end }}{{ if .Prompt }}USER: {{ .Prompt }} {{ end }}ASSISTANT: {{ .Response }}\"\"\"\n",
    "SYSTEM \"\"\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\"\"\"\n",
    "PARAMETER stop \"USER:\"\n",
    "PARAMETER stop \"ASSISTANT:\"\n",
    "\n",
    "nomic_embed\n",
    "TEMPLATE \"\"\"{{ .Prompt }}\"\"\"\n",
    "PARAMETER num_ctx 8192\n",
    "\n",
    "\n",
    "Mistral 7b\n",
    "TEMPLATE \"\"\"[INST] {{ .System }} {{ .Prompt }} [/INST]\"\"\"\n",
    "PARAMETER stop \"[INST]\"\n",
    "PARAMETER stop \"[/INST]\"\n",
    "\n",
    "Mistral 7b-wizard\n",
    "TEMPLATE \"\"\"{{ if .System }}{{ .System }} {{ end }}{{ if .Prompt }}USER: {{ .Prompt }} {{ end }}ASSISTANT: {{ .Response }}\"\"\"\n",
    "SYSTEM \"\"\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\"\"\"\n",
    "PARAMETER stop \"USER:\"\n",
    "PARAMETER stop \"ASSISTANT:\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
