{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import brikasutils as bu\n",
    "importlib.reload(bu)\n",
    "import fb_msg_reader as fb\n",
    "importlib.reload(fb)\n",
    "import shared_utils as utils\n",
    "from shared_utils import systemMsg, userMsg, assistantMsg\n",
    "importlib.reload(utils)\n",
    "import survey\n",
    "importlib.reload(survey)\n",
    "import persona\n",
    "importlib.reload(persona)\n",
    "\n",
    "import ollama\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "from typing import List\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 1916 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-09-13 to 2024-03-06\n",
      "Messages saved to self.chats['airidas']\n",
      "Read 618 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-09-10 to 2024-03-03\n",
      "Messages saved to self.chats['christian']\n",
      "Read 297 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2018-07-25 to 2024-01-01\n",
      "Messages saved to self.chats['nikolay']\n",
      "Read 144 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-03-28 to 2021-12-30\n",
      "Messages saved to self.chats['mathis']\n",
      "Read 104 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-08-25 to 2024-03-05\n",
      "Messages saved to self.chats['jacob']\n",
      "Read 159 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-10-12 to 2023-04-30\n",
      "Messages saved to self.chats['chris']\n",
      "Read 161 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-03-28 to 2021-06-06\n",
      "Messages saved to self.chats['aziz']\n",
      "Read 350 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-03-26 to 2022-04-11\n",
      "Messages saved to self.chats['daniela']\n",
      "Read 105 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-03-28 to 2022-07-10\n",
      "Messages saved to self.chats['mihi']\n",
      "Read 117 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-05-17 to 2022-04-11\n",
      "Messages saved to self.chats['viktoria']\n",
      "Read 172 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2019-08-11 to 2023-12-21\n",
      "Messages saved to self.chats['diba']\n",
      "Read 154 messages from 1 files. Failed to read 0 messages.\n",
      "Messages ranged from 2021-12-02 to 2023-04-29\n",
      "Messages saved to self.chats['filip']\n",
      "Filtering\n",
      "link-filter: 109\n",
      "react-filter: 0\n",
      "cookie-data-filter: 12\n",
      "Selected chat airidas for 9995 (668 messages)\n",
      "Selected chat christian for 5252 (576 messages)\n",
      "Selected chat nikolay for 3128 (266 messages)\n",
      "Selected chat mathis for 1325 (134 messages)\n",
      "Selected chat daniela for 2881 (293 messages)\n",
      "Selected chat diba for 1799 (156 messages)\n",
      "Selected chat aziz for 1219 (136 messages)\n",
      "Selected chat jacob for 1124 (93 messages)\n",
      "Selected chat chris for 1535 (151 messages)\n",
      "Selected chat filip for 1306 (146 messages)\n",
      "Selected chat mihi for 1098 (102 messages)\n",
      "Selected chat viktoria for 1168 (107 messages)\n",
      "Combined tokens: 31830\n"
     ]
    }
   ],
   "source": [
    "et = persona.PersonaEncoder()\n",
    "\n",
    "# ==== FB messages ====\n",
    "et.parse_fb_messages([\"data-raw/1_airidas.json\"], \"airidas\")\n",
    "et.parse_fb_messages([\"data-raw/2_christian.json\"], \"christian\")\n",
    "et.parse_fb_messages([\"data-raw/1_nikolay.json\"], \"nikolay\")\n",
    "et.parse_fb_messages([\"data-raw/2_mathis.json\"], \"mathis\")\n",
    "et.parse_fb_messages([\"data-raw/2_jacob.json\"], \"jacob\")\n",
    "et.parse_fb_messages([\"data-raw/2_chris.json\"], \"chris\")\n",
    "et.parse_fb_messages([\"data-raw/3_aziz.json\"], \"aziz\")\n",
    "et.parse_fb_messages([\"data-raw/3_daniela.json\"], \"daniela\")\n",
    "et.parse_fb_messages([\"data-raw/3_mihi.json\"], \"mihi\")\n",
    "et.parse_fb_messages([\"data-raw/3_viktoria.json\"], \"viktoria\")\n",
    "et.parse_fb_messages([\"data-raw/4_diba.json\"], \"diba\")\n",
    "et.parse_fb_messages([\"data-raw/6_filip.json\"], \"filip\")\n",
    "#et.parse_wa_messages(texts_with_rebecca, \"rebecca\")\n",
    "# texts_with_others_dict = {\n",
    "#     \"rebecca\": [\"data-raw/messages_1000.json\"],\n",
    "# }\n",
    "# for name, texts in texts_with_others_dict.items():\n",
    "#     et.parse_fb_messages(texts, name)\n",
    "\n",
    "# Regex cleaning\n",
    "et.filter_chats_empty()\n",
    "et.filter_chats_regex(utils.BLACKLIST_CHAT_REGEX_FILTERS)\n",
    "\n",
    "# Compress names\n",
    "for nameid, chat in et.chats.items():\n",
    "    for msg in chat:  \n",
    "        msg.sender = \"Persona\" if msg.sender == \"Elias Salvador Smidt Torjani\"  else \"Friend\"\n",
    "\n",
    "# Start all chats from 2/3rds\n",
    "# for name, chat in et.chats.items():\n",
    "#     et.chats[name] = chat[int(len(chat)/3 * 2):]\n",
    "# Select the final modules\n",
    "et.select_chat_limited_by_tokens(\"airidas\", 10000)\n",
    "et.select_chat_limited_by_tokens(\"christian\", 10000)\n",
    "et.select_chat_full(\"nikolay\")\n",
    "et.select_chat_full(\"mathis\") \n",
    "et.select_chat_full(\"daniela\")\n",
    "et.select_chat_full(\"diba\")\n",
    "et.select_chat_full(\"aziz\")\n",
    "et.select_chat_full(\"jacob\")  \n",
    "et.select_chat_full(\"chris\")\n",
    "et.select_chat_full(\"filip\")\n",
    "et.select_chat_full(\"mihi\")\n",
    "et.select_chat_full(\"viktoria\")\n",
    "# for name in texts_with_others_dict.keys():\n",
    "#     ab.select_chat_full(name)\n",
    "\n",
    "# save\n",
    "big_module=et.output()\n",
    "bu.quickTXT(big_module, filename=f\"data/big_module_{bu.get_timestamp()}\")\n",
    "\n",
    "# stats\n",
    "token_counts = et.count_all_selected_chat_tokens() # token_counts used later for statistics\n",
    "print(f\"Combined tokens: {sum(token_counts.values())}\")\n",
    "# utils.count_tokens(big_module) \n",
    "# or list(et.selectedChats.keys()) --> et.count_chat_tokens(\"{friend}\")\n",
    "# et.selectedChats[\"{friend}\"][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default Personality Survey CSV file: surveys/survey_personality-test.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    I am the life of the party.\n",
       "1            I don't talk a lot.\n",
       "Name: question, dtype: object"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surv = survey.PersonalitySurvey()\n",
    "# surv = survey.KanoSurvey()\n",
    "# surv = survey.buildFairnessPrompts()\n",
    "# surv = survey.DictatorGameSurvey()\n",
    "surv.questions[:2]#.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "EMBED_MODEL = \"nomic-embed-text\"        # nomic-embed-text = long ctx / mxbai-embed-large = big\n",
    "CHUNK_SIZE = 30                         # Number of messages per chunk\n",
    "OVERLAP_SIZE = 10                       # Number of overlapping messages between consecutive chunks\n",
    "# COMMENT 04-16, perhaps we could try 5x retrievals with isolated semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk count: 130\n",
      "Average chunk character length: 1244\n",
      "Rough estimate of tokens per chunk: 311 (4 characters per token)\n",
      "Messagees in input count: 2828\n",
      "Messages in chunks count: 3900\n",
      "Chunk \\ Input ratio: 1.38 (OVERLAP_SIZE=10)\n",
      "Chunk Python type: <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists for storing chunks â€“ and embeddings later\n",
    "# different chunk size\n",
    "chunks = []\n",
    "stat_total_msgs_in_chunks = 0 # for statistics\n",
    "\n",
    "# different chunk size\n",
    "# Iterate over chats and messages to create chunks\n",
    "for chat in et.selectedChats.values():\n",
    "    messages = list(chat)  # Convert chat iterator to list for easier slicing\n",
    "    num_messages = len(messages)\n",
    "\n",
    "    # Create overlapping chunks of messages\n",
    "    for i in range(0, num_messages - CHUNK_SIZE + 1, CHUNK_SIZE - OVERLAP_SIZE):\n",
    "        chunk = messages[i:i + CHUNK_SIZE]  # Extract chunk of messages\n",
    "        chunk_text = \"\\n\".join(str(msg) for msg in chunk)  # Concatenate messages into a single string\n",
    "        chunks.append(chunk_text)  # Append chunk to list of chunks\n",
    "\n",
    "        stat_total_msgs_in_chunks += len(chunk) # For statistics\n",
    "\n",
    "##### Display Info\n",
    "total_messages = sum(len(chat) for chat in et.selectedChats.values())\n",
    "chunks_count = len(chunks)\n",
    "avg_chunk_char_len = np.mean([len(chunk) for chunk in chunks])\n",
    "\n",
    "print(\n",
    "    f\"Chunk count: {chunks_count}\",\n",
    "    f\"Average chunk character length: {round( avg_chunk_char_len)}\",\n",
    "    f\"Rough estimate of tokens per chunk: {round(avg_chunk_char_len / 4)} (4 characters per token)\",\n",
    "    f\"Messagees in input count: {total_messages}\",\n",
    "    f\"Messages in chunks count: {stat_total_msgs_in_chunks}\",\n",
    "    f\"Chunk \\ Input ratio: {round(stat_total_msgs_in_chunks / total_messages,2)} (OVERLAP_SIZE={OVERLAP_SIZE})\",\n",
    "    f\"Chunk Python type: {type(chunks[0])}\",\n",
    "    sep=\"\\n\"\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generaterating embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Serialization ########\n",
    "EMBEDDING_NAMEID = \"mixtral\"\n",
    "AUTO_INFO = {\n",
    "    \"model\": EMBED_MODEL,\n",
    "    \"CHUNK_SIZE\": CHUNK_SIZE,\n",
    "    \"OVERLAP_SIZE\": OVERLAP_SIZE,\n",
    "    \"chunks_count\": chunks_count,\n",
    "    \"total_messages\": total_messages,\n",
    "    \"stat_total_msgs_in_chunks\": stat_total_msgs_in_chunks,\n",
    "    \"modules_chat\": token_counts,\n",
    "}\n",
    "##################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 130/130"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for each chunk\n",
    "embeddings = []\n",
    "\n",
    "progress, chunks_len = 0, len(chunks) # for progress bar\n",
    "for chunk_text in chunks:\n",
    "    progress += 1\n",
    "    print(f\"\\rChunk {progress}/{chunks_len}\", end=\"\")\n",
    "\n",
    "    embedding = ollama.embeddings(model=EMBED_MODEL, prompt=chunk_text)[\"embedding\"]\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "####################################################\n",
    "# Generate embeddings for each chunk\n",
    "# for chunk_text in chunks:\n",
    "#     embedding = ollama.embeddings(model=EMBED_MODEL, prompt=chunk_text)[\"embedding\"]\n",
    "#     embeddings.append(embedding)\n",
    "\n",
    "\n",
    "# token counts in all similar chunks\n",
    "# tokens_in_chunks = 0\n",
    "# for chunk in chunks_most_similar:\n",
    "#     tokens_in_chunks += utils.count_tokens(chunk)\n",
    "# print(f\"Tokens in chunks: {tokens_in_chunks}\")\n",
    "\n",
    "bu.if_dir_not_exist_make(\"embeddings\")\n",
    "bu.quickJSON(AUTO_INFO, f\"embeddings/{EMBEDDING_NAMEID}_info.json\")\n",
    "bu.quickJSON({\"chunks\": chunks, \"embeddings\": embeddings}, f\"embeddings/{EMBEDDING_NAMEID}_embeddings.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = \"elias\"\n",
    "\n",
    "# persona_small = \"{small module}\"\n",
    "# persona_med = \"{med module}\"\n",
    "# persona_text = \"Favorite video games are Rimworld, Minecraft, Age of Empires, 7 Days to Die\"\n",
    "\n",
    "# Change below accoring to survey above\n",
    "RETRIEVAL_PROMPT = \"openess conciousness extrovert aggreableness neuroticism\" #\"personality\"\n",
    "# RETRIEVAL_PROMPT = \"video game features\"\n",
    "# q_retrival_prompt =\n",
    "# SURVEY_PROMPT = \"Determine how much {subject} aggree with the statement. Guestimate how {subject} would answer to the question\"\n",
    "\n",
    "CHUNKS_COUNT_IN_CTX = 30 # Number of nearby chunks to put in context window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens in chunks: 10444\n",
      "Chunks:130, embeds:130\n"
     ]
    }
   ],
   "source": [
    "prompt_embedding = ollama.embeddings(model=EMBED_MODEL, prompt=RETRIEVAL_PROMPT)[\"embedding\"]\n",
    "chunks_most_similar_embeddings  = utils.find_most_similar(prompt_embedding, embeddings)[:CHUNKS_COUNT_IN_CTX]\n",
    "chunks_most_similar = []\n",
    "for embedding in chunks_most_similar_embeddings:\n",
    "    chunks_most_similar.append(chunks[embedding[1]])\n",
    "\n",
    "# Display results\n",
    "bu.quickTXT(\"\\n\\n\".join(chunks_most_similar), filename=\"ignorefolder/chunks.txt\")\n",
    "\n",
    "# token counts in all similar chunks\n",
    "tokens_in_chunks = 0\n",
    "for chunk in chunks_most_similar:\n",
    "    tokens_in_chunks += utils.count_tokens(chunk)\n",
    "print(f\"Tokens in chunks: {tokens_in_chunks}\")\n",
    "####################################################\n",
    "print(f\"Chunks:{len(chunks)}, embeds:{len(embeddings)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 50/50\n",
      "Tokens in average chunk group: 1651.78\n"
     ]
    }
   ],
   "source": [
    "dynamic_retrieval_prompts = list(surv.questions)\n",
    "\n",
    "CHUNKS_COUNT_IN_CTX = 5 # Number of nearby chunks to put in context window\n",
    "dynamic_chunks_most_similar: List[List[str]] = [] \n",
    "\n",
    "progress = 0\n",
    "lenn = len(dynamic_retrieval_prompts)\n",
    "for prompt in dynamic_retrieval_prompts:\n",
    "    progress += 1\n",
    "    print(f\"\\rPrompt {progress}/{lenn}\", end=\"\")\n",
    "\n",
    "    prompt_embedding = ollama.embeddings(model=EMBED_MODEL, prompt=prompt)[\"embedding\"]\n",
    "    chunks_most_similar_embeddings = utils.find_most_similar(prompt_embedding, embeddings)[:CHUNKS_COUNT_IN_CTX]\n",
    "    chunks_most_similar = []\n",
    "    for embedding in chunks_most_similar_embeddings:\n",
    "        chunks_most_similar.append(chunks[embedding[1]])\n",
    "\n",
    "    dynamic_chunks_most_similar.append(chunks_most_similar)\n",
    "print(end=\"\\n\")\n",
    "    \n",
    "# VANITY PRINT\n",
    "tokens_in_chunks = 0\n",
    "for chunks_most_similar in dynamic_chunks_most_similar:\n",
    "    for chunk in chunks_most_similar:\n",
    "        tokens_in_chunks += utils.count_tokens(chunk)\n",
    "\n",
    "del chunks_most_similar_embeddings # free memory\n",
    "print(f\"Tokens in average chunk group: {tokens_in_chunks/len(dynamic_chunks_most_similar)}\")\n",
    "bu.quickJSON(dynamic_chunks_most_similar, filename=f\"ignorefolder/dynamic-chunks.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanity preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_text = \"\"\n",
    "PREVIEW_LIMIT = 10\n",
    "\n",
    "for i, chunks_most_similar in enumerate(dynamic_chunks_most_similar):\n",
    "    preview_text += f\"==============Prompt: {dynamic_retrieval_prompts[i]}==============\\n\"\n",
    "    for j, chunk in enumerate(chunks_most_similar):\n",
    "        if j >= PREVIEW_LIMIT:\n",
    "            break\n",
    "        preview_text += f\"=======CHUNK {j}=======\\n{chunk}\\n\\n\"\n",
    "    preview_text += \"\\n\\n\"\n",
    "bu.quickTXT(preview_text, filename=f\"ignorefolder/dynamic-chunks_preview.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Builder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With persona (dynamic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 50 prompts.\n",
      "Average prompt size: 1844 tokens.\n",
      "Min prompt size: 1441, Max prompt size: 2648\n"
     ]
    }
   ],
   "source": [
    "final_prompts = []\n",
    "\n",
    "for question, chunks_most_similar in zip(surv.questions, dynamic_chunks_most_similar):\n",
    "    p = [\n",
    "        systemMsg(\n",
    "            \"You are specialized in impersonating people. You will be presented to the subject through explicit datapoints of their digital footprint. In addition, you will deduct their implicit tastes by shadowing chats between the subject and friends. You will be asked to answer questions from the point of view of the persona. Text below:\",\n",
    "            \"Conversations between persona and friends\",\n",
    "            \"\\nNEW CONVERSATION:\\n\".join(chunks_most_similar)\n",
    "        ),\n",
    "        # Understanding affirmation\n",
    "        assistantMsg('I will answer from the point of view of the persona, based on what I could the deduct from the text provided.'),\n",
    "        # Survey question. With Simulation\n",
    "        userMsg(\"\\n\".join([\n",
    "            f\"Persona is surveyed about their video game survey. The persona must choose answer the question below with one of the given options: {', '.join(surv.POSSIBLE_ANSWERS)}. Persona's answer must only contain the chosen option, without any elaboration, nor introduction. \",\n",
    "            question,\n",
    "            \"Persona chooses: \"\n",
    "        ])),\n",
    "        # assistantMsg(\"\\n\".join([f\"response: \"\n",
    "        # ])),\n",
    "    ]\n",
    "    final_prompts.append(p)\n",
    "\n",
    "prompt_info = utils.describe_prompts_and_print(final_prompts)\n",
    "bu.quickJSON(final_prompts, \"ignorefolder/prompts.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = {\n",
    "    \"role\": \"system\", \n",
    "    \"content\": \"You are an actor specializing in impersonating non-famouns people. You will be presented to the subject through explicit datapoints of their digital footprint. In addition, you will deduct their implicit personality traits by shadowing chats between the subject and friends. You will be asked to answer questions from the point of view of the persona. The persona you will be impersonating is named Elias. Context:\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "Created 50 prompts.\n",
      "Average prompt size: 10878 tokens.\n",
      "Min prompt size: 10875, Max prompt size: 10883\n"
     ]
    }
   ],
   "source": [
    "final_prompts = []\n",
    "\n",
    "for question in surv.questions:\n",
    "    p = [\n",
    "        systemMsg(PROMPT['content']+\"\\n## chat conversions between subject and friends\\n\".join(chunks_most_similar)),\n",
    "        assistantMsg('Understood. I will answer from the point of view of the persona, {subject}, based on what I could the deduct from the text provided above.'),\n",
    "        userMsg(\"\\n\".join([\n",
    "            f'\\n\\n**Your answer should only contain the chosen option without further explanation!** Reply to the statement below - how {subject} would reply - with one of these five options: {\", \".join(surv.POSSIBLE_ANSWERS)}.',\n",
    "            question,\n",
    "            \"The persona chooses: \"\n",
    "        ])),\n",
    "    ]\n",
    "    final_prompts.append(p)\n",
    "\n",
    "print(f\"{len(final_prompts)}\")#,{final_prompts[:1]}\")\n",
    "prompt_info = utils.describe_prompts_and_print(final_prompts) # Vanity print\n",
    "bu.quickJSON(final_prompts, \"ignorefolder/prompts.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base (no persona)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompts = []\n",
    "\n",
    "for question in surv.questions:\n",
    "    p = [\n",
    "        systemMsg(\n",
    "            \"You are participating in a survey. You will be presented with a series of questions about your video game preferrences.\",\n",
    "            f\"You must choose answer to the question below with one of the five options: {', '.join(surv.POSSIBLE_ANSWERS)}. The answer must only contain the chosen option. \"\n",
    "        ),\n",
    "        # Understanding affirmation\n",
    "        assistantMsg('Understood. I will answer the question below with one of the given options.'),\n",
    "        # Survey question. With Simulation\n",
    "        userMsg(\n",
    "            question,\n",
    "            \"Your choice: \"\n",
    "        ),\n",
    "    ]\n",
    "    final_prompts.append(p)\n",
    "\n",
    "prompt_info = utils.describe_prompts_and_print(final_prompts) # Vanity print\n",
    "bu.quickJSON(final_prompts, \"ignorefolder/prompts.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json file to dict\n",
    "# with open(\"simulations/toElias/run2-airidas-personality_cv1_prompts.json\", \"r\") as read_file:\n",
    "# with open(\"simulations/toElias/run2-airidas-video-game-cv1_prompts.json\", \"r\") as read_file:\n",
    "# with open(\"simulations/toElias/run2-base-personality-cv1_prompts.json\", \"r\") as read_file:\n",
    "with open(\"simulations/toElias/run2-base-video-game-cv1_prompts.json\", \"r\") as read_file:\n",
    "    pre_final_prompts = json.load(read_file)\n",
    "\n",
    "\n",
    "# pre_final_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Simulation\n",
    "SETTINGS = {\n",
    "     \"model\": \"command-r-plus:104b-q2_K\", # mixtral, command-r-plus:104b-q2_K\n",
    "     # \"temperature\": 0.5,\n",
    "     # best wizard and mixtral try mixtral-8x22b wizard in uCloud\n",
    "}\n",
    "\n",
    "##################################\n",
    "SIM_ID = f\"run2-base-video-game_rplus_cv2\"\n",
    "LIMIT = None # For testing purposes. Set to NONE to run all\n",
    "AUTO_INFO = {\n",
    "    \"date\": bu.get_timestamp(),\n",
    "    # \"EMBEDDING_NAMEID\": EMBEDDING_NAMEID,\n",
    "    # \"RETRIEVAL_PROMPT\": RETRIEVAL_PROMPT,\n",
    "    # \"CHUNKS_COUNT_IN_CTX\": CHUNKS_COUNT_IN_CTX,\n",
    "    # \"survey_type\": str(type(surv)),\n",
    "    # \"prompt_count\": min(len(final_prompts), LIMIT) if LIMIT != None else len(final_prompts),\n",
    "    # \"avg_tokens_in_prompt\": round(prompt_info[\"total_all_prompt_tokens\"]/len(final_prompts)),\n",
    "}\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama', # required, but unused\n",
    ")\n",
    "\n",
    "save = f\"{SETTINGS['model']}_{SIM_ID}\"\n",
    "##################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/50...\tI am the life of the party.: NEUTRAL\n",
      "process_1 took 2.562s\n",
      "1/50...\tI don't talk a lot.: DISAGREE\n",
      "2/50...\tI feel comfortable around people.: NEUTRAL\n",
      "  \n",
      "Most likely response(s) by Elias to the statement '**I feel comfortable around people**: **\n",
      "process_2 took 7.749s\n",
      "3/50...\tI keep in the background.: DISAGREE\n",
      "4/50...\tI start conversations.: NEUTRAL\n",
      "process_3 took 1.678s\n",
      "5/50...\tI have little to say.: DISAGREE\n",
      "6/50...\tI talk to a lot of different people at parties.: NEUTRAL\n",
      "process_4 took 1.683s\n",
      "7/50...\tI don't like to draw attention to myself.: SOMEWHAT DISAGREE\n",
      "8/50...\tI don't mind being the center of attention.: SOMEWHAT AGREE\n",
      "process_5 took 1.954s\n",
      "9/50...\tI am quiet around strangers.: NEUTRAL\n",
      "10/50...\tI get stressed out easily.: DISAGREE\n",
      "process_6 took 1.677s\n",
      "11/50...\tI am relaxed most of the time.: SOMEWHAT AGree\n",
      "12/50...\tI worry about things.: NEUTRAL\n",
      "process_7 took 1.673s\n",
      "13/50...\tI seldom feel blue.: DISAGREE\n",
      "14/50...\tI am easily disturbed.: NEUTRAL\n",
      "process_8 took 1.675s\n",
      "15/50...\tI get upset easily.: SOMEWHAT AGREEMENT\n",
      "16/50...\tI change my mood a lot.: DISAGREE\n",
      "process_9 took 1.72s\n",
      "17/50...\tI have frequent mood swings.: SOMEWHAT AGREEE\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "## Chat conversions between subject and friends\n",
      "\n",
      " Persona : I think we should have it open , to let him add his competences Friend : mins *Friend : alright ... do you want to call the minds before call with him ahah ? Persona : Long ago Person **a** : I think Carsten is going 2 be quite good on idea !\n",
      "18/50...\tI get irritated easily.: DISAGREE\n",
      "process_10 took 1.622s\n",
      "19/50...\tI often feel blue.: NEUTRAL\n",
      "20/50...\tI feel little concern for others.: DISAGREE\n",
      "process_11 took 1.641s\n",
      "21/50...\tI am interested in people.: AGREE\n",
      "22/50...\tI insult people.: DISAGREE\n",
      "process_12 took 1.63s\n",
      "23/50...\tI sympathize with others' feelings.: AGREE!\n",
      "24/50...\tI am not interested in other people's problems.: DISAGREE\n",
      "process_13 took 1.654s\n",
      "25/50...\tI have a soft heart.: NEUTRAL\n",
      "26/50...\tI am not really interested in others.: SOMEWHAT DISAGREE\n",
      "process_14 took 2.537s\n",
      "27/50...\tI take time out for others.: DISAGREE\n",
      "28/50...\tI feel others' emotions.: DISAGREE\n",
      "process_15 took 1.752s\n",
      "29/50...\tI make people feel at ease.: AGREE!\n",
      "30/50...\tI am always prepared.: NEUTRAL\n",
      "process_16 took 1.754s\n",
      "31/50...\tI leave my belongings around.: DISAGREE\n",
      "32/50...\tI pay attention to details.: SOMEWHAT AGREEMENT\n",
      "process_17 took 2.084s\n",
      "33/50...\tI make a mess of things.: DISAGREE\n",
      "34/50...\tI get chores done right away.: SOMEWHAT AGREE\n",
      "process_18 took 2.357s\n",
      "35/50...\tI often forget to put things back in their proper place.: DISAGREE\n",
      "36/50...\tI like order.: SOMEWHAT AGREE\n",
      "process_19 took 2.07s\n",
      "37/50...\tI shirk my duties.: DISAGREE\n",
      "38/50...\tI follow a schedule.: DISAGREE\n",
      "process_20 took 1.774s\n",
      "39/50...\tI am exacting in my work.: SOMEWHAT AGREES\n",
      "40/50...\tI have a rich vocabulary.: SOMEWHAT AGREEE\n",
      "process_21 took 2.337s\n",
      "41/50...\tI have difficulty understanding abstract ideas.: NEUTRAL\n",
      "42/50...\tI have a vivid imagination.: DISAGREE\n",
      "process_22 took 1.795s\n",
      "43/50...\tI am not interested in abstract ideas.: DISAGREE\n",
      "44/50...\tI have excellent ideas.: SOMEWHAT AGREES\n",
      "process_23 took 2.323s\n",
      "45/50...\tI do not have a good imagination.: DISAGREE\n",
      "46/50...\tI am quick to understand things.: AGREE\n",
      "process_24 took 1.439s\n",
      "47/50...\tI use difficult words.: NEUTRAL\n",
      "48/50...\tI spend time reflecting on things.: Agree!\n",
      "process_25 took 1.776s\n",
      "49/50...\tI am full of ideas.: NEUTRAL\n"
     ]
    }
   ],
   "source": [
    "### ==== THE FUNCTIONAL 1!!!! =====\n",
    "completions = []\n",
    "l = len(final_prompts)\n",
    "timer = bu.Benchmarker()\n",
    "for i, (prompt, question) in enumerate(list(zip(final_prompts, surv.questions))):\n",
    "    if LIMIT != None and i > LIMIT:\n",
    "        break\n",
    "    timer.mark()\n",
    "    print(f\"{i}/{l}...\", end=\"\\t\") # Print progress\n",
    "    # Send the Request    \n",
    "    full_response = client.chat.completions.create(\n",
    "        model=SETTINGS[\"model\"],\n",
    "        messages=prompt,\n",
    "        # timeout=120,\n",
    "        # temperature=SETTINGS[\"temperature\"],\n",
    "    )\n",
    "\n",
    "    r = full_response.choices[0].message.content\n",
    "    completions.append({'question': question, 'answer': r})\n",
    "    print(f\"{question}: {r}\")\n",
    "\n",
    "timer.mark()\n",
    "# Save results\n",
    "df = pd.DataFrame(completions)\n",
    "# df.to_csv(f\"results/{save}_simulation.csv\", index=False)\n",
    "df.to_csv(f\"simulations/{SIM_ID}_simulation.csv\", index=False)\n",
    "# bu.quickJSON(final_prompts, f\"results/{save}_prompts.json\")\n",
    "bu.quickJSON(final_prompts, f\"ignorefolder/{SIM_ID}_prompts.json\")\n",
    "# bu.quickJSON(SETTINGS, f\"results/{save}_setings.json\")\n",
    "bu.quickJSON({\"settings\": SETTINGS, \"info\": AUTO_INFO}, f\"simulations/{SIM_ID}_info.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dbug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(completions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings and info loaded:\n",
      "date: 2024-04-18_200100\n",
      "model: command-r-plus:104b-q2_K\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am the life of the party.</td>\n",
       "      <td>Somewhat agree!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I don't talk a lot.</td>\n",
       "      <td>Persona's answer is: Disagree.\"\"\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I feel comfortable around people.</td>\n",
       "      <td>Persona: Somewhat Disagree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I keep in the background.</td>\n",
       "      <td>Agree.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I start conversations.</td>\n",
       "      <td>Persona Answer: Somewhat Agree!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I have little to say.</td>\n",
       "      <td>Agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I talk to a lot of different people at parties.</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I don't like to draw attention to myself.</td>\n",
       "      <td>Persona: Disagree \\n\\n\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I don't mind being the center of attention.</td>\n",
       "      <td>SOMEWHAT DISAGREE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I am quiet around strangers.</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          question  \\\n",
       "0                      I am the life of the party.   \n",
       "1                              I don't talk a lot.   \n",
       "2                I feel comfortable around people.   \n",
       "3                        I keep in the background.   \n",
       "4                           I start conversations.   \n",
       "5                            I have little to say.   \n",
       "6  I talk to a lot of different people at parties.   \n",
       "7        I don't like to draw attention to myself.   \n",
       "8      I don't mind being the center of attention.   \n",
       "9                     I am quiet around strangers.   \n",
       "\n",
       "                              answer  \n",
       "0                    Somewhat agree!  \n",
       "1  Persona's answer is: Disagree.\"\"\"  \n",
       "2         Persona: Somewhat Disagree  \n",
       "3                             Agree.  \n",
       "4    Persona Answer: Somewhat Agree!  \n",
       "5                              Agree  \n",
       "6                            NEUTRAL  \n",
       "7         Persona: Disagree \\n\\n\\n\\n  \n",
       "8                  SOMEWHAT DISAGREE  \n",
       "9                            Neutral  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "SIMULATION_NAMEID = \"run2-airidas-personality_rplus_cv1\"\n",
    "\n",
    "df = pd.read_csv(f'simulations/{SIMULATION_NAMEID}_simulation.csv')\n",
    "# df = df.drop(df.columns[0], axis=1) #if loaded from csv, drop the added index col\n",
    "\n",
    "with open(f'simulations/{SIMULATION_NAMEID}_info.json', 'r') as f:\n",
    "    loaded = json.load(f)\n",
    "try:\n",
    "    AUTO_INFO = loaded[\"info\"]\n",
    "    SETTINGS = loaded[\"settings\"]\n",
    "    print(\"Settings and info loaded:\")\n",
    "    for k, v in AUTO_INFO.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "    for k, v in SETTINGS.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "except:\n",
    "    print(\"No settings and/or info found\")\n",
    "\n",
    "\n",
    "try:\n",
    "    if str(type(surv)) != AUTO_INFO[\"survey_type\"]:\n",
    "        print(f\"WARNING: surv variable is not of the same type. {str(type(surv))} != {AUTO_INFO['survey_type']}\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_kano= 'results/18th results/command-r-plus:104b-q2_K_base_kano_18th_prompts.json'\n",
    "base_personality = 'results/18th results/command-r-plus:104b-q2_K_base_personality_18th_simulation.csv'\n",
    "elias_kano = 'results/18th results/command-r-plus:104b-q2_K_elias_kano_simulation.csv'\n",
    "elias_personality = 'results/18th results/command-r-plus:104b-q2_K_elias_personality_18th_simulation.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(base_personality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # csv_file = \"surveys/survey_kano-model_v1.csv\"\n",
    "# # surv = survey.KanoSurvey(csv_file)\n",
    "csv_file = \"surveys/survey_personality-test_v1.csv\"\n",
    "surv = survey.PersonalitySurvey(csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some answers were not valid\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>isValid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am the life of the party.</td>\n",
       "      <td>SOMEWHAT AGREE!</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I don't talk a lot.</td>\n",
       "      <td>PERSONA'S ANSWER IS: DISAGREE\"\"\"</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I feel comfortable around people.</td>\n",
       "      <td>PERSONA: SOMEWHAT DISAGREE</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I keep in the background.</td>\n",
       "      <td>AGREE</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I start conversations.</td>\n",
       "      <td>PERSONA ANSWER: SOMEWHAT AGREE!</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I have little to say.</td>\n",
       "      <td>AGREE</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I talk to a lot of different people at parties.</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I don't like to draw attention to myself.</td>\n",
       "      <td>PERSONA: DISAGREE</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I don't mind being the center of attention.</td>\n",
       "      <td>SOMEWHAT DISAGREE</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I am quiet around strangers.</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          question  \\\n",
       "0                      I am the life of the party.   \n",
       "1                              I don't talk a lot.   \n",
       "2                I feel comfortable around people.   \n",
       "3                        I keep in the background.   \n",
       "4                           I start conversations.   \n",
       "5                            I have little to say.   \n",
       "6  I talk to a lot of different people at parties.   \n",
       "7        I don't like to draw attention to myself.   \n",
       "8      I don't mind being the center of attention.   \n",
       "9                     I am quiet around strangers.   \n",
       "\n",
       "                             answer  isValid  \n",
       "0                   SOMEWHAT AGREE!    False  \n",
       "1  PERSONA'S ANSWER IS: DISAGREE\"\"\"    False  \n",
       "2        PERSONA: SOMEWHAT DISAGREE    False  \n",
       "3                             AGREE     True  \n",
       "4   PERSONA ANSWER: SOMEWHAT AGREE!    False  \n",
       "5                             AGREE     True  \n",
       "6                           NEUTRAL     True  \n",
       "7                 PERSONA: DISAGREE    False  \n",
       "8                 SOMEWHAT DISAGREE     True  \n",
       "9                           NEUTRAL     True  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove all characters from a black list from the column answer\n",
    "df['answer'] = df['answer'].apply(lambda x: x.strip())\n",
    "\n",
    "for substr in utils.BLACKLIST_ANSWER_SUBSTRINGS:\n",
    "    df['answer'] = df['answer'].apply(lambda x: re.sub(substr, \"\", x))\n",
    "\n",
    "df['answer'] = df['answer'].str.upper()\n",
    "# Update isValid\n",
    "df['isValid'] = df['answer'].apply(lambda x: x in surv.POSSIBLE_ANSWERS)\n",
    "\n",
    "# if all values in isValid is true, drop the column, else print a message\n",
    "if df['isValid'].all():\n",
    "    df = df.drop('isValid', axis=1)\n",
    "    print(\"All answers were valid\")\n",
    "else:\n",
    "    print(\"Some answers were not valid\")\n",
    "\n",
    "df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "isValid\n",
       "True     34\n",
       "False    16\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"isValid\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(surv, survey.KanoSurvey):\n",
    "    remap_dict = {\"I EXPECT IT\": 5, \"I LIKE IT\": 4, \"I AM NEUTRAL\": 3, \"I CAN TOLERATE IT\": 2, \"I DISLIKE IT\": 1}\n",
    "    df['answer'] = df['answer'].map(remap_dict)\n",
    "    df['airidas'] = df['airidas'].map(remap_dict)\n",
    "    df['elias'] = df['elias'].map(remap_dict)\n",
    "elif isinstance(surv, survey.PersonalitySurvey):\n",
    "    remap_dict = {\"AGREE\": 5, \"SOMEWHAT AGREE\": 4, \"NEUTRAL\": 3, \"SOMEWHAT DISAGREE\": 2, \"DISAGREE\": 1}\n",
    "    df['answer'] = df['answer'].map(remap_dict)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# surv.POSSIBLE_ANSWERS[0]\n",
    "# list(surv.POSSIBLE_ANSWERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remap_dict = {f\"{surv.POSSIBLE_ANSWERS[0]}\": 1, f\"{surv.POSSIBLE_ANSWERS[1]}\": 2, f\"{surv.POSSIBLE_ANSWERS[2]}\": 3, f\"{surv.POSSIBLE_ANSWERS[3]}\": 4, f\"{surv.POSSIBLE_ANSWERS[4]}\": 5}\n",
    "#remap_dict = {\"I EXPECT IT\": 5, \"I LIKE IT\": 4, \"I AM NEUTRAL\": 3, \"I CAN TOLERATE IT\": 2, \"I DISLIKE IT\": 1}\n",
    "remap_dict = {str(value): index + 1 for index, value in enumerate(surv.POSSIBLE_ANSWERS)}\n",
    "\n",
    "def extract_uppercase_text(text):\n",
    "    \"\"\"Extract uppercase text from a string using regex.\"\"\"\n",
    "    \n",
    "    phrases_to_extract = [\n",
    "        surv.POSSIBLE_ANSWERS[0],\n",
    "        surv.POSSIBLE_ANSWERS[1],\n",
    "        surv.POSSIBLE_ANSWERS[2],\n",
    "        surv.POSSIBLE_ANSWERS[3],\n",
    "        surv.POSSIBLE_ANSWERS[4],\n",
    "    #     \"I EXPECT IT\",\n",
    "    #     \"I LIKE IT\",\n",
    "    #     \"I AM NEUTRAL\",\n",
    "    #     \"I CAN TOLERATE IT\",\n",
    "    #     \"I DISLIKE IT\"\n",
    "    ]\n",
    "    pattern = r'\\b(?:' + '|'.join(re.escape(phrase) for phrase in phrases_to_extract) + r')\\b'\n",
    "    matches = re.findall(pattern, text, flags=re.IGNORECASE) \n",
    "    return ' '.join(matches) if matches else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(f'results/{save}_simulation.csv')\n",
    "# df = pd.read_csv('results/mistral_elias_personality_02_simulation.csv')\n",
    "#### Proces simulation output\n",
    "air = surv.test_answers[\"airidas\"]\n",
    "eli = surv.test_answers[\"elias\"]\n",
    "df.insert(2, \"airidas\", air[:len(df)])\n",
    "df.insert(3, \"elias\", eli[:len(df)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with NaN\n",
    "df = df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute one number of how the percentage of correct answers\n",
    "result_data = {\n",
    "    \"Exact Matches\": (df['answer'] == df['airidas']).sum() / len(df),\n",
    "    \"Correlation\": df['answer'].corr(df['airidas']),\n",
    "    \"Exact Matches - elias\": (df['answer'] == df['elias']).sum() / len(df),\n",
    "    \"Correlation - elias\": df['answer'].corr(df['elias']),\n",
    "}\n",
    "\n",
    "for k, v in result_data.items():\n",
    "    print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bu.if_dir_not_exist_make(\"results\")\n",
    "res = bu.LiveCSV(\"results/elias_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_res = {\n",
    "    # \"label\": None,\n",
    "    \"SIMULATION_NAMEID\": SIM_ID,\n",
    "    \"timestamp\": bu.get_timestamp(),\n",
    "    \"survey_type\": str(type(surv)),\n",
    "    # \"temperature\": SETTINGS[\"temperature\"],\n",
    "    # \"note\": \"\",\n",
    "    \"exact_matches\": result_data[\"Exact Matches\"],\n",
    "    \"corr\": result_data[\"Correlation\"],\n",
    "    \"exact_matches_elias\": result_data[\"Exact Matches - elias\"],\n",
    "    \"corr_elias\": result_data[\"Correlation - elias\"],\n",
    "}\n",
    "\n",
    "tmp = bu.convert_dicts_to_table([new_res])\n",
    "res.append_data(tmp[1], tmp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIM_ID = \"run1-airidas-personality\"\n",
    "\n",
    "df = pd.read_csv(f'results/{SIM_ID}_simulation.csv')\n",
    "# df = df.drop(df.columns[0], axis=1) #if loaded from csv, drop the added index col\n",
    "df.head()\n",
    "\n",
    "with open(f'results/{SIM_ID}_info.json', 'r') as f:\n",
    "    AUTO_INFO = json.load(f)\n",
    "for k, v in AUTO_INFO.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "try:\n",
    "    if str(type(surv) != AUTO_INFO[\"survey_type\"]):\n",
    "        print(f\"WARNING: surv variable is not of the same type. {str(type(surv))} != {AUTO_INFO['survey_type']}\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all characters from a black list from the column answer\n",
    "for substr in utils.BLACKLIST_ANSWER_SUBSTRINGS:\n",
    "    df['answer'] = df['answer'].apply(lambda x: re.sub(substr, \"\", x))\n",
    "\n",
    "df['answer'] = df['answer'].str.upper()\n",
    "# Update isValid\n",
    "df['isValid'] = df['answer'].apply(lambda x: x in surv.POSSIBLE_ANSWERS)\n",
    "\n",
    "# if all values in isValid is true, drop the column, else print a message\n",
    "if df['isValid'].all():\n",
    "    df = df.drop('isValid', axis=1)\n",
    "    print(\"All answers were valid\")\n",
    "else:\n",
    "    print(\"Some answers were not valid\")\n",
    "\n",
    "df\n",
    "#### Cleanup\n",
    "# remove all characters from a black list from the column answer\n",
    "# for substr in utils.BLACKLIST_ANSWER_SUBSTRINGS:\n",
    "#      df['answer'] = df['answer'].apply(lambda x: re.sub(substr, \"\", x))\n",
    "# # Update isValid\n",
    "#      df['isValid'] = df['answer'].apply(lambda x: x in surv.POSSIBLE_ANSWERS)\n",
    "\n",
    "# if all values in isValid is true, drop the column, else print a message\n",
    "# if df['isValid'].all():\n",
    "#     df = df.drop('isValid', axis=1)\n",
    "# else:\n",
    "#     print(\"Some answers were not valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proces simulation output - KANO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add airidas and elias answers\n",
    "air = surv.test_answers[\"airidas\"]\n",
    "eli = surv.test_answers[\"elias\"]\n",
    "df.insert(2, \"airidas\", air[:len(df)])\n",
    "df.insert(3, \"elias\", eli[:len(df)])\n",
    "\n",
    "df['answer'] = df['answer'].str.upper()\n",
    "df['airidas'] = df['airidas'].str.upper()\n",
    "df['elias'] = df['elias'].str.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proces simulation output - PERSONALITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add airidas and elias answers\n",
    "air = surv.test_answers[\"airidas\"]\n",
    "eli = surv.test_answers[\"elias\"]\n",
    "df.insert(2, \"airidas\", air[:len(df)])\n",
    "df.insert(3, \"elias\", eli[:len(df)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remaps - UNIVERSAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(surv, survey.KanoSurvey):\n",
    "    remap_dict = {\"I EXPECT IT\": 5, \"I LIKE IT\": 4, \"I AM NEUTRAL\": 3, \"I CAN TOLERATE IT\": 2, \"I DISLIKE IT\": 1}\n",
    "    df['answer'] = df['answer'].map(remap_dict)\n",
    "    df['airidas'] = df['airidas'].map(remap_dict)\n",
    "    df['elias'] = df['elias'].map(remap_dict)\n",
    "elif isinstance(surv, survey.PersonalitySurvey):\n",
    "    remap_dict = {\"AGREE\": 5, \"SOMEWHAT AGREE\": 4, \"NEUTRAL\": 3, \"SOMEWHAT DISAGREE\": 2, \"DISAGREE\": 1}\n",
    "    df['answer'] = df['answer'].map(remap_dict)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Airi\n",
    "df['answer'] = df['answer'].str.upper()\n",
    "df['airidas'] = df['airidas'].str.upper()\n",
    "df['elias'] = df['elias'].str.upper()\n",
    "\n",
    "df['answer'] = df['answer'].map(remap_dict)\n",
    "df['airidas'] = df['airidas'].map(remap_dict)\n",
    "df['elias'] = df['elias'].map(remap_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CLONE_eli'] = df['answer'].apply(extract_uppercase_text)\n",
    "df['CLONE_eli'] = df['CLONE_eli'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop(columns=['uppercase_text'])\n",
    "# .str.upper() or .lower()\n",
    "# df['answer'] = df['answer'].map(remap_dict, na_action='ignore')\n",
    "\n",
    "df['CLONE_eli'] = df['CLONE_eli'].map(remap_dict)\n",
    "#df['CLONE_eli'] = df['CLONE_eli'].fillna(0).astype(int)\n",
    "# df['air'] = df['air'].map(remap_dict)\n",
    "# df['eli'] = df['eli'].map(remap_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute one number of how the percentage of correct answers\n",
    "print(f\"Exact Matches: {(df['CLONE_eli'] == df['IRL_eli']).sum() / len(df)}\")\n",
    "print(f\"Correlation: {df['CLONE_eli'].corr(df['IRL_eli'])}\")\n",
    "\n",
    "df['elias_correct'] = df['CLONE_eli'] == df['IRL_eli']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Force short JSON answer\n",
    "\n",
    "Add this to the end of your prompt:\n",
    "> ```json\n",
    "\n",
    "Add this to the \"stop\" sequence:\n",
    ">```\n",
    "\n",
    "The idea is to force the model to continue writing json markdown. And end the generation when it outputs \"```\" which ends the json markdown section.\n",
    "\n",
    "----\n",
    "## Modelfile\n",
    "\n",
    "Command-r-plus\n",
    "TEMPLATE \"\"\"{{ if .System }}<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>{{ .System }}<|END_OF_TURN_TOKEN|>{{ end }}{{ if .Prompt }}<|START_OF_TURN_TOKEN|><|USER_TOKEN|>{{ .Prompt }}<|END_OF_TURN_TOKEN|>{{ end }}<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>{{ .Response }}<|END_OF_TURN_TOKEN|>\"\"\"\n",
    "PARAMETER stop \"<|START_OF_TURN_TOKEN|>\"\n",
    "PARAMETER stop \"<|END_OF_TURN_TOKEN|>\"\n",
    "\n",
    "\n",
    "Mixtral\n",
    "TEMPLATE \"\"\" [INST] {{ .System }} {{ .Prompt }} [/INST]\"\"\"\n",
    "PARAMETER stop \"[INST]\"\n",
    "PARAMETER stop \"[/INST]\"\n",
    "\n",
    "\n",
    "\n",
    "TEMPLATE \"\"\" [INST] {{ .System }} {{ .Prompt }} ```json [/INST] \"\"\"\n",
    "PARAMETER stop \"[INST]\"\n",
    "PARAMETER stop \"[/INST]\"\n",
    "PARAMETER stop \"```\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mixtral x22\n",
    "TEMPLATE \"\"\"[INST] {{ if .System }}{{ .System }} {{ end }}{{ .Prompt }} [/INST]\"\"\"\n",
    "PARAMETER stop \"[INST]\"\n",
    "PARAMETER stop \"[/INST]\"\n",
    "\n",
    "wizard x22\n",
    "TEMPLATE \"\"\"{{ if .System }}{{ .System }} {{ end }}{{ if .Prompt }}USER: {{ .Prompt }} {{ end }}ASSISTANT: {{ .Response }}\"\"\"\n",
    "SYSTEM \"\"\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\"\"\"\n",
    "PARAMETER stop \"USER:\"\n",
    "PARAMETER stop \"ASSISTANT:\"\n",
    "\n",
    "nomic_embed\n",
    "TEMPLATE \"\"\"{{ .Prompt }}\"\"\"\n",
    "PARAMETER num_ctx 8192\n",
    "\n",
    "\n",
    "Mistral 7b\n",
    "TEMPLATE \"\"\"[INST] {{ .System }} {{ .Prompt }} [/INST]\"\"\"\n",
    "PARAMETER stop \"[INST]\"\n",
    "PARAMETER stop \"[/INST]\"\n",
    "\n",
    "Mistral 7b-wizard\n",
    "TEMPLATE \"\"\"{{ if .System }}{{ .System }} {{ end }}{{ if .Prompt }}USER: {{ .Prompt }} {{ end }}ASSISTANT: {{ .Response }}\"\"\"\n",
    "SYSTEM \"\"\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\"\"\"\n",
    "PARAMETER stop \"USER:\"\n",
    "PARAMETER stop \"ASSISTANT:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = bu.FileRunQueue()\n",
    "\n",
    "for run in queue:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
