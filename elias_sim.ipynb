{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Batch Sim**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "import traceback\n",
    "import sys\n",
    "\n",
    "import brikasutils as bu\n",
    "import shared_utils as utils\n",
    "import survey\n",
    "importlib.reload(bu)\n",
    "importlib.reload(utils)\n",
    "importlib.reload(survey)\n",
    "\n",
    "# SAVE_SIMULATION_DIR = \"batch/output\"\n",
    "SAVE_SIMULATION_DIR = \"batch/output\"\n",
    "queue = bu.FileRunQueue(queue_folder_path=\"batch/queue\", completed_folder_path=\"batch/done/monster_7b\")\n",
    "report_live_csv = bu.LiveCSV(\"batch/run_reports/monster_7b.csv\")\n",
    "timer = bu.Benchmarker()\n",
    "\n",
    "for filepath in queue:\n",
    "    timer.mark_start(filepath)\n",
    "\n",
    "    try: \n",
    "        ########## Handle batch stuff ########\n",
    "        filename = os.path.splitext(os.path.basename(filepath))[0]\n",
    "        with open(filepath, 'r') as f:\n",
    "            rundata = json.load(f)\n",
    "\n",
    "        # Load prompt file\n",
    "        with open(rundata[\"instructions\"][\"prompt_file\"], 'r') as f:\n",
    "            final_prompts = json.load(f)\n",
    "\n",
    "        # Make the surv\n",
    "        if rundata[\"instructions\"][\"survey_type\"] == \"KanoSurvey\":\n",
    "            surv = survey.KanoSurvey()\n",
    "        elif rundata[\"instructions\"][\"survey_type\"] == \"PersonalitySurvey\":\n",
    "            surv = survey.PersonalitySurvey()\n",
    "        else:\n",
    "            raise Exception(\"Invalid survey type\")\n",
    "\n",
    "        timestamp = bu.get_timestamp()\n",
    "        ######### Run Simulation ########\n",
    "        SIMULATION_NAMEID = filename\n",
    "        LIMIT = rundata[\"instructions\"][\"LIMIT\"] if \"LIMIT\" in rundata[\"instructions\"] else None\n",
    "        AUTO_INFO = {\n",
    "            \"date\": timestamp,\n",
    "            **rundata[\"info\"], # unpacked from rundata\n",
    "            \"limit\": LIMIT,\n",
    "            \"prompt_count\": min(len(final_prompts), LIMIT) if LIMIT != None else len(final_prompts),\n",
    "            \"avg_tokens_in_prompt\": round(utils.describe_prompts(final_prompts)[\"total_all_prompt_tokens\"]/len(final_prompts)),\n",
    "        }\n",
    "        SETTINGS = {\n",
    "            **rundata[\"settings\"], # unpacked from rundata\n",
    "        }\n",
    "\n",
    "        # client depends on if it's local or not\n",
    "        if rundata[\"instructions\"][\"isLocal\"]:\n",
    "            client = OpenAI(\n",
    "                base_url = 'http://localhost:11434/v1',\n",
    "                api_key='ollama', # required, but unused\n",
    "            )\n",
    "        else:\n",
    "            client = OpenAI(\n",
    "                api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "            )\n",
    "\n",
    "        completions = []\n",
    "        l = len(final_prompts)\n",
    "\n",
    "        for i, (prompt, question) in enumerate(list(zip(final_prompts, surv.questions))):\n",
    "            if LIMIT != None and i > LIMIT:\n",
    "                break\n",
    "\n",
    "            print(f\"{i}/{l}...\", end=\"\\t\") # Print progress\n",
    "            # Send the Request\n",
    "            full_response = client.chat.completions.create(\n",
    "                messages=prompt,\n",
    "                **SETTINGS,\n",
    "            )\n",
    "            r = full_response.choices[0].message.content\n",
    "\n",
    "            completions.append({'question': question, 'answer': r})\n",
    "\n",
    "            print(f\"{question}: {r}\")\n",
    "            \n",
    "        ############ Save Important results\n",
    "        df = pd.DataFrame(completions)\n",
    "        bu.if_dir_not_exist_make(SAVE_SIMULATION_DIR)\n",
    "        df.to_csv(f\"{SAVE_SIMULATION_DIR}/{SIMULATION_NAMEID}_simulation.csv\", index=False)\n",
    "        bu.if_dir_not_exist_make(os.path.join(SAVE_SIMULATION_DIR, \"info\"))\n",
    "        bu.quickJSON({\"settings\": SETTINGS, \"info\": AUTO_INFO}, f\"{SAVE_SIMULATION_DIR}/info/{SIMULATION_NAMEID}_info.json\")\n",
    "\n",
    "        status = \"OK\"\n",
    "    \n",
    "    except Exception:\n",
    "        print(f\"##### Error while running {filename}.\")\n",
    "        error_string = traceback.format_exc()\n",
    "        print(error_string)\n",
    "        status = \"Failed\"\n",
    "\n",
    "    ########### Time the run\n",
    "    try:\n",
    "        time_taken = timer.mark_end(filepath)\n",
    "    except:\n",
    "        print(\"Error while timing run: \")\n",
    "        print(traceback.format_exc())\n",
    "        time_taken = None\n",
    "\n",
    "    ########### Report the run\n",
    "    try:\n",
    "        new_report = {\n",
    "            \"filename\": filename,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"time_taken\": time_taken,\n",
    "            \"status\": status,\n",
    "            **rundata[\"instructions\"],\n",
    "            \"error\": error_string if status == \"Failed\" else \"\",\n",
    "        }\n",
    "\n",
    "        tmp = bu.convert_dicts_to_table([new_report])\n",
    "        report_live_csv.append_data(tmp[1], tmp[0])\n",
    "    except Exception as e:\n",
    "        print(f\"Error while reporting: \")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    print(f\"Processed {filename}. Stauts: {status}\")\n",
    "\n",
    "timer.print_total_execution_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Live simulation (not batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json file to dict\n",
    "with open(\"batch/prompts/personality_batch-B_8k-dynamic_prompts.json\", \"r\") as read_file:\n",
    "    pre_final_prompts = json.load(read_file)\n",
    "\n",
    "# Setup for below\n",
    "SETTINGS = {\n",
    "     \"model\": \"llama3\", # mixtral, command-r-plus:104b-q2_K\n",
    "     \"stream\": True,\n",
    "    #  \"format\": \"json\",\n",
    "     # \"temperature\": 0.5,\n",
    "}\n",
    "##################################\n",
    "SIM_ID = f\"eli-pers_8k-dynamic_llama3-7_json-test\"\n",
    "save = f\"{SETTINGS['model']}_{SIM_ID}\"\n",
    "LIMIT = None # For testing purposes. Set to NONE to run all\n",
    "AUTO_INFO = {\n",
    "    \"date\": bu.get_timestamp(),\n",
    "    # \"EMBEDDING_NAMEID\": EMBEDDING_NAMEID,\n",
    "    # \"RETRIEVAL_PROMPT\": RETRIEVAL_PROMPT,\n",
    "    # \"CHUNKS_COUNT_IN_CTX\": CHUNKS_COUNT_IN_CTX,\n",
    "    # \"survey_type\": str(type(surv)),\n",
    "    # \"prompt_count\": min(len(final_prompts), LIMIT) if LIMIT != None else len(final_prompts),\n",
    "    # \"avg_tokens_in_prompt\": round(prompt_info[\"total_all_prompt_tokens\"]/len(final_prompts)),\n",
    "}\n",
    "client = OpenAI(\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama', # required, but unused\n",
    ")\n",
    "##################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "##### ==== THE FUNCTIONAL 1!!!! ===== #####\n",
    "###########################################\n",
    "completions = []\n",
    "l = len(final_prompts)\n",
    "timer = bu.Benchmarker()\n",
    "for i, (prompt, question) in enumerate(list(zip(final_prompts, surv.questions))):\n",
    "    if LIMIT != None and i > LIMIT:\n",
    "        break\n",
    "    timer.mark()\n",
    "    print(f\"{i}/{l}...\", end=\"\\t\") # Print progress\n",
    "    # Send the Request    \n",
    "    full_response = client.chat.completions.create(\n",
    "        model=SETTINGS[\"model\"],\n",
    "        messages=prompt,\n",
    "        # timeout=120,\n",
    "        # temperature=SETTINGS[\"temperature\"],\n",
    "    )\n",
    "    r = full_response.choices[0].message.content\n",
    "    completions.append({'question': question, 'answer': r})\n",
    "    print(f\"{question}: {r}\")\n",
    "\n",
    "# Save results\n",
    "df = pd.DataFrame(completions)\n",
    "df.to_csv(f\"simulations/{SIM_ID}_simulation.csv\", index=False)\n",
    "bu.quickJSON(final_prompts, f\"ignorefolder/{SIM_ID}_prompts.json\")\n",
    "bu.quickJSON({\"settings\": SETTINGS, \"info\": AUTO_INFO}, f\"simulations/{SIM_ID}_info.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
